\chapter{Conclusion}
\label{cha:conclusion}

Visual search is ubiquitous in our daily lives as humans.
Automated visual search systems therefore naturally have many potential applications.
Building an autonomous visual search system can be done using domain knowledge, but implementing efficient searchers in arbitrary environments is not trivial.

In this work, we have asked ourselves whether a system can learn to search efficiently from a set of sample scenes with known target locations.
Such a system should pick up patterns in the samples seen during training, and generalize to similar but unseen test scenes.
We have presented a method for this task for jointly learning control of visual attention, recognition and localization using deep reinforcement learning.
Our approach can be used to train agents to look around in environments to search for multiple targets.

The agents try to locate all targets in a minimal number of time steps, by indicating when they are visible.
They are designed to be capable of integrating visual information over time, and reason over explored parts of their environment.
This allows them to avoid searching locations multiple times, and prioritize regions where targets are likely to be found.

Using three environments with different characteristics, we have shown that agents trained with our method are capable of utilizing visual cues to guide search and localize targets.
We have compared two neural network architectures for this purpose - one using a temporal memory and one using a spatial memory.
In all three environments, the architecture with a spatial memory outperforms all three simple baselines in terms of average search length and success rate.
Furthermore, its searches are comparable in performance to a human searcher.
The temporal memory struggles with remembering precise information over many time steps, which is required for efficient search.

By comparing the search paths chosen by both agents to optimal search paths, we show that they on average select paths that are as close or closer to optimal than humans.
Beyond this, the quality of the searches remains undecided.
Comparison to provably optimal behavior could give further insights into the performance of the searches.

By comparing both architectures in varying search space sizes, we have shown that a spatial memory scales better to visual search tasks in large search spaces.
When trained using different numbers of training samples, both architectures overfit to these and perform worse on held out test samples.
This illustrates the need for being mindful of overfitting in reinforcement learning
With a sufficient number of training samples, they are able to generalize to unseen environment samples.
We found that the spatial memory is less prone to overfitting than the temporal memory architecture for this particular search task.

\section{Future Work}

Several limitations of this work have been discussed in Chapter~\ref{cha:discussion}.
We suggest that future work in the area of visual search with deep reinforcement learning should look closer into the following questions:

\begin{itemize}
    \item What are the minimal conditions under which an agent can learn to search optimally?
    
    It is possible that the presented approach does not satisfy the requirements needed to enable it to learn search behaviors that approach optimal ones.
    The number of model parameters, memory capacity, reinforcement learning algorithm, reward signal, etc. could limit the ability of the approach to find an optimal policy.
    This might depend on particular aspects of the search problem, such as action space and observation space.
    It is interesting to see what the requirements are on these aspects of the problem.

    \item How does the approach presented here compare to rule-based agents in each environment?

    With rule-based agents, we mean non-learning agents that are designed with expert knowledge to act well in a particular environments.
    This is related to the question above, but not equivalent.
    We have compared the performance of our approach to humans and three baselines.
    It is expected that rule-based agent can search more intelligently than these.
    To see it if viable to deploy a learning-agent instead of a rule-based one, the two should be compared to one another.

    \item Does the approach scale to real-world search scenarios?

    We have looked at relatively simple search scenarios, where environment cues follow some clear pattern and target detection is trivial.
    Real-world search scenarios will exhibit a different level of complexity.
    Whether the proposed approach scales to such scenarios should be investigated further.
    Such investigations could use existing simulated visual navigation environments, but specialized environments that offer control of the search difficulty could give more insights into the questions above.
\end{itemize}

%Bajcsy, Aloimonos and Tsotsos~\cite{bajcsy_revisiting_2018} connect past work in active vision with recent advances in robotics, artificial intelligence and computer vision.
%They argue that a complete artificial agent must include active perception.
%The goal of artificial intelligence research is the computational generation of intelligent behavior.
%Agents that choose their behavior based on their context and know why they behave as they do would certainly seem to embody this.
%In this work, we have introduced such an agent.
