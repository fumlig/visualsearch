\chapter{Conclusion}
\label{cha:conclusion}

Visual search is ubiquitous in our daily lives as humans.
Automated visual search systems therefore naturally have many potential applications.
Building an autonomous visual search system can be done using domain knowledge, but implementing efficient searchers in arbitrary environments is not trivial.

In this work, we have asked ourselves whether a system can learn to search efficiently from a set of sample scenes with known target locations.
Such a system should pick up patterns in the samples seen during training, and generalize to similar but unseen test scenes.
We have presented a method for this task for jointly learning control of visual attention, recognition and localization using deep reinforcement learning.
Our approach can be used to train agents to look around in environments to search for multiple targets.

The agents try to locate all targets in a minimal number of time steps, by indicating when they are visible.
They are designed to be capable of integrating visual information over time, and reason over explored parts of their environment.
This allows them to avoid searching locations multiple times, and prioritize regions where targets are likely to be found.

Using three environments with different characteristics, we have shown that agents trained with our method are capable of utilizing visual cues to guide search and localize targets.
We have compared two neural network architectures for this purpose - one using a temporal memory and one using a spatial memory.
In all three environments, the architecture with a spatial memory outperforms all three simple baselines in terms of average search length and success rate.
Furthermore, its searches are comparable in performance to a human searcher.
The temporal memory struggles with remembering precise information over many time steps, which is required for efficient search.

By comparing the search paths chosen by both agents to optimal search paths, we show that they on average select paths that are as close or closer to optimal than humans.
Beyond this, the quality of the searches remains undecided.
Comparison to provably optimal behavior could give further insights into the performance of the searches.

By comparing both architectures in varying search space sizes, we have shown that a spatial memory scales better to visual search tasks in large search spaces.
When trained using different numbers of training samples, both architectures overfit to these and perform worse on held out test samples.
This illustrates the need for being mindful of overfitting in reinforcement learning
With a sufficient number of training samples, they are able to generalize to unseen environment samples.
We found that the spatial memory is less prone to overfitting than the temporal memory architecture for this particular search task.

\section{Future Work}

Several limitations of this work have been discussed in Chapter~\ref{cha:discussion}.
We suggest that future work in the area of visual search with deep reinforcement learning should look closer into the following questions:

\begin{itemize}
    \item What are the minimal conditions under which an agent can learn to search optimally?
    % reward, observations, ablations, memory storage, etc.
    \item How does the approach presented here compare to provably optimal agents in each environment?
    % rule-based, handcrafted
    \item Does the approach scale to real-world search scenarios?
    % noise, detection, interpretability, patterns
\end{itemize}

%Bajcsy, Aloimonos and Tsotsos~\cite{bajcsy_revisiting_2018} connect past work in active vision with recent advances in robotics, artificial intelligence and computer vision.
%They argue that a complete artificial agent must include active perception.
%The goal of artificial intelligence research is the computational generation of intelligent behavior.
%Agents that choose their behavior based on their context and know why they behave as they do would certainly seem to embody this.
%In this work, we have introduced such an agent.
