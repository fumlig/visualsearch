\chapter{Conclusion}
\label{cha:conclusion}

Visual search is ubiquitous in our daily lives as humans.
Automated visual search systems therefore naturally have many potential applications.
Building an autonomous visual search system can be done using domain knowledge, but implementing efficient searchers in arbitrary environments is not trivial.

In this work, we have asked ourselves whether a system can learn to search efficiently from a set of sample scenes with known target locations.
Such a system should pick up patterns in the samples seen during training and generalize to similar but unseen test scenes.
We have presented a method for this task for jointly learning control of visual attention, recognition and localization using deep reinforcement learning.
Our approach can be used to train agents to look around in environments to search for multiple targets.

The agents try to locate all targets in a minimal number of time steps, by indicating when they are visible.
They are designed to be capable of integrating visual information over time and reason over explored parts of their environment.
This allows them to avoid searching locations multiple times while prioritizing regions where targets are likely to be found.

Using three environments with different characteristics, we have shown that agents trained with our method are capable of utilizing visual cues to guide search and localize targets.
We have compared two neural network architectures for this purpose - one using a temporal memory and one using a spatial memory.
In all three environments, the architecture with a spatial memory outperforms all three simple baselines in terms of average search length and success rate.
Furthermore, its searches are comparable in performance to a human searcher.
The temporal memory struggles with remembering precise information over many time steps, which is required for efficient search.

By comparing the search paths chosen by both agents to the shortest possible paths, we show that they on average select paths that are better than a set of general baselines that do not utilize environment cues.
Furthermore, our learning agents select paths that are of equal quality or better than those selected by humans.
Comparison with a simple handcrafted policy suggested that neither agent learns policies that are close to optimal.
Beyond this, the quality of the searches needs further investigation.

By comparing both architectures in varying search space sizes, we have shown that a spatial memory scales better to visual search tasks in large search spaces.
When trained using different numbers of training samples, both architectures overfit to these and perform worse on held out test samples.
This illustrates the need for being mindful of overfitting in reinforcement learning.
With a sufficient number of training samples, they are able to generalize to unseen environment samples.
We found that the spatial memory is less prone to overfitting than the temporal memory architecture for chosen search tasks.

\section{Future Work}

Several limitations of this work have been discussed in Chapter~\ref{cha:discussion}.
We suggest that future work in the area of visual search with deep reinforcement learning should look closer into the following questions:

\begin{itemize}
    \item What are the minimal conditions under which an agent can learn to search optimally?
    
    It is possible that the presented approach does not satisfy the requirements needed to enable it to learn search behaviors that are closer to optimal.
    We have illustrated that our two learning agents in their current forms do not find optimal policies in at least one of the environments.
    The ability of our learning agents to find optimal policies could be limited by neural network architecture, choice of reinforcement learning algorithm, reward signal, etc.
    It is interesting to see if and how our approach can be modified slightly to find policies that are closer to optimal.

    \item Does the approach scale to real-world search scenarios?

    We have looked at relatively simple search scenarios, where environment cues follow some clear pattern and target detection is trivial.
    Real-world search scenarios will exhibit a different level of complexity.
    Whether the proposed approach scales to such scenarios should be investigated further.
    Such investigations should either train agents in the real world, or in simulated environments that mirror its properties and evaluate them in the real world.

    \item How can the behavior of learning agents be explained and formally verified?
    
    In this work, we have used performance on a held out test set to judge the performance of our agents.
    Beyond this and some qualitative evaluations, we have treated trained agents as black boxes. 
    Before deploying learning agents in safety-critical scenarios, more guarantees are needed.
    How can we prove that an agent trained with reinforcement learning acts safely?
\end{itemize}

%Bajcsy, Aloimonos and Tsotsos~\cite{bajcsy_revisiting_2018} connect past work in active vision with recent advances in robotics, artificial intelligence and computer vision.
%They argue that a complete artificial agent must include active perception.
%The goal of artificial intelligence research is the computational generation of intelligent behavior.
%Agents that choose their behavior based on their context and know why they behave as they do would certainly seem to embody this.
%In this work, we have introduced such an agent.
