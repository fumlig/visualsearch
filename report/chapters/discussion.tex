\chapter{Discussion}
\label{cha:discussion}

In this chapter, we reflect the obtained results and critically discuss and evaluate the chosen method.
We also discuss the work from a broader perspective (Section~\ref{sec:discussion-wider})

\section{Results}
\label{sec:discussion-results}

The results of the experiments provide insight into the search performance of our method,
and how the two approaches compare in terms of search behavior, scalability to larger search spaces and ability to generalize from limited training sets.
Here, we reflect on the obtained results and evaluate the two approaches.

\subsection{Reflection on Search Performance}

To evaluate the search performance, we first consider the at the average search path length of each agent.
It is useful to compare the path length to the number of points in each search space.
The gaussian and terrain environments contain \(10 \times 10 = 100\) searchable positions, and the camera environment contains \(10 \times 20 = 200\) searchable positions.
A search path length above this suggests many revisited positions.
The exhaustive baseline policy provides a good measure for what sort of path lengths can be achieved when only avoiding revisiting positions.
Both the random and greedy baseline policies select search paths that are on average much longer than the number of positions, suggesting several redundant steps.
This is further reinforced by the sample search paths in Figures \ref{fig:path-random}, \ref{fig:path-greedy} and \ref{fig:path-exhaustive}.

Notably, our temporal memory agent chooses search paths that are substantially longer than those selected by our spatial memory agent and human searchers.
This is especially true for the gaussian and terrain environments, which suggests several redundant steps.
Our spatial memory agent achieves better average lengths in these two environments.

We see that search paths are particularly short in the camera environment,
despite its larger search space.
This has two reasons:
First, targets may be visible in the periphery from a large set of camera positions.
Once targets are visible, it is simpler for an agent to select actions that center them.
Second, targets are not distributed uniformly across many samples of the camera environment.
They are most commonly found near the horizon.
In the gaussian and terrain environments, where the distribution is approximately uniform over many samples, the search path lengths are closer to the number of positions in the search space.

Our temporal memory agent performs very well in the camera environment.
We suspect that this is due to the fact that less emphasis is placed on remembering over many time steps.
Search in this environment can be more reactive.
When inspecting the search patterns used by our temporal memory agent in this environment, we see that it has learned to alternate between scanning across the horizon and locating targets in its periphery.
The baselines are unable to utilize this imbalance in target distributions.
The exhaustive policy is an exception, where the deterministic ordering of actions happened to favor that distribution. 

By looking at some sample search paths, we see that our two approaches both seem to utilize scene appearance to guide their search.
%This claim is supported further by the visualization of the contents of the feature map in Appendix~\ref{app:memory}.
However, they also tend to revisit positions, and it is not clear that this is good.
Figures \ref{fig:path-lstm} and \ref{fig:path-map} illustrate this well --
both agents are drawn to areas with high probability but seem to revisit positions redundantly.
Some of these revisits could be attributed to the stochastic policy.
Making the policy completely deterministic by selecting the action with the highest probability is not a solution, as it tends to lead to dead states or loops where the agent does not progress.

Also worth noting is that revisiting positions is not always the wrong choice.
One such example is when the current position of the agent is directly between two targets.
The optimal path to find both of these is to first go to any one of them, and then to go back towards the other one by revisiting positions.

Next, we consider the SPL score of each agent.
When averaged over many episodes, the SPL score should be a better measure of search path quality than average length, as it also accounts for how close each path is to the optimal one.
From the definition of the SPL metric in Equation \ref{equ:spl}, we that it favors search paths whose length are close to the shortest one in each sample.
Finding the shortest path in a search scenario such as the one we consider here is not feasible.
However, it still serves as a useful normalization method as it is an indication of how difficult each sample is to solve.

Both our two approaches and human searchers achieve similar SPL scores in all three environments.
While the exhaustive baseline policy achieves average path lengths that are close to those of our two approaches and human searchers, its lower SPL score suggests that it is less successful in finding close to optimal solutions.
In all three environments, one of our approaches achieve a higher SPL score,
although we would not consider this significant due to the large variance.

Under these metrics, both approaches achieve comparable performance to that of a human searcher. 
Overall, it would seem like our spatial memory approach is most suited where large search spaces have to be explored strategically, while our temporal memory approach is sufficient for search problems where search spaces can be explored reactively.

\subsection{Scalability To Larger Search Space Sizes}

In order to be useful for real-world search tasks, an agent should be able to search in sufficiently large spaces.
In Section~\ref{sec:shape} we saw that both of our approaches achieve comparable performance on the \(10 \times 10\) search space.
Both quickly converge towards a high success rate and reasonable search path lengths.
The spatial memory is consistently capable of finding all targets in less steps than the number of searchable positions, and the temporal memory is not far behind.

For larger search spaces, the slight difference is accentuated.
The temporal memory agent converges to search paths that are longer than needed, suggesting that it is not capable of remembering visited locations.
In the \(20 \times 20\) search space, it seems to have converged to path lengths of around 800, and has a success rate of less than 50\%.
By the end of training, the search paths of the spatial memory agent are near the total number of positions and it has a success rate of around 90\%.
Furthermore, the training does not seem to have fully converged.

It seems reasonable to state that the spatial memory scales better to large search spaces in terms of performance, when compared to the temporal memory.
Searching efficiently in large search spaces places extra emphasis on remembering what has already been explored.
From our results, it would seem like an LSTM struggles with remembering such precise information over many time steps.
Similar results where presented by \cite{oh_control_2016}.

An hypothesis is that our temporal memory agent would handle larger search spaces with more parameters.
We experimented with stacking multiple LSTM layers, but found that it led to overfitting and unstable learning, even with dropout.
Further analysis is needed to see if such augmentations can make it more comparable to the spatial memory in terms of performance.

Even though we have investigated how our approaches scale to larger search spaces,
it should be noted that realistic search spaces are often considerably larger.
This poses a problem for both of our approaches, as the number of trainable parameters scales with the search space size in both of them.
One solution is to keep search spaces small:
Large (and continuous) spaces could be discretized into smaller ones at the cost of precision.
Limit the need for memory could be a viable solution for certain search problems..
In the case of the temporal memory, it might be sufficient to only remember recent interactions.
Similarly, in the case of the spatial memory, a possibility is to use an ego-centric recurrent feature map and only remember proximous interactions~\cite{parisotto_neural_2017}.

\subsection{Generalizing from Limited Training Sets}

The hope that a learning agent can pick up patterns that are not visible to humans (or easily communicated to machines) is only relevant if it can do so from a reasonable number of samples.
From the results in Section~\ref{sec:sample}, we see that both of our approaches can overfit to small training sets.
This is in line with what is reported by \cite{cobbe_quantifying_2019}, \cite{cobbe_leveraging_2020} and \cite{zhang_study_2018}.
Past a certain number of interactions with small a small training set, performance on a held out test set decreases.
This effect is well known in supervised learning, but important to keep in mind for reinforcement learning.
If any sort of generalization is expected from our agents, they should be tested on held out samples.

Interestingly, our spatial memory agent seems to be better at generalizing from a limited number of samples --
for 500 and 1000 samples, its performance on the test set does not decrease as severely as our temporal memory agent.
Furthermore, the test performance of our spatial memory agent is strictly better than that of our temporal memory agent in all training set sizes.
This suggests that the spatial memory provides a more suitable inductive bias for the considered problem.

It should be stated that these numbers are not representative of all search problems.
The number of samples needed to generalize to other search tasks may be different than for the terrain environment we used in Section~\ref{sec:sample}.
While the terrain environment exhibits more subtle patterns than the gaussian environment, it is free from noise and is relatively low in variance compared to realistic scenarios.
However, the results do offer insights into how these two architectures compare when it comes to generalization from a limited number of samples.

\section{Method}
\label{sec:discussion-method}

The method chosen covers a fairly broad category of visual search problems,
but focuses on the research questions in Section~\ref{sec:questions}.
Some of the method details have implications for its applicability and validity, and should be discussed further.

\subsection{Image Observations, Detections and Indications}

The image observations are fundamental to the visual search problem.
We have chosen to test our agents in environments with \(64 \times 64\) image observations.
In many cases, images of this size are not sufficient to convey the full meaning of a scene.
Cues that may guide attention or are needed to correctly detect targets could require higher detail.

To scale up our approach to such problems, it could be sufficient to modify the CNN architecture.
CNNs have been used for detection in images of higher resolution~\cite{zhao_object_2019}.
At a certain point, however, high-dimensional images could become a speed bottleneck.
This is important to consider in many real-time applications.
For such cases, some covert attention mechanism such as the one proposed in~\cite{mnih_recurrent_2014} could be considered.

While we explicitly delimit ourselves from difficult detection problems in this work,
detection is likely to be difficult in most real-world search scenarios.
Further investigations are needed to evaluate how viable our approach is for more realistic scenes,
where variance is higher and it is more difficult to interpret image observations.
One option could be to is to incorporate an existing object detection architectures into our deep RL approach, as in \cite{ye_active_2018}

Related to the detection problem is our choice of including an indication action.
It could be argued that explicitly indicating when targets are in view is redundant.
Indications could instead happen implicitly when targets are in view.
This is true if the focus is only on how environment cues can be used to guide visual attention during search.
Including explicit indication actions is in our view more interesting, as it requires some form of intentionality and is necessary for actually locating targets.

Furthermore, the indication action has several interesting applications.
One can imagine a task in which target detection is done through some external object detection that is expensive to run.
In such a case, the indication could be used by the agent to convey that it is likely that a target is in view, and it is worth spending resources on running the detection process.

\subsection{Position, Movement and Search Space Dimensionality}

To aid our agents with navigating, we chose to give them access to their current position at each time step.
An agent can generally not localize itself with just an image of an unseen environment.
Furthermore, the position is needed to index the spatial memory we chose to use.

An issue with including the position of the agent in its observations is that it becomes reliant on a certain search space shape.
If our agent is trained in a \(10 \times 10\) search space it will not be able to handle smaller or larger spaces.
With just image observations, the agent can be deployed in search spaces of arbitrary shape.

Another aspect to consider is whether the position can be provided to the agent.
It seems reasonable to assume that the position can be provided to the agent.
In certain scenarios it is possible to determine the global position of an agent with some sensor (e.g. GPS).
If how each action moves the agent is well-defined, the relative position can be used instead of the absolute one (e.g. motor controlled pan and tilt camera).

There are alternatives to giving the agent its position that achieve similar effects.
By having the agent observe its last action \(a_{t-1}\) it can learn to navigate relative to its starting position.
This approach is used in~\cite{mirowski_learning_2017}.

The action space we chose in this work contains four different moving actions that change the position of the agent in two dimensions.
This is reasonable for several real-world search tasks such as search and rescue with a UAV, where the actions correspond to translations in each cardinal direction.
Another example is surveillance with a pan-tilt-camera, where the actions could correspond to pitch and yaw rotations.
However, many vision systems have more degrees of freedom.
Camera sensors often have zoom functionality, which in practice requires additional actions for zooming in and out, and adds an extra dimension to the search space.
Similarly, if an agent is able to both move and look around we get additional dimensions to consider.

Nothing suggests that our two approaches would be unable to handle more than two dimensions.
The temporal memory simply has to encode an additional component of the agent's position.
The spatial memory can store a higher dimensional feature map.
Its read operation can be modified to use convolutions in higher dimensions, as noted by \cite{parisotto_neural_2017}.

\subsection{Implications of Reward Signal}

Finally, our reward signal assumes certain knowledge of the scenes:
The environment (or the provider of the reward signal) has to keep track of visited positions.
It also has to know where targets are located -- even when they are not visible.
While the reward is not needed when acting according to the policy, it is needed when learning.
This has implications for what kind of environments can be used for learning.
One can imagine a scenario where the locations of targets are not known, but a black-box system is responsible for detecting them and providing positive feedback to the RL agent.
In this case, the bonus for moving towards targets is not computable.

In the terrain environment, we found that the additional reward bonuses where required for convergence to good policies.
One reason for this could be that it is difficult to interpret compared to the other environment.
With a small bonus for desired behavior, reinforcements are more frequent and may aid with picking up subtle patterns.

However, a specialized reward like ours risks introducing bias that inhibit learning by leading to poor local optima.
One example of such a bias is the bonus reward for moving towards targets.
In the travelling salesman problem, always picking the nearest node unvisited node does not yield optimal paths in general.
Several works argue for minimizing such biases, as learning systems can sometimes find better solutions without them~\cite{hessel_inductive_2019}.
There is often a trade-off between strong bias/fast learning and weak bias/more general agents.
For our set of environments, this particular reward seems to work well, but this might not be the case for search in other types of scenes.

\subsection{Neural Network Architecture and Memory}

The neural network architecture used in this work consists of a CNN, an RNN and two MLPs for estimating the policy and value function.
With the exception of a few educated selections, the network parameters are somewhat arbitrary.
The space of possible architecture parameters is huge, and infeasible to search exhaustively.
There is certainly the possibility that some part of our architecture is a capacity bottleneck for search performance.

Using a CNN is standard when encoding images, and using the encoding to select actions has in a way become synonymous with deep RL.
The choice of CNN 
We have chosen to use the same architecture as ~\cite{mnih_playing_2013}, as it is likely to be sufficient for the level of detail in our environments.
Furthermore, this particular CNN architecture has become common in deep RL literature.
Still, it is possible that another CNN architecture could have yielded better search performance.

The choice of policy and value MLPs is also somewhat arbitrary.
We found that small tweaks did not affect performance much.
Some have reported that a deeper value network yields better performance in some cases~\cite{andrychowicz_what_2020}.
This could be investigated further.

Instead, emphasis has been placed on two RNN which serve as the memory of the agent: one spatial, and one temporal.
This follows from the observation that memory is crucial for effective search in structured and moderately large environments.
The memory represents the agent's beliefs of its environment, and the semantics of the memory constrain the agent's ability to achieve its task.
Both memory architectures were chosen to match the desired properties listed in Section~\ref{sec:aim}:
an agent should be able to remember visited positions and their appearance, and use them to prioritize locations where targets are likely to be found.
We argue that both RNN architectures achieve this as both are capable of remembering positions and encoded images.
Whether the architectures are sufficient to enable the agent to learn an optimal policy for the task is another question.

Using an LSTM is common for POMDPs~\cite{hausknecht_deep_2017,mnih_asynchronous_2016,mirowski_learning_2017}, and has the advantage of being general --
it can be used to remember any features over time.
Its disadvantage is that how it remembers is difficult to predict.

The spatial memory seems like an obvious choice for this particular task.
It stores visual features of the explored scene in an explicit structure, and is read using an operation that is suited for such information.
In our experiments we saw that the spatial memory performed better than the temporal one,
but it has some disadvantages as well.

First of all, it relies on the agent knowing its position so that it can index the memory properly.
Secondly, the feature map of the memory grows with the size of the search space.
This is not an inherent feature of the LSTM (although an LSTM may need to grow to be able to act well in large search spaces).

%~\cite{oh_role_2004}

\subsection{Reinforcement Learning for Visual Search}

It is worth considering whether using a learning agent is suitable for visual search problems.
Several of the works which we cover in Section~\ref{sec:relatedwork} utilize other solution methods with apparent success.
Such methods can sometimes be probably correct and efficient.
It is not obvious that RL is a good choice, and it has even been shown experimentally to struggle with generalizing to unseen samples in certain navigation scenarios~\cite{dhiman_critical_2019}.
One could imagine that it is possible to compute an optimal strategy for certain environments.

The perspective we take with this work is that non-learning solution methods may offer these guarantees, but designing such approaches for every search problem is laborious.
The characteristics of environments can vary considerably which may drastically affect how a manual approach is implemented.
Interpreting and utilizing patterns in arbitrary scenes may even be difficult for us humans.
If we cannot understand patterns ourselves, we can not expect to communicate how to search to machines.
Furthermore, there have been several examples of learning systems surpassing humans in specific tasks~\cite{silver_mastering_2016,vinyals_grandmaster_2019}.

These two points alone are strong arguments for exploring whether learning systems can be good searchers.
We would argue that RL is a suitable framework for implementing such learning systems.
Our approach seems to be able to utilize environment appearance to search more efficiently.
It is less clear whether the behavior is close to optimal or not.
In addition to the points above about neural network architecture and reward signal, it is not known whether policy gradient methods converge to globally optimal policies~\cite{agarwal_optimality_2020}.
More work is needed to evaluate the approaches ability to learn optimal search behavior.

\subsection{Replicability, Reliability and Validity}

As discussed in Section~\ref{sec:theory-evaluation}, reproducibility is a problem in current deep RL research.
This comes with the risk of stagnating the field by inhibiting correct interpretation of results~\cite{henderson_deep_2018}.
In this work, we have taken certain precautions to avoid such problems.

For each experiment, we have collected results across at least 3 runs.
While several sources state that as many as 10 runs are needed for significant results~\cite{colas_hitchhikers_2019,agarwal_deep_2022},
such undertakings are not feasible without sufficient compute resources.
Our hope is that 3 runs gives a sense for the variance involved in our approach.
However, more runs would be preferred.

We have placed emphasis on evaluating our approach on samples that have not been seen previously.
This ensures that our agents have not simply remembered the training levels, but actually generalized to unseen samples.
One issue we would like to note is that we have used a relatively small number of test samples when measuring search performance.
This is to make it feasible to collect results for human searchers.
Ideally, we would have tested on a larger set.

The three metrics we have used (success rate, average search path length, and SPL) all offer different insights into the performance of agents.
They should not be used as absolute measures, but rather to compare different agents in the same environments.
The SPL metric has been used in several previous works for navigation tasks~\cite{anderson_evaluation_2018,yang_visual_2018}.
SPL considers both search path length, shortest path length and whether the agent was successful in each episode,
and is useful as a measure for the quality of the search paths chosen by an agent.

However, SPL is not a perfect metric. As noted by \cite{batra_objectnav_2020}, it fails to consider the fact that some failures are less of a failure than others.
For example, an agent might have been close to discovering a target at some time step but missed it.
It might be desirable to give a higher score to such search paths than ones that are never close to targets.
The binary success indicator introduces high variance into the metric.
They also note that it is not suitable for comparison across different datasets, as obtaining high SPL scores is more difficult for short paths.
All of our agents have achieved relatively high success rates, so the issue of failure does not seem like a major concern.
We have also only compared SPL within the same datasets.

Finally, there is a possibility that there are errors in our implementation.
We have tested our PPO implementation against standard tasks, but there is still the possibility of errors in the algorithm, environments or evaluation.
We have provided source code for our implementation and invite others to reproduce or scrutinize our results.

\subsection{Source Criticism}

Deep RL is a relatively new research field, with lots of progress in recent years.
This coupled with known reproducibility problems of published work discussed in Section~\ref{sec:theory-evaluation} means that being critical of sources is of particular importance.

The used sources are all exclusively scientific literature or textbooks.
The textbooks used (\cite{russell_artificial_2021}, \cite{sutton_reinforcement_2018} and \cite{goodfellow_deep_2016}) are well-known and themselves well-sourced.
Most of the scientific literature is published in peer-reviewed journals.
A few of the referenced works are preprints that have not been through a standard peer review process.
All of these sources are well cited and can be relied on with some confidence.
An attempt has been made to find several references each piece of information used in our theory and methodology.

\section{The work in a wider context}
\label{sec:discussion-wider}

The capabilities of artificial intelligence (AI) are growing, and it is having a progressively wider societal impact.
This progress is expected to continue, and several experts are of the opinion that human-level AI may arrive in this century~\cite{russell_provably_2022}.
Such an intelligence may be of great benefit.
AI could help solve some of today's greatest environmental, societal and economic problems~\cite{vinuesa_role_2020}.

Crucially, it can also have a negative impact if not designed properly.
A common definition of an intelligent system is ones that acts rationally by perceiving and acting in order to maximally achieve its objective~\cite{russell_provably_2022}.
An sufficiently intelligent agent with malicious capabilities and the wrong objective can pose a great danger.
This is a major concern among many researchers and the broader public~\cite{russell_research_2015,brundage_malicious_2018,vinuesa_role_2020,russell_provably_2022}

The system presented in this work also exemplifies the duality in utility of AI systems to some extent.
A system that can search efficiently by utilizing environment cues has positive applications in helping robots, rescue scenarios, fire detection, etc.
It can also be used for malicious purposes, such as autonomous weapons and mass surveillance.

This poses the question of how to avoid AI systems having negative impact on the world.
Following the above definition of an intelligent system, such scenarios can be seen as a failures of \textit{value alignment}~\cite{russell_provably_2022}.
We have to avoid giving intelligent agents purposes that do not align with our own.

In the context of RL, we have to ensure that the reward signal an agent acts according to aligns with our purposes as humans.
Before a system like ours is deployed it must be verified to act according to this principle, or lack the capability to cause harm.
%As the authors of this work, we explicitly condemn its use for malicious use cases.
