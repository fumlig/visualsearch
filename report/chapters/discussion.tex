\chapter{Discussion}
\label{cha:discussion}

This chapter contains the following sub-headings.

\section{Results}
\label{sec:discussion-results}

% Are there anything in the results that stand out and need be
% analyzed and commented on? How do the results relate to the
% material covered in the theory chapter? What does the theory
% imply about the meaning of the results? For example, what
% does it mean that a certain system got a certain numeric value
% in a usability evaluation; how good or bad is it? Is there
% something in the results that is unexpected based on the
% literature review, or is everything as one would theoretically
% expect?

\subsection{Evaluation of Search Quality}

To evaluate the search quality, we first look at the average search path length of each agent.
It is useful to compare the path length to the number of points in each search space.
The gaussian and terrain environments contain \(10 \times 10 = 100\) searchable positions, and the camera environment contains \(10 \times 20 = 200\) searchable positions.
A search path length above this suggests many revisited positions.
The exhaustive baseline policy provides a good measure for what sort of path lengths can be achieved when only avoiding revisiting positions.
Both the random and greedy baseline policies select search paths that are on average much longer than the number of positions, suggesting several redundant steps.

Notably, our temporal memory agent chooses search paths that are substantially longer than those selected by our spatial memory agent and human searchers.
This is especially true for the gaussian and terrain environments, which suggests several redundant steps.
Our spatial memory agent achieves better average lengths in these two environments.

We see that search paths are particularly short in the camera environment,
despite its larger search space.
This has two reasons:
First, targets may be visible in the periphery from a large set of camera positions.
Once targets are visible, it is simpler for an agent to select actions that center them.
Second, targets are not distributed uniformly across many samples of the camera environment.
They are most commonly found near the horizon.
In the gaussian and terrain environments, where the distribution is approximately uniform over many samples, the search path lengths are closer to the number of positions in the search space.

Our temporal memory agent performs very well in the camera environment.
We suspect that this is due to the fact that less emphasis is placed on remembering over many time steps.
Search in this environment can be more reactive.
When inspecting the search patterns used by our temporal memory agent in this environment, we see that it has learned to alternate between scanning across the horizon and locating targets in its periphery.

The baselines are unable to utilize this imbalance in target distributions.
The exhaustive policy is an exception, where the deterministic ordering of actions happened to favor that distribution. 

Worth noting is that revisiting positions is not always the wrong choice.
One such example is when the current position of the agent is directly between two targets.
The optimal path to find both of these is to first go to any one of them, and then to go back towards the other one by revisiting positions.

Next, we consider the SPL score of each agent.
When averaged over many episodes, the SPL score should be a better measure of search path quality than average length, as it also accounts for how close each path is to the optimal one.
From the definition of the SPL metric in Equation \ref{equ:spl}, we that it favors search paths whose length are close to the shortest one in each sample.

Both our two approaches and human searchers achieve similar SPL scores in all three environments.
While the exhaustive baseline policy achieves average path lengths that are close to those of our two approaches and human searchers, its lower SPL score suggests that it is less successful in finding close to optimal solutions.
In all three environments, one of our approaches achieve a higher SPL score,
although we would not consider this significant due to the large variance.

Under these metrics, both approaches achieve comparable performance to that of a human searcher. Overall, it would seem like our spatial memory approach is most suited where large search spaces have to be explored strategically, while our temporal memory approach is most suited for search problems where search spaces can be explored reactively.

\subsection{Applicability to Real-world Search Scenarios}

\begin{itemize}
    \item Do the environments test realistic tasks?
    \item Is the needed number of training samples realistic?
    \item What happens when detection is more difficult?
    \item Real-world search has higher variance.
    \item Has the agent simply seen the full distribution, i.e. is it bridging any generalization gap?
\end{itemize}

\section{Method}
\label{sec:discussion-method}

% This is where the applied method is discussed and criticized.
% Taking a self-critical stance to the method used is an
% important part of the scientific approach.
% 
% A study is rarely perfect. There are almost always things one
% could have done differently if the study could be repeated or
% with extra resources. Go through the most important
% limitations with your method and discuss potential
% consequences for the results. Connect back to the method
% theory presented in the theory chapter. Refer explicitly to
% relevant sources.
% 
% The discussion shall also demonstrate an awareness of methodological
% concepts such as replicability, reliability, and validity. The concept
% of replicability has already been discussed in the Method chapter
% (\ref{cha:method}). Reliability is a term for whether one can expect
% to get the same results if a study is repeated with the same method. A
% study with a high degree of reliability has a large probability of
% leading to similar results if repeated. The concept of validity is,
% somewhat simplified, concerned with whether a performed measurement
% actually measures what one thinks is being measured. A study with a
% high degree of validity thus has a high level of credibility. A
% discussion of these concepts must be transferred to the actual context
% of the study.
% 
% The method discussion shall also contain a paragraph of
% source criticism. This is where the authorsâ€™ point of view on
% the use and selection of sources is described.
% 
% In certain contexts it may be the case that the most relevant
% information for the study is not to be found in scientific
% literature but rather with individual software developers and
% open source projects. It must then be clearly stated that
% efforts have been made to gain access to this information,
% e.g. by direct communication with developers and/or through
% discussion forums, etc. Efforts must also be made to indicate
% the lack of relevant research literature. The precise manner
% of such investigations must be clearly specified in a method
% section. The paragraph on source criticism must critically
% discuss these approaches.
% 
% Usually however, there are always relevant related research.
% If not about the actual research questions, there is certainly
% important information about the domain under study.

\subsection{Reinforcement Learning for Visual Search}

\begin{itemize}
    \item Is it a good idea to begin with?
    \item Other methods can potentially provide more guarantees.
    \item Connect back to \cite{dhiman_critical_2019}, who find that DRL is not able to exploit environment information to navigate more efficiently.
\end{itemize}

\subsection{Characteristics of Experimental Environments}

The three environments we have used to train and test searching agents are all quite different.

Notably, the camera environment does not follow the same 

The image observations are low-dimensional and likely not sufficient for real-world scenarios\dots

Position observations are\dots
In many realistic scenarios it is possible to determine the global position of an agent (GPS, pan/tilt, etc.).
If how each action moves the agent is well-defined, the relative position can be used instead of the absolute one.

The action space is realistic for many real-world search tasks,
such as search and rescue with a UAV, where the actions correspond to translations in each cardinal direction,
and surveillance with a pan-tilt-camera, where the actions correspond to pitch and yaw rotations.

In some cases, like human eye movements, \dots


\begin{itemize}
    \item Reward signal requires good knowledge of scene.
    \item As long as we have labelled scenes, we can use the reward.
\end{itemize}

\subsection{Evaluation and Comparison of Agents}

\begin{itemize}
    \item Is the comparison between spatial and temporal memory meaningful?
    \item Would have been nice with a comparison to a close-to-optimal rule-based baseline in each environment.
    \item Discussion about metrics, like SPL.
    \item What does the difference in SPL mean?
\end{itemize}

%Batra et al.~\cite{batra_evaluation_2020} revisit the problem of evaluating embodied navigation agents.
%They note some issues with the SPL metric.
%It fails to consider the fact that some failures are less of a failure than others.
%Some failures might in fact be close to reaching the goal while some fail completely.
%The binary success introduces high variance in average SPL computation.
%Furthermore, SPL is not particularly suitable for comparison across different datasets,
%as obtaining a high SPL is more difficult for short paths than for long paths.
%They suggest that SPL should be replaced by some metric that takes these issues into account.
%However, to our knowledge such a metric is yet to be proposed and widely adopted.

\subsection{Replicability, Reliability and Validity}

\begin{itemize}
    \item We have published source code.
    \item Reinforcement learning is inherently unreliable.
    \item Stochasticity necessary to avoid getting stuck, leads to less precise paths.
    \item Possibility of implementation errors.
    \item Low number of runs, quite high variance.
    \item Few samples during testing.
\end{itemize}

\subsection{Source Criticism}

\begin{itemize}
    \item We have based most of our work on known theory
\end{itemize}

\section{The work in a wider context}

% There must be a section discussing ethical and societal
% aspects related to the work. This is important for the authors
% to demonstrate a professional maturity and also for achieving
% the education goals. If the work, for some reason, completely
% lacks a connection to ethical or societal aspects this must be
% explicitly stated and justified in the section Delimitations in
% the introduction chapter.
% 
% In the discussion chapter, one must explicitly refer to sources
% relevant to the discussion.

Furthermore, automating manual tasks has societal implications.


\subsection{Ethics of Autonomous Agents}

Russell et al.~\cite{russell_priorities_2015}

\cite{vinuesa_sustainable_2020}


\cite{russell_ethics_2015}

\cite{russell_beneficial_2022}

\cite{brundage_malicious_2018}

Several applications are positive, like search-and-rescue and fire detection.
In such scenarios, a short search time can be the difference between life and death.
Artificial intelligence systems have proven before that they are capable to outperform humans in specific tasks~\cite{silver_alphago_2016,vinyals_alphastar_2019}, and they could potentially do the same in visual search.

However, there automated visual search systems also have destructive applications.
A major concern of artificial intelligence researchers and the broader public today is the implications of autonomous systems for 

The widely discussed topic of autonomous weapon systems

% https://people.eecs.berkeley.edu/~russell/research/LAWS.html

As the authors of this work, we explicitly condemn its use for lethal weaponry.

\subsection{Trustworthiness for Safety-Critical Applications}

How can we trust an autonomous visual search system?
For safety critical applications, certain requirements when it comes to robustness are needed.
Things to consider before deploying a system for such a scenario include verification, validity, security, control

% what can this be used for? good and bad things?

%While automated search systems have many positive uses, like XXX, there are certainly other use cases that could be considered negative.
%Mass surveillance, XXX, are both very relevant today.



%It is worth considering whether using a learning agent like this is suitable for this task.
%One could imagine that it is possible to compute an optimal strategy for certain environments.
%However, this quickly falls apart.
%The dynamics of environments can vary considerably which may drastically affect how a manual approach is implemented.

%Another thing worth discussing is the possibility of combining manual search method with reinforcement learning. One could imagine combining a frontier based approach with a learning approach.

\rule{5cm}{1pt}

\begin{itemize}
    \item Evaluate generalization and viability for real world training sets. 
    \item Discuss advantages and disadvantages of RL for this task.
    \item The less bias we introduce, the more general the method has the potential to be.
    \item However, we could make the observation space clearer (for example, give the agent visited position directly).
    \item Search space size and its impact on performance.
    \item Larger search space could correspond to larger area or higher granularity.
    \item Is it better than exhaustive search?
    \item Is it better than a human?
    \item Not just visual search -- also a practical example of RL for exploration, generalization, \dots
    \item Stacked LSTM was unstable, even with dropout. Could it have remembered more?
    \item Differences in number of weights for different environment sizes\dots
    \item Comparison to exploration problems, other solutions methods?
    \item Camera movements in three dimensions: method could be expanded with higher-dimensional convolutions. 
    \item Discuss if indication is necessary
    \item Look into epsilon greedy
    \item Can gamma be used for quick exploration
    \item Rephrase target?
    \item Could give bad vibes...
    \item Greedy and random baselines have unfair advantage, but this is fine since we are not putting our method at an advantage.
    \item The map approach seems to scale better - with more time we could have trained for a larger search space.
    \item Neither scales well (weights in map scale increase quadratically with search space size -- fine for fixed positions, bad for movable cameras).
    \item If we find that bonuses speed up training but converge to poor solutions, we could discuss reward shaping. Remove bonus after convergence.
    \item The scene characteristics matter - in some cases, amortized probability of targets is not uniform, and patterns are more complex. We need more types of environments.
    \item Discuss the generality of the proposed approach - what types of cues can it pick up? Spatial and semantic relationships, non-uniform probabilities, etc. 
    \item An ego-centric architecture might be more general, does not need the position and can work over larger territories (although not take whole territory into account).
    \item Show some search paths and discuss them?
    \item The more specialized the reward, the higher the risk that it is good for some environments and bad for some. We saw with the unspecialized reward that it did not work well for the terrain environment. If we end up using only a specialized reward, discuss cases when it would not be able to find an optimal solution.
    \item Discuss lack of completion action (as suggested in related work).
    \item Talk about optimal paths and SPL (not realistic).
    \item Should have looked at more interesting environments (at least for optimal paths).
    \item Hopefully the camera environment is interesting.
    \item Early experiments with stacking multiple LSTM layers lead to heavy overfitting and unstable learning curves.
    \item The difficulty of each environment is related to the potential region targets can appear in. We can measure this in the environments themselves.
    \item We can visualize attention as in "A critical investigation of deep reinforcement learning for navigation"
    \item We have illustrated that reinforcement learning agents overfit to training samples and must be tested on separate sets.
    \item SPL metric
\end{itemize}

% learning actions, recognition and localization simultaneously feels in itself interesting
% how do we interpret such a model? are there papers for visualizing reinforcement learning weights
