\chapter{Discussion}
\label{cha:discussion}

In this chapter, we reflect the obtained results and critically discuss and evaluate the chosen method.
We also discuss the work from a broader perspective (Section~\ref{sec:discussion-wider})

\section{Results}
\label{sec:discussion-results}



\subsection{Reflection on Search Performance}

To evaluate the search quality, we first consider the at the average search path length of each agent.
It is useful to compare the path length to the number of points in each search space.
The gaussian and terrain environments contain \(10 \times 10 = 100\) searchable positions, and the camera environment contains \(10 \times 20 = 200\) searchable positions.
A search path length above this suggests many revisited positions.
The exhaustive baseline policy provides a good measure for what sort of path lengths can be achieved when only avoiding revisiting positions.
Both the random and greedy baseline policies select search paths that are on average much longer than the number of positions, suggesting several redundant steps.
This is further reinforced by the sample search paths in Figures \ref{fig:path-random}, \ref{fig:path-greedy} and \ref{fig:path-exhaustive}.

Notably, our temporal memory agent chooses search paths that are substantially longer than those selected by our spatial memory agent and human searchers.
This is especially true for the gaussian and terrain environments, which suggests several redundant steps.
Our spatial memory agent achieves better average lengths in these two environments.

We see that search paths are particularly short in the camera environment,
despite its larger search space.
This has two reasons:
First, targets may be visible in the periphery from a large set of camera positions.
Once targets are visible, it is simpler for an agent to select actions that center them.
Second, targets are not distributed uniformly across many samples of the camera environment.
They are most commonly found near the horizon.
In the gaussian and terrain environments, where the distribution is approximately uniform over many samples, the search path lengths are closer to the number of positions in the search space.

Our temporal memory agent performs very well in the camera environment.
We suspect that this is due to the fact that less emphasis is placed on remembering over many time steps.
Search in this environment can be more reactive.
When inspecting the search patterns used by our temporal memory agent in this environment, we see that it has learned to alternate between scanning across the horizon and locating targets in its periphery.
The baselines are unable to utilize this imbalance in target distributions.
The exhaustive policy is an exception, where the deterministic ordering of actions happened to favor that distribution. 

By looking at some sample search paths, we see that our two approaches both seem to utilize scene appearance to guide their search.
However, they also tend to revisit positions, and it is not clear that this is good.
Figures \ref{fig:path-lstm} and \ref{fig:path-map} illustrate this well.
Some of these revisits could be attributed to the stochastic policy.
Making the policy completely deterministic by selecting the action with the highest probability is not a solution, as it tends to lead to dead states or loops where the agent does not progress.

Worth noting is that revisiting positions is not always the wrong choice.
One such example is when the current position of the agent is directly between two targets.
The optimal path to find both of these is to first go to any one of them, and then to go back towards the other one by revisiting positions.

Next, we consider the SPL score of each agent.
When averaged over many episodes, the SPL score should be a better measure of search path quality than average length, as it also accounts for how close each path is to the optimal one.
From the definition of the SPL metric in Equation \ref{equ:spl}, we that it favors search paths whose length are close to the shortest one in each sample.
Finding the shortest path in a search scenario such as the one we consider here is not feasible.
However, it still serves as a useful normalization method as it is an indication of how difficult each sample is to solve.

Both our two approaches and human searchers achieve similar SPL scores in all three environments.
While the exhaustive baseline policy achieves average path lengths that are close to those of our two approaches and human searchers, its lower SPL score suggests that it is less successful in finding close to optimal solutions.
In all three environments, one of our approaches achieve a higher SPL score,
although we would not consider this significant due to the large variance.

Under these metrics, both approaches achieve comparable performance to that of a human searcher. 
Overall, it would seem like our spatial memory approach is most suited where large search spaces have to be explored strategically, while our temporal memory approach is sufficient for search problems where search spaces can be explored reactively.

\subsection{Scalability To Different Search Space Sizes}

In Section~\ref{sec:shape} we saw that both of our approaches achieve comparable performance on the \(10 \times 10\) search space.
Both quickly converge towards a high success rate and reasonable search path lengths.
The spatial memory is consistently capable of finding all targets in less steps than the number of searchable positions, and the temporal memory is not far behind.

For larger search spaces, the slight difference is accentuated.
The temporal memory agent converges to search paths that are longer than needed, suggesting that it is not capable of remembering visited locations.
In the \(20 \times 20\) search space, it seems to have converged to path lengths of around 800, and has a success rate of less than 50\%.
By the end of training, the search paths of the spatial memory agent are near the total number of positions and it has a success rate of around 90\%.
Furthermore, the training does not seem to have fully converged.

It seems reasonable to state that the spatial memory scales better to large search spaces in terms of performance, when compared to the temporal memory.
Searching efficiently in large search spaces places extra emphasis on remembering what has already been explored.
From our results, it would seem like an LSTM struggles with remembering such precise information over many time steps.
Similar results where presented by \cite{oh_control_2016}.

An hypothesis is that our temporal memory agent would handle large state spaces better with more parameters.
We experimented with stacking multiple LSTM layers, but found that it led to overfitting and unstable learning, even with dropout.
Further analysis is needed to see if such augmentations can make it more comparable to the spatial memory in terms of performance.

Even though we have investigated how our approaches scale to larger search spaces,
it should be noted that realistic search spaces are often considerably larger.
This poses a problem for both of our approaches, as the number of trainable parameters scales with the search space size in both of them.
One solution is to keep search spaces small:
Large (and continuous) spaces could be discretized into smaller ones at the cost of precision.
Limit the need for memory could be a viable solution for certain search problems..
In the case of the temporal memory, it might be sufficient to only remember recent interactions.
Similarly, in the case of the spatial memory, a possibility is to use an ego-centric recurrent feature map and only remember proximous interactions~\cite{parisotto_neural_2017}.

\subsection{Generalizing from Limited Training Sets}

From the results in Section~\ref{sec:sample}, we see that both of our approaches can overfit to small training sets.
This is in line with what is reported by \cite{cobbe_quantifying_2019}, \cite{cobbe_leveraging_2020} and \cite{zhang_study_2018}.
Past a certain number of interactions with small a small training set, performance on a held out test set decreases.
This effect is well known in supervised learning, but important to keep in mind for reinforcement learning.
If any sort of generalization is expected from our agents, they should be tested on held out samples.

Interestingly, our spatial memory agent seems to be better at generalizing from a limited number of samples --
for 500 and 1000 samples, its performance on the test set does not decrease as severely as our temporal memory agent.
Furthermore, the test performance of our spatial memory agent is strictly better than that of our temporal memory agent in all training set sizes.
This suggests that the spatial memory provides a more suitable inductive bias for the considered problem.

It should be stated that these numbers are not representative of all search problems.
The number of samples needed to generalize to other search tasks may be different than for the terrain environment we used in Section~\ref{sec:sample}.
While the terrain environment exhibits more subtle patterns than the gaussian environment, it is free from noise and is relatively low in variance compared to realistic scenarios.
However, the results do offer insights into how these two architectures compare when it comes to generalization from a limited number of samples.

\section{Method}
\label{sec:discussion-method}

There are several aspects of the method chosen that are worth discussing.

\subsection{Observations and Detections}

The observations in the three environments in this work contain the position and an image of the current view.
It seems reasonable to assume that the position can be provided to the agent.
In certain scenarios it is possible to determine the global position of an agent with some sensor (e.g. GPS).
If how each action moves the agent is well-defined, the relative position can be used instead of the absolute one (e.g. motor controlled pan and tilt camera).

In many cases, a \(64 \times 64\) images are not sufficient to convey the full meaning of a scene.
While we explicitly delimit ourselves from difficult detection problems in this work,
detection is likely to be difficult in most real-world search scenarios.
Further investigations are needed to evaluate how viable our approach is for more realistic scenes,
where variance is higher and it is more difficult to interpret image observations.

Related to the detection problem is our choice of including an indication action.
It could be argued that explicitly indicating when targets are in view is redundant.
Indications could instead happen implicitly when targets are in view.
This is true if the focus is only on how environment cues can be used to guide visual attention during search.
Including explicit indication actions is in our view more interesting, as it requires some form of intentionality and is necessary for actually locating targets.

Furthermore, the indication action has several interesting applications.
One can imagine a task in which target detection is done through some external object detection that is expensive to run.
In such a case, the indication could be used by the agent to convey that it is likely that a target is in view, and it is worth spending resources on running the detection process.

\subsection{Actions and Search Space Dimensionality}

The action space we chose in this work contains four different moving actions that change the position of the agent in two dimensions.
This is reasonable for several real-world search tasks such as search and rescue with a UAV, where the actions correspond to translations in each cardinal direction.
Another example is surveillance with a pan-tilt-camera, where the actions could correspond to pitch and yaw rotations.

Many vision systems have more degrees of freedom.
Camera sensors often have zoom functionality, which in practice requires additional actions for zooming in and out, and adds an extra dimension to the search space.
Similarly, if an agent is able to both move and look around we get additional dimensions to consider.

Nothing suggests that our two approaches would be unable to handle more than two dimensions.
The temporal memory simply has to encode an additional component of the agent's position.
The spatial memory can store a higher dimensional feature map.
Its read operation can be modified to use convolutions in higher dimensions, as noted by ~\cite{parisotto_neural_2017}.

\subsection{Implications of Reward Signal}

Finally, our reward signal assumes certain knowledge of the scenes:
The environment (or the provider of the reward signal) has to keep track of visited positions.
It also has to know where targets are located -- even when they are not visible.
While the reward is not needed when acting according to the policy, it is needed when learning.
This has implications for what kind of environments can be used for learning.
One can imagine a scenario where the locations of targets are not known, but a black-box system is responsible for detecting them and providing positive feedback to the RL agent.
In this case, the bonus for moving towards targets is not computable.

In the gaussian environment, we found that the additional reward bonuses where required for convergence to good policies.
One reason for this could be that it is difficult to interpret compared to the other environment.
With a small bonus for desired behavior, reinforcements are more frequent and may aid with picking up subtle patterns.
However, a specialized reward like ours risks introducing bias that inhibit learning by leading to poor local optima.
Several works argue for minimizing such biases, as learning systems can sometimes find better solutions without them~\cite{hessel_inductive_2019}.
There is often a trade-off between strong bias/fast learning and weak bias/more general agents.
For our set of environments, this particular reward seems to work well, but this might not be the case for search in other types of scenes.

%\subsection{Choice of Neural Network Architecture}

% todo: oh_role_2004

%We have compared two neural network architectures from the perspective of a particular visual search problem.
%The choice of architectures and their hyperparameters may seem somewhat arbitrary.

%The CNN architecture was chosen for we use is widely used and likely sufficient for %the visuals in our environments.

\subsection{Reinforcement Learning for Visual Search}

It is worth considering whether using a learning agent is suitable for visual search problems.
Several of the works which we cover in Section ~\ref{sec:relatedwork} utilize other solution methods with apparent success.
Such methods can sometimes be probably correct and efficient.
It is not obvious that RL is a good choice, and it has even been shown experimentally to struggle with generalizing to unseen samples in certain navigation scenarios~\cite{dhiman_critical_2019}.
One could imagine that it is possible to compute an optimal strategy for certain environments.

The perspective we take with this work is that non-learning solution methods may offer these guarantees, but designing such approaches for every search problem is laborious.
The characteristics of environments can vary considerably which may drastically affect how a manual approach is implemented.
Interpreting and utilizing patterns in arbitrary scenes may even be difficult for us humans.
If we cannot understand patterns ourselves, we can not expect to communicate how to search to machines.
Furthermore, there have been several examples of learning systems surpassing humans in specific tasks~\cite{silver_mastering_2016,vinyals_grandmaster_2019}.

These two points alone are strong arguments for exploring whether learning systems can be good searchers.
We would argue that RL is a suitable framework for implementing such learning agents.
Our approach seems to be able to utilize environment appearance to search more efficiently.
It is less clear if their behavior is close to optimality or not.

\subsection{Replicability, Reliability and Validity}

As discussed in Section~\ref{sec:theory-evaluation}, reproducibility is a problem in current deep RL research.
This comes with the risk of stagnating the field by inhibiting correct interpretation of results~\cite{henderson_deep_2018}.
In this work, we have taken certain precautions to avoid such problems.

For each experiment, we have collected results across at least 3 runs.
While several sources state that as many as 10 runs are needed for significant results~\cite{colas_hitchhikers_2019,agarwal_deep_2022},
such undertakings are not feasible without sufficient compute resources.
Our hope is that 3 runs gives a sense for the variance involved in our approach.
However, more runs would be preferred.

We have placed emphasis on evaluating our approach on samples that have not been seen previously.
This ensures that our agents have not simply remembered the training levels, but actually generalized to unseen samples.
One issue we would like to note is that we have used a relatively small number of test samples when measuring search performance.
This is to make it feasible to collect results for human searchers.
Ideally, we would have tested on a larger set.

The three metrics we have used (success rate, average search path length, and SPL) all offer different insights into the performance of agents.
They should not be used as absolute measures, but rather to compare different agents in the same environments.
The SPL metric has been used in several previous works for navigation tasks~\cite{anderson_evaluation_2018,yang_visual_2018}.
SPL considers both search path length, shortest path length and whether the agent was successful in each episode,
and is useful as a measure for the quality of the search paths chosen by an agent.

However, SPL is not a perfect metric. As noted by \cite{batra_objectnav_2020}, it fails to consider the fact that some failures are less of a failure than others.
For example, an agent might have been close to discovering a target at some time step but missed it.
It might be desirable to give a higher score to such search paths than ones that are never close to targets.
The binary success indicator introduces high variance into the metric.
They also note that it is not suitable for comparison across different datasets, as obtaining high SPL scores is more difficult for short paths.
All of our agents have achieved relatively high success rates, so the issue of failure does not seem like a major concern.
We have also only compared SPL within the same datasets.

Finally, there is a possibility that there are errors in our implementation.
We have tested our PPO implementation against standard tasks, but there is still the possibility of errors in the algorithm, environments or evaluation.
We have provided source code for our implementation and invite others to reproduce or scrutinize our results.

\subsection{Source Criticism}

\begin{itemize}
    \item Some cited papers are preprints.
    \item Several of the cited preprints have actually been published in journals, replace these.
\end{itemize}

\section{The work in a wider context}
\label{sec:discussion-wider}

A major concern of artificial intelligence researchers and the broader public today is the implications of autonomous systems\dots

\begin{itemize}
    \item \cite{russell_research_2015}, \cite{vinuesa_role_2020}, \cite{russell_provably_2022}, \cite{brundage_malicious_2018}
    \item \url{https://people.eecs.berkeley.edu/~russell/research/LAWS.html}
    \item Dangers of lethal autonomous weapon systems
    \item Search-and-rescue and fire detection require high reliability, and a short search time can be the difference between life and death.
    \item Things to consider before deploying a system for such a scenario include verification, validity, security, control, human intervention, etc.
\end{itemize}

``As the authors of this work, we explicitly condemn its use for lethal weaponry.''
