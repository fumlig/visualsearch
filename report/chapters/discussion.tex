\chapter{Discussion}
\label{cha:discussion}

In this chapter, we discuss the chosen method (Section~\ref{sec:discussion-method}) and obtained results (Section~\ref{sec:discussion-results}).
We also discuss the work in a wider context (Section~\ref{sec:discussion-wider})

\section{Results}
\label{sec:discussion-results}

In this section, we discuss the results of each of the conducted experiments.

\subsection{Reflection on Search Performance}

To evaluate the search quality, we first consider the at the average search path length of each agent.
It is useful to compare the path length to the number of points in each search space.
The gaussian and terrain environments contain \(10 \times 10 = 100\) searchable positions, and the camera environment contains \(10 \times 20 = 200\) searchable positions.
A search path length above this suggests many revisited positions.
The exhaustive baseline policy provides a good measure for what sort of path lengths can be achieved when only avoiding revisiting positions.
Both the random and greedy baseline policies select search paths that are on average much longer than the number of positions, suggesting several redundant steps.
This is further reinforced by the sample search paths in Figures \ref{fig:path-random}, \ref{fig:path-greedy} and \ref{fig:path-exhaustive}.

Notably, our temporal memory agent chooses search paths that are substantially longer than those selected by our spatial memory agent and human searchers.
This is especially true for the gaussian and terrain environments, which suggests several redundant steps.
Our spatial memory agent achieves better average lengths in these two environments.

We see that search paths are particularly short in the camera environment,
despite its larger search space.
This has two reasons:
First, targets may be visible in the periphery from a large set of camera positions.
Once targets are visible, it is simpler for an agent to select actions that center them.
Second, targets are not distributed uniformly across many samples of the camera environment.
They are most commonly found near the horizon.
In the gaussian and terrain environments, where the distribution is approximately uniform over many samples, the search path lengths are closer to the number of positions in the search space.

Our temporal memory agent performs very well in the camera environment.
We suspect that this is due to the fact that less emphasis is placed on remembering over many time steps.
Search in this environment can be more reactive.
When inspecting the search patterns used by our temporal memory agent in this environment, we see that it has learned to alternate between scanning across the horizon and locating targets in its periphery.
The baselines are unable to utilize this imbalance in target distributions.
The exhaustive policy is an exception, where the deterministic ordering of actions happened to favor that distribution. 

By looking at some sample search paths, we see that our two approaches both seem to utilize scene appearance to guide their search.
However, they also tend to revisit positions, and it is not clear that this is good.
Figures \ref{fig:path-lstm} and \ref{fig:path-map} illustrate this well.
Some of these revisits could be attributed to the stochastic policy.
Making the policy completely deterministic by selecting the action with the highest probability is not a solution, as it tends to lead to dead states or loops where the agent does not progress.

Worth noting is that revisiting positions is not always the wrong choice.
One such example is when the current position of the agent is directly between two targets.
The optimal path to find both of these is to first go to any one of them, and then to go back towards the other one by revisiting positions.

Next, we consider the SPL score of each agent.
When averaged over many episodes, the SPL score should be a better measure of search path quality than average length, as it also accounts for how close each path is to the optimal one.
From the definition of the SPL metric in Equation \ref{equ:spl}, we that it favors search paths whose length are close to the shortest one in each sample.
Finding the shortest path in a search scenario such as the one we consider here is not feasible.
However, it still serves as a useful normalization method as it is an indication of how difficult each sample is to solve.

Both our two approaches and human searchers achieve similar SPL scores in all three environments.
While the exhaustive baseline policy achieves average path lengths that are close to those of our two approaches and human searchers, its lower SPL score suggests that it is less successful in finding close to optimal solutions.
In all three environments, one of our approaches achieve a higher SPL score,
although we would not consider this significant due to the large variance.

Under these metrics, both approaches achieve comparable performance to that of a human searcher. 
Overall, it would seem like our spatial memory approach is most suited where large search spaces have to be explored strategically, while our temporal memory approach is sufficient for search problems where search spaces can be explored reactively.

\subsection{Scalability To Different Search Space Sizes}

In Section~\ref{sec:shape} we saw that both of our approaches achieve comparable performance on the \(10 \times 10\) search space.
Both quickly converge towards a high success rate and reasonable search path lengths.
The spatial memory is consistently capable of finding all targets in less steps than the number of searchable positions, and the temporal memory is not far behind.

For larger search spaces, the slight difference is accentuated.
The temporal memory agent converges to search paths that are longer than needed, suggesting that it is not capable of remembering visited locations.
In the \(20 \times 20\) search space, it seems to have converged to path lengths of around 800, and has a success rate of less than 50\%.
By the end of training, the search paths of the spatial memory agent are near the total number of positions and it has a success rate of around 90\%.
Furthermore, it does not seem to have fully converged.

It seems reasonable to state that the spatial memory scales better to large search spaces in terms of performance, when compared to the temporal memory.
Searching efficiently in large search spaces places extra emphasis on remembering what has already been explored.
From our results, it would seem like an LSTM struggles with remembering such precise information over many time steps.
Similar results where presented by \cite{oh_minecraft_2016}.

\subsection{Generalizing from Limited Training Sets}

From the results in Section~\ref{sec:sample}, we see that both of our approaches can overfit to small training sets.
This is in line with what is reported by \cite{cobbe_procgen_2020}, \cite{cobbe_generalization_2019} and \cite{zhang_overfitting_2018}.
Past a certain number of interactions with small a small training set, performance on a held out test set decreases.
This effect is well known in supervised learning, but important to keep in mind for reinforcement learning.
If any sort of generalization is expected from our agents, they should be tested on held out samples.

Interestingly, our spatial memory agent seems to be better at generalizing from a limited number of samples --
For 500 and 1000 samples, its performance on the test set does not decrease as severely as our temporal memory agent.
Furthermore, the test performance of our spatial memory agent is strictly better than that of our temporal memory agent in all training set sizes.
This suggests that the spatial memory provides a more suitable inductive bias for the considered problem.

It should be stated that these numbers are not representative of all search problems.
The number of samples needed to generalize to other search tasks may be different than for the terrain environment we used in Section~\ref{sec:sample}.
While the terrain environment exhibits more subtle patterns than the gaussian environment, it is free from noise and is relatively low in variance compared to realistic scenarios.
However, the results do offer insights into how these two architectures compare when it comes to generalization from a limited number of samples.

\section{Method}
\label{sec:discussion-method}

% This is where the applied method is discussed and criticized.
% Taking a self-critical stance to the method used is an
% important part of the scientific approach.
% 
% A study is rarely perfect. There are almost always things one
% could have done differently if the study could be repeated or
% with extra resources. Go through the most important
% limitations with your method and discuss potential
% consequences for the results. Connect back to the method
% theory presented in the theory chapter. Refer explicitly to
% relevant sources.
% 
% The discussion shall also demonstrate an awareness of methodological
% concepts such as replicability, reliability, and validity. The concept
% of replicability has already been discussed in the Method chapter
% (\ref{cha:method}). Reliability is a term for whether one can expect
% to get the same results if a study is repeated with the same method. A
% study with a high degree of reliability has a large probability of
% leading to similar results if repeated. The concept of validity is,
% somewhat simplified, concerned with whether a performed measurement
% actually measures what one thinks is being measured. A study with a
% high degree of validity thus has a high level of credibility. A
% discussion of these concepts must be transferred to the actual context
% of the study.
% 
% The method discussion shall also contain a paragraph of
% source criticism. This is where the authorsâ€™ point of view on
% the use and selection of sources is described.
% 
% In certain contexts it may be the case that the most relevant
% information for the study is not to be found in scientific
% literature but rather with individual software developers and
% open source projects. It must then be clearly stated that
% efforts have been made to gain access to this information,
% e.g. by direct communication with developers and/or through
% discussion forums, etc. Efforts must also be made to indicate
% the lack of relevant research literature. The precise manner
% of such investigations must be clearly specified in a method
% section. The paragraph on source criticism must critically
% discuss these approaches.
% 
% Usually however, there are always relevant related research.
% If not about the actual research questions, there is certainly
% important information about the domain under study.

\subsection{Reinforcement Learning for Visual Search}

It is worth considering whether using a learning agent like this is suitable for visual search problems.
Several of the works which we cover in Section ~\ref{sec:relatedwork} utilize other solution methods with apparent success.
Such methods can sometimes be probably correct and efficient.
It is not obvious that RL is a good choice, and it has even been shown experimentally to struggle with generalizing to unseen samples in certain navigation scenarios~\cite{dhiman_critical_2019}.
One could imagine that it is possible to compute an optimal strategy for certain environments.

The perspective we take with this work is that non-learning solution methods may offer these guarantees, but designing such approaches for every search problem is laborious.
The characteristics of environments can vary considerably which may drastically affect how a manual approach is implemented.
Interpreting and utilizing patterns in arbitrary scenes may even be difficult for us humans.
If we cannot understand patterns ourselves, we can not expect to communicate how to search to machines.
Furthermore, there have been several examples of learning systems surpassing humans in specific tasks~\cite{silver_alphago_2016,vinyals_alphastar_2019}.

These two points alone are strong arguments for exploring whether learning systems can be good searchers.
We would argue that RL is a suitable framework for implementing such learning agents.
Our approach seems to be able to utilize environment appearance to search more efficiently.
It is less clear if their behavior is close to optimality or not.

\subsection{Design of Environments}

The three environments we have used to train and test searching agents are all quite different.

Notably, the camera environment does not follow the same 

The image observations are low-dimensional and likely not sufficient for real-world scenarios\dots

Position observations are\dots
In many realistic scenarios it is possible to determine the global position of an agent (GPS, pan/tilt, etc.).
If how each action moves the agent is well-defined, the relative position can be used instead of the absolute one.

The action space is realistic for many real-world search tasks,
such as search and rescue with a UAV, where the actions correspond to translations in each cardinal direction,
and surveillance with a pan-tilt-camera, where the actions correspond to pitch and yaw rotations.

In some cases, like human eye movements, \dots


\begin{itemize}
    \item Reward signal requires good knowledge of scene.
    \item As long as we have labelled scenes, we can use the reward.
    \item However, we could make the observation space clearer (for example, give the agent visited position directly).
    \item The difficulty of each environment is related to the potential region targets can appear in. We can measure this in the environments themselves.
    \item The scene characteristics matter - in some cases, amortized probability of targets is not uniform, and patterns are more complex. We need more types of environments.
    \item The more specialized the reward, the higher the risk that it is good for some environments and bad for some. We saw with the unspecialized reward that it did not work well for the terrain environment. If we end up using only a specialized reward, discuss cases when it would not be able to find an optimal solution.
    \item Discuss lack of completion action (as suggested in related work).

\end{itemize}


\subsection{Implicit or Explicit Indications}

\dots


\subsection{Evaluation and Comparison of Agents}

\begin{itemize}
    \item Is the comparison between spatial and temporal memory meaningful?
    \item Would have been nice with a comparison to a close-to-optimal rule-based baseline in each environment.
    \item Discussion about metrics, like SPL.
    \item What does the difference in SPL mean?
\end{itemize}

%Batra et al.~\cite{batra_evaluation_2020} revisit the problem of evaluating embodied navigation agents.
%They note some issues with the SPL metric.
%It fails to consider the fact that some failures are less of a failure than others.
%Some failures might in fact be close to reaching the goal while some fail completely.
%The binary success introduces high variance in average SPL computation.
%Furthermore, SPL is not particularly suitable for comparison across different datasets,
%as obtaining a high SPL is more difficult for short paths than for long paths.
%They suggest that SPL should be replaced by some metric that takes these issues into account.
%However, to our knowledge such a metric is yet to be proposed and widely adopted.


\subsection{Higher-dimensional Search Spaces}

Also continuous\dots

\begin{itemize}
    \item Larger search space could correspond to larger area or higher granularity.
    \item Neither scales well (weights in map scale increase quadratically with search space size -- fine for fixed positions, bad for movable cameras).
    \item An ego-centric architecture might be more general, does not need the position and can work over larger territories (although not take whole territory into account).
\end{itemize}


\subsection{Scaling up Model Sizes}

\begin{itemize}
    \item Stacked LSTM unstable, even with dropout. More weights could be used.
    \item Early experiments with stacking multiple LSTM layers lead to heavy overfitting and unstable learning curves.
\end{itemize}

\subsection{Replicability, Reliability and Validity}

As discussed in Section~\ref{sec:theory-evaluation}, reproducibility is a problem in current deep RL research.
This comes with the risk of stagnating the field by inhibiting correct interpretation of results~\cite{henderson_matters_2018}.
In this work, we have taken certain precautions to avoid such problems.

For each experiment, we have collected results across at least 3 runs.
While several sources state that as many as 10 runs are needed for significant results~\cite{colas_hitchhiker_2019,agarwal_rlliable_2022},
such undertakings are not feasible without sufficient compute resources.
We feel that 

Finally, we have provided source code for our implementation, and invite others to reproduce our results.

\begin{itemize}
    \item We have published source code.
    \item Reinforcement learning is inherently unreliable.
    \item Stochasticity necessary to avoid getting stuck, leads to less precise paths.
    \item Possibility of implementation errors.
    \item Low number of runs, quite high variance.
    \item Few samples during testing.
\end{itemize}

\subsection{Applicability to Real-world Search Scenarios}

\begin{itemize}
    \item Do the environments test realistic tasks?
    \item Is the needed number of training samples realistic?
    \item What happens when detection is more difficult?
    \item Real-world search has higher variance.
\end{itemize}


\subsection{Source Criticism}

\begin{itemize}
    \item We have based most of our work on known theory
\end{itemize}



\section{The work in a wider context}
\label{sec:discussion-wider}

% There must be a section discussing ethical and societal
% aspects related to the work. This is important for the authors
% to demonstrate a professional maturity and also for achieving
% the education goals. If the work, for some reason, completely
% lacks a connection to ethical or societal aspects this must be
% explicitly stated and justified in the section Delimitations in
% the introduction chapter.
% 
% In the discussion chapter, one must explicitly refer to sources
% relevant to the discussion.

Furthermore, automating manual tasks has societal implications.


\subsection{Ethics of Autonomous Agents}

Russell et al.~\cite{russell_priorities_2015}

\cite{vinuesa_sustainable_2020}, \cite{russell_ethics_2015}, \cite{russell_beneficial_2022}, \cite{brundage_malicious_2018}

Several applications are positive, like search-and-rescue and fire detection.
In such scenarios, a short search time can be the difference between life and death.


However, there automated visual search systems also have destructive applications.
A major concern of artificial intelligence researchers and the broader public today is the implications of autonomous systems for 

The widely discussed topic of autonomous weapon systems

% https://people.eecs.berkeley.edu/~russell/research/LAWS.html

As the authors of this work, we explicitly condemn its use for lethal weaponry.

\subsection{Trustworthiness for Safety-Critical Applications}

How can we trust an autonomous visual search system?
For safety critical applications, certain requirements when it comes to robustness are needed.
Things to consider before deploying a system for such a scenario include verification, validity, security, control

% what can this be used for? good and bad things?

%While automated search systems have many positive uses, like XXX, there are certainly other use cases that could be considered negative.
%Mass surveillance, XXX, are both very relevant today.


\rule{5cm}{1pt}

\begin{itemize}
    \item The less bias we introduce, the more general the method has the potential to be.
    \item Not just visual search -- also a practical example of RL for exploration, generalization, \dots
    \item Discuss the generality of the proposed approach - what types of cues can it pick up? Spatial and semantic relationships, non-uniform probabilities, etc. 
    \item Show some search paths and discuss them?
    \item We can visualize attention as in "A critical investigation of deep reinforcement learning for navigation"
\end{itemize}

% learning actions, recognition and localization simultaneously feels in itself interesting
% how do we interpret such a model? are there papers for visualizing reinforcement learning weights
