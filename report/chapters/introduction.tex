\chapter{Introduction}
\label{cha:introduction}

In this thesis project, the problem of searching for target objects in unknown but familiar environments is addressed.
This chapter presents the motivation behind the project, the research questions that are addressed, and the delimitations. 

\section{Motivation}
\label{sec:motivation}

The ability to visually search for things in an environment is fundamental to intelligent behavior.
We humans are constantly looking for things, be it be it the right book in the bookshelf, a certain keyword in an article or blueberries in the forest.
In many cases, it is important that this search is strategic, efficient, and fast.
Animals need to quickly identify predators, and drivers need to be able to search for pedestrians crossing the road they are driving on.

Automating visual search is of great interest for several reasons.
Visual search is crucial for applications such as search and rescue, surveillance, fire detection, etc.
Autonomous vehicles can both reduce risk and potentially exhibit more intelligent searching behavior than human-controlled ones.

However, while visual search is often seemingly effortless to us humans, it is a complex process.
Attempts to understand and recreate human visual search in machines has been a big challenge~\cite{eckstein_visual_2011}.
At the root of the visual search problem is partial observability.
A searcher can only perceive, or pay attention to, a limited region of the searched environment at once.
Therefore which regions to observe and in what order becomes an important decision. 

How humans and animals search for things has been studied extensively in neuroscience~\cite{eckstein_visual_2011,wolfe_visual_2010,nakayama_situating_2011}.
When we search, we use features of the environment to guide our attention~\cite{wolfe_five_2017,eckstein_visual_2011}.
For example, we know to look for berries at the forest floor, and not to look for boats on land.
Furthermore, search is not purely reactive but involves the use of memory.
We also use memory to take the history of the search into account when deciding where to move our attention~\cite{wolfe_five_2017}.

Such features can in some cases be quite subtle and difficult to pick up, even for humans.
Manually engineering guidance in accordance with these features can be expensive,
especially if a searching system should be deployed in many different environments.
If one could instead learn a good searching strategy from a limited set of sample environments this would be circumvented.
Such a system could be taught to search in arbitrary environments without the use of manually encoded environment-specific rules.

Reinforcement learning~\cite{sutton_reinforcement_2018} (RL) is a paradigm that is suited for learning mappings from sensor values to actions.
In recent years, RL has been combined with deep learning~\cite{goodfellow_deep_2016} with tremendous success.
It has been used to master arcade games~\cite{mnih_human-level_2015}, board games~\cite{silver_mastering_2016}, and even complex real-time strategy games~\cite{vinyals_grandmaster_2019}.
Several works have also applied RL to tasks involving embodied agents with visual input~\cite{minut_reinforcement_2001,mnih_recurrent_2014,zhu_target-driven_2017,mirowski_learning_2017}.
This makes it interesting to see if RL can also be applied to visual search.

\section{Aim}
\label{sec:aim}

The aim of this thesis is to investigate how an intelligent agent that learns to search for targets using visual input can be implemented with deep reinforcement learning.
Such an agent should learn the characteristics of the environments it is trained on and utilize this knowledge to search strategically in unseen environments.
Specifically, we consider scenarios where the agent can only observe a small portion of its environment at any given time through an camera whose movements are unrestricted.
The agent has to actively choose where to look in order to gain new information about the environment.

We postulate that an effective searcher learns how the distribution of targets and prioritizes regions where they are more likely according to previous experience.
The distribution of targets may be correlated with the appearance of the searched scene.
A good searcher should integrate information over time to build an internal representation of the environment and use it to make informed decisions.
The agent should be able to search the environment exhaustively while avoiding visiting the same region twice.
Finally, the agent should be able to locate multiple targets while minimizing the number of steps taken.

If such a system is to be trained and deployed for a real-world task, there is likely a limited set of samples to learn from.
Therefore it is also of interest to investigate how many samples are required to infer how to effectively search in similar environments.
While similar problems have been addressed in the past, both with learning agents~\cite{minut_reinforcement_2001,mirowski_learning_2017,ourselin_artificial_2016,caicedo_active_2015} and non-learning agents~\cite{shubina_visual_2010,forssen_informed_2008}, 
our impression is that this is the first work to address learning to search in unseen environments where emphasis is placed on how arbitrary visual cues can guide search.
Our contributions are as follows:

\begin{itemize}
  %\item We formally introduce the visual search problem for reinforcement learning.
  %\item We discuss the problem from several angles.
  %\item We compare two memory architectures, one spatial and one temporal.
  \item We provide a set of environments to train and evaluate visual search agents.
  \item We propose two approaches for solving the visual search task with reinforcement learning.
  \item We evaluate the performance of the proposed methods to a set of common baseline agents.
  \item We investigate how well each agent is able to generalize to unseen environments.
  \item We discuss our method in terms of applicability and usefulness.
\end{itemize}

\section{Research Questions}
\label{sec:questions}

This thesis will address the following questions:

\begin{enumerate}
  \item \label{itm:rq1} How can an agent that learns to intelligently search for targets be implemented with reinforcement learning?
  \item \label{itm:rq2} How does the learning agent compare to random walk, greedy search, and a human searcher with prior knowledge of the searched environment?
  \item \label{itm:rq3} How well does the learning agent generalize from a limited number of training samples to unseen in-distribution scene samples?
\end{enumerate}

\section{Delimitations}
\label{sec:delimitations}

We focus on the behavioral and decision-making aspects of the presented problem, and delimit ourselves from difficult detection problems.
For this reason, targets will deliberately be made easy to detect once visible.
Furthermore, we make the assumption that the searched environment is static.
The appearance of the environment and the location of the targets does not change from one observation to the next.
Finally, we are specifically interested in deep reinforcement learning approaches.

