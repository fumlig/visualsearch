\chapter{Introduction}
\label{cha:introduction}

The ability to search for and locate things in our environments is fundamental to our daily lives as humans.
We are constantly looking for things, be it be it the right book in the bookshelf, a certain keyword in an article, or birds in the sky.
In many cases, it is important that this search is strategic, efficient, and fast.
Animals need to quickly identify predators, and drivers need to be able to search for pedestrians crossing the road they are driving on.

At the root of the visual search problem is partial observability.
A searcher can only perceive, or pay attention to, a limited region of the searched environment at once.
Therefore which regions to observe and in what order becomes an important decision. 
Humans are able to utilize features of the environment to guide our search~\cite{wolfe_five_2017,eckstein_visual_2011,nakayama_situating_2011}.
For example, we know to look for berries at the forest floor, and not to look for boats on land.

This thesis investigates the implementation of agent that learn to locate targets in their environments.
Specifically, we look at how agents that learn to utilize environment cues to search more efficiently can be implemented with deep reinforcement learning.

\section{Outline}
\label{sec:outline}

The motivation and aim of the thesis are introduced in the remainder of this chapter. 
Relevant theory, such as background information and related work, is covered in Chapter~\ref{cha:theory}.
Methodology for answering the research questions is laid out in Chapter~\ref{cha:method}.
Experimental results are covered in Chapter~\ref{cha:results}, and discussed in Chapter~\ref{cha:discussion}.
Finally, Chapter~\ref{cha:conclusion} concludes the thesis with some remarks and suggestions for future work.

\section{Motivation}
\label{sec:motivation}

Automated visual search has applications in several areas including search-and-rescue, surveillance, fire detection and home assistance.
Employing autonomous vehicles instead of human-controlled ones for such tasks can both reduce risk and cost of labour.
Additionally, autonomous agents can potentially search more efficiently.

However, while visual search is often seemingly effortless to us humans, it is a complex process.
Understanding human visual search and recreating it in machines has proven to be a great challenge~\cite{eckstein_visual_2011}.
An autonomous searcher has to both be able to recognize useful features and know how to use them to efficiently guide its search.
Doing this requires some contextual scene understanding.

Although we can teach machines to search by encoding human knowledge, this is a difficult and expensive process.
Communicating to a machine how to pick up these features understand its visual input is difficult.
Useful features may be subtle and difficult to pick up, even for humans.
Furthermore, properly utilizing the features to minimize search time is not trivial~\cite{ye_complexity-level_2001}.

Systems that rely on handcrafted rules for search are also restricted to the contexts they were designed for.
Visual features that can be used to search efficiently in one environment may not be present in the next.
Before deploying such a system in a new environment, the particulars of that environment have to be encoded.
Real-world search scenarios exhibit high variance that can be infeasible to teach to a machine.

An alternative to manually encoding search strategies in machines is to instead use a learning system.
Reinforcement learning~\cite{sutton_reinforcement_2018} (RL) is a general paradigm for learning how to act in order to achieve some goal.
In recent years, RL has been combined with deep learning~\cite{goodfellow_deep_2016} with tremendous success.
RL agents have surpassed human performance in arcade games~\cite{mnih_human-level_2015}, board games~\cite{silver_mastering_2016}, and even complex real-time strategy games~\cite{vinyals_grandmaster_2019}. 
Several works have also applied RL to tasks involving complex control problems with visual input~\cite{minut_reinforcement_2001,mnih_recurrent_2014,zhu_target-driven_2017,mirowski_learning_2017}.

This makes it interesting to see whether RL can also be applied to visual search.
A learning system could learn patterns and strategies that are difficult for humans to pick up and utilize to minimize search times.
Given a sufficient number of samples, such a system could learn to search in arbitrary environments.

\section{Aim}
\label{sec:aim}

The aim of this thesis is to investigate how an intelligent agent that learns to search for targets using visual input can be implemented with deep reinforcement learning.
Such an agent should learn the characteristics of the environments it is trained on and utilize this knowledge to search strategically in unseen environments.
Specifically, we consider scenarios where the agent can only observe a small portion of its environment at any given time through an camera whose movements are unrestricted.
The agent has to actively choose where to look in order to gain new information about the environment.

We postulate that an effective searcher learns how the distribution of targets and prioritizes regions where they are more likely according to previous experience.
The distribution of targets may be correlated with the appearance of the searched scene.
A good searcher should integrate information over time to build an internal representation of the environment and use it to make informed decisions.
The agent should be able to search the environment exhaustively while avoiding visiting the same region twice.
Finally, the agent should be able to locate multiple targets while minimizing the number of steps taken.

If such a system is to be trained and deployed for a real-world task, there is likely a limited set of samples to learn from.
Therefore it is also of interest to investigate how many samples are required to infer how to effectively search in similar environments.
While similar problems have been addressed in the past, both with learning agents~\cite{minut_reinforcement_2001,mirowski_learning_2017,ourselin_artificial_2016,caicedo_active_2015} and non-learning agents~\cite{shubina_visual_2010,forssen_informed_2008}, 
our impression is that this is the first work to address learning to search in unseen environments where emphasis is placed on how arbitrary visual cues can guide search.
Our contributions are as follows:

\begin{itemize}
  %\item We formally introduce the visual search problem for reinforcement learning.
  %\item We discuss the problem from several angles.
  %\item We compare two memory architectures, one spatial and one temporal.
  \item We provide a set of environments to train and evaluate visual search agents.
  \item We propose two approaches for solving the visual search task with reinforcement learning.
  \item We evaluate the performance of the proposed methods qualitatively and quantitatively.
  \item We investigate how well each agent is able to generalize to unseen environments.
  \item We discuss our method in terms of applicability and usefulness.
\end{itemize}

\section{Research Questions}
\label{sec:questions}

This thesis will address the following questions:

\begin{enumerate}
  \item \label{itm:rq1} How can an agent that learns to intelligently search for targets be implemented with reinforcement learning?
  \item \label{itm:rq2} How does the learning agent compare to random walk, greedy search, and a human searcher with prior knowledge of the searched environment?
  \item \label{itm:rq3} How well does the learning agent generalize from a limited number of training samples to unseen in-distribution scene samples?
\end{enumerate}

\section{Delimitations}
\label{sec:delimitations}

We focus on the behavioral and decision-making aspects of the presented problem, and delimit ourselves from difficult detection problems.
For this reason, targets will deliberately be made easy to detect once visible.
Furthermore, we make the assumption that the searched environment is static.
The appearance of the environment and the location of the targets does not change from one observation to the next.
Finally, we are specifically interested in deep reinforcement learning approaches.
