\chapter{Method}
\label{cha:method}

% ~10p

% In this chapter, the method is described in a way which shows how the
% work was actually carried out. The description must be precise and
% well thought through. Consider the scientific term
% replicability. Replicability means that someone reading a scientific
% report should be able to follow the method description and then carry
% out the same study and check whether the results obtained are
% similar. Achieving replicability is not always relevant, but precision
% and clarity is.
% 
% Sometimes the work is separated into different parts, e.g.  pre-study,
% implementation and evaluation. In such cases it is recommended that
% the method chapter is structured accordingly with suitable named
% sub-headings.

In this chapter, the method used is described.
Section \ref{sec:problem} formalizes the problem solved.
Section \ref{sec:environments} details the environment used to evaluate solutions.
Section \ref{sec:baseline} describes the baseline learning method.
Section \ref{sec:approach} describes the approach used to solve the problem with a learning agent.
Section \ref{sec:experiments} describes the experiments conducted to answer research questions \ref{itm:rq2} and \ref{itm:rq3}.

\section{Problem Statement}
\label{sec:problem}

% Maybe keep the POMDP stuff in environment section

We can now formally define the problem of searching for targets in unknown environments.

We denote the task by \(\langle \mathcal{M}, \mathcal{T}_0 \rangle\), where \(\mathcal{M}\) is a POMDP and \(\mathcal{T}_0\) is the probability distribution on the initial states.
The state is defined by a (Euclidean) space \(S \subset \mathbb{R}^d\) which we refer to as the \textit{scene}.
At each timestep, the agent observes a subspace \(V \subset S\) of the environment which we refer to as the \textit{view}.
The actions in \(\mathcal{A}\) transform the view.
In the scene, there is a set of \(N\) targets \(T = \{t_0, t_1, \dots t_N | t_i \in S\}\).
With a final trigger action, the agent can indicate that there is one or more target in the view.
The goal of the agent is to select actions that bring each target into view and indicate that they are visible with the trigger action, while minimizing the number of actions taken.
The observations \(o \in \Omega\) are tuples \(o = \left\langle x, p \right\rangle\).
Here, \(x \in \mathbb{R}^{3 \times W \times H}\) is an RGB image representing the current view, and \(p \in S\) is the position of the agent which uniquely identifies the view.
If \(T \cap V \neq \varnothing\) there are \(h = \left\lvert T \cap V \right\rvert\) targets in view.

\section{Approach}
\label{sec:approach}

% discretize the problem
% environment description after

% this should maybe not be specificed yet...

To design an agent that effectively solves the task, we draw inspiration from several previous works and adapt them to better suit this particular task.
The final agent should be able to recognize targets, regardless of where they appear in view.
It should also be able to integrate features over time in order to remember which locations have been visited.
Remembering features of of these locations may also be of importance,
as it can provide clues for what is in their proximity. 

With this in mind, we use an episodic spatial memory that stores the agent's belief state.
The architecture of the neural network is presented in Figure X.
\todo{Describe approach once it is fully decided. Will be a (spatial) memory that allows for reasoning over the whole explored scene.}

%This memory acts as an inhibition-of-return mechanism.
% inhibition of return could be done programmatically, but there are advantages to learning it:
% in some cases it is good to return to visited locations, such as when traversing a visited region to reach an area with high probability
% cite paper that advocated for having as little bias as possible (since removing it can lead to better learned strategies)

Due to time constraints and the advantages described in Section \ref{sec:policy-value}, we limit our approaches to policy gradients.
Specifically, we use an actor-critic method.
The policy and value function are approximated using a multi-headed neural network.
The neural network of the agent is split into four parts:
A feature extraction network is connected to a recurrent network.
The recurrent network is in turn is connected to an actor network head and a critic network head, which approximate the policy and value function respectively.

\section{Baselines}
\label{sec:baseline}

We compare our approach to three different baselines.
The first baseline is the agent from \cite{mnih_human_2015}, which has previously been used as a baseline in \cite{mirowski_navigate_2017} and \dots.
The agent receives only image observations, and uses a convolutional neural network with three layers for feature extraction.

The second baseline is a recurrent version of the first baseline, the same architecture used in \cite{mnih_asynchronous_2016}, \cite{mirowski_navigate_2017},and \cite{gupta_cognitive_2019}.
After the initial CNN there is an LSTM layer with 256 cells.

The third baseline also uses the position of the agent, which is encoded as one one-hot vector per dimension. 
The encoded positions are concatenated together with the output of the CNN and used as input to the LSTM layer.
Including position should help the agent avoid revisiting previous locations and understand which parts of the scene are yet to be explored. 

With these three baselines, we can clearly see what role the observations and memory plays for the search task.

\section{Environments}
\label{sec:environments}

To train an test an agent for the problem, we use three different environments.
The three environments have different characteristics to test the applicability of the evaluated approaches to different search problems.
In each environment there is a scene with a background of distractors and a foreground of targets.
As \cite{cobbe_procgen_2020,mnih_asynchronous_2016}, we leverage procedural generation in all environments.
The scenes are drawn from some unknown distribution.
The appearance of the scenes and the location of targets have some correlation in all environments.
This means that the agent should be able to search more efficiently using knowledge from the scene.

The observations are

\[
    o = \left\langle x, p \right\rangle \in \Omega, x \in \mathbb{R}^{3 \times 64 \times 64}, p \in \mathbb{N}^2
\]

For the position part of the observations, we assume the presense of some oracle.
In many realistic scenarios this is the case (GPS, pan/tilt, etc.).
If how each action moves the agent is well-defined, we do not need the position at all.
We can use relative positions instead of absolute ones.
Some of the baselines do not use the position.

The action space is the same in all environments:

\[
    \mathcal{A} = \lbrace \mathtt{UP}, \mathtt{DOWN}, \mathtt{LEFT}, \mathtt{RIGHT}, \mathtt{TRIGGER} \rbrace,
\]

\(\mathtt{UP}\), \(\mathtt{DOWN}\), \(\mathtt{LEFT}\), and \(\mathtt{RIGHT}\) translate the view and \(\mathtt{TRIGGER}\) indicates that a target is in view.
% rotates in third environment though

We experiment with three rewards signals.
The first reward signal is defined as

\[
    \mathcal{R}(s_t, a_t) =
    \begin{cases}
        10h & \text{if \(a_t = \mathtt{TRIGGER}\) and a target is in view,} \\
        -1  & \text{otherwise.}
    \end{cases}
\]

with \(h = \left\lvert T \cap V \right\rvert\). 
We argue that this reward provides a suitable inductive bias for the task at hand.
Early experiments show that a constant reward of \(r_t = -1\) that simply incentivize the agent to complete the episode as quickly as possible converge too slowly for large state spaces.
The reward for finding a target speeds up training without deviating from the goal of the task -
targets should be triggered when in view, but triggers when targets are out of view should be penalized.
The constant penalty of \(-1\) in all other cases assures that the agent is rewarded for quick episode completion.

In practice, early experiments show that even this reward might be too sparse.
To speed up training, we experiment with two extensions to the reward:

\[
    \mathcal{R}'(s_t, a_t) =
    \begin{cases}
        1 & \text{if \(a_t \neq \mathtt{TRIGGER}\) moves the view closer to the nearest target, and} \\
        \mathcal{R}(s_t, a_t) & \text{otherwise}.
    \end{cases}
\]

\[
    \mathcal{R}''(s_t, a_t) =
    \begin{cases}
        1 & \text{if \(a_t \neq \mathtt{TRIGGER}\) moves the view to a previously unseen subspace, and} \\
        \mathcal{R}(s_t, a_t) & \text{otherwise}.
    \end{cases}
\]

These three reward signals are interesting to compare for a few reasons.
\(R\) does not clearly mediate to the agent what actions are desireable until a target is found.
It may therefore lead to slow learning, but it also does not steer away from the goal of finding targets quickly.
\(R'\) uses the supervised distance between targets and the agents, which is available during training.
This is similar to the reward used by \cite{caicedo_active_2015} and \cite{ghesu_artificial_2016}.
In addition to speeding up learning, we hypothesize that this reward may help the agent pick up correlations between scene appearance and target probability.
However, it can never yield policies that search exhaustively as such actions are never rewarded.
It may therefore perform worse during testing where the reward is not available to the agent.
It will also not learn to take the shortest paths in the general case, as selecting waypoints greedily does not yield optimal paths.
\(R''\) strikes a balance between the two other signals by instead encouraging exploration.
This should cause the agent to learn to search the environment exhaustively.
% tabu search
% optimal paths

The episode is terminated when all targets have been found, or when a certain number of time steps have passed.
This number of time steps is set so that the agent can reasonably find the targets with random search, and then gradually improve its policy from there.
Terminating episodes early this way is common to speed up training~\cite{pardo_timelimits_2022}.

% Averaged over all possible samples, the probability of targets should be uniform over the scene.
% to make comparison with exhaustive search fair
% I think?

\subsection{Gaussian Environment}
% environment 1: easy, understandable

The first environment is the simplest environment. 
The scene is described by a \(256 \times 256\) RGB image.
The agent observes a \(64 \times 64\) sub-image at each time step.

In the image there are three Gaussian kernels with random positions.
The height of the kernel is indicated by a higher intensity in the blue channel.
Targets are \(1 \times 1\) pixels in the red channel.
The locations of the targets are randomized weighted by the height of the Gaussian kernels.
This means that the more intense the blue channel, the higher the probability of a target.
The idea with this environment is to test that the method learns what we want it to learn.
There is a clear correlation between observations and desirable actions.
It is also easy to determine whether the agent acts well in this environment.
Our feeling is that this is something that previous similar works has not done. % cite

\begin{figure}
    \centering
    \input{figures/gaussian.pgf}
    \label{fig:gaussian}
    %\vspace*{-1cm}
    \caption[Gaussian environment]{Four samples of the first environment. There are three gaussian kernels in the environment, whose height is visualized with the blue channel. There are three targets in the environment, whose location is sampled from the distribution defined by the sum of the three gaussian kernels.}
\end{figure}

\subsection{Terrain Environment}
% environment 2: medium, procedural and configurable

The second environment is intended to look like realistic terrain.
The scene's appearance is given by a \(512 \times 512\) RGB image.
The agent's view is a \(64 \times 64\) sub-image.
Gradient noise is generated and used as a height map.
The height map determines the color of the terrain.
The height also correlates to the probability of targets.
Specifically, targets are located between shores and mountain bases.
There are 10 targets in each scene.

The environment roughly corresponds to a UAV search-and-rescue scenario.
It is desirable that a searching agent should learn to not search oceans and lakes.
The agent should prioritize searching along the edges of land masses.
The appearance of the environment is highly configurable and has high variance.
We use this environment for evaluating the generalization capabilities of agents.

\begin{figure}
    \centering
    \input{figures/terrain.pgf}
    \label{fig:terrain}
    %\vspace*{-1cm}
    \caption[Terrain environment]{Three samples of the terrain environment. Terrain seen from above with red targets scattered along island edges. The white border indicates the agent's current view.}
\end{figure}

\subsection{Camera Environment}
% environment 3: hard, realistic

The third environment is a three-dimensional version of the second one.
The height map is turned into a three-dimensional mesh, and the agent is placed at its center.
The agent observes the scene through a perspective projection pan-tilt camera.
Targets are, as before, placed along island edges.
The agent is therefore expected to avoid searching the skies and oceans.

This environment is intended to model more realistic scenarios where the image is more difficult to interpret.

\begin{figure}
    \centering
    \input{figures/camera.pgf}
    \label{fig:camera}
    %\vspace*{-1cm}
    \caption[Camera environment]{Three samples from the camera environment. Terrain seen from a pan-tilt-zoom camera. The pan and tilt of the camera can be adjusted to move the view around. In the mid-right image, a target can be seen.}
\end{figure}

\section{Experiments}
\label{sec:experiments}

To compare our approach to the different baselines,
we train and test all agents on all three environments.
The agents are trained on the full distribution of environments,
We do this for all three reward signals \(R\), \(R'\) and \(R''\).
All agents are trained for 25 million time steps.
They are then tested on 1000 held out samples from each environment.

For each agent, environment, and reward signal we report the average return and episode length over time during training.
During testing, we compare the learning agents to random walk, exhaustive search and a human searcher with prior knowledge of the characteristics of the searched environments.
We report the SPL metric for all agents, where the shortest path length is the optimal travel distance between the targets and the initial position of the agent.
The distance between two points is computed as the minimal number of actions to transform the view between the two.

During testing, we increase the maximum episode length from 1000 time steps to 5000 time steps.
This is to give more accurate metrics for episodes that do not terminate within 1000 time steps. 

Additionally, we conduct experiments to measure the generalization capabilities of the agents.
For this, we use the terrain environment only.
We test on the same held out levels as before, but vary the number of samples seen during training.
The training set size is varied from 100, 1000, 10 000, and 100 000 samples.
This is done by limiting the seed pool used to generate the environments.
This way, we can get a sense of how much data and simulation is required to apply the approach to real-world tasks.
For each training set size, we train the agents until convergence.
This is the same approach as in \cite{cobbe_procgen_2020}.
Once again, we compare all three reward signals.

Following the recommendations of \cite{henderson_matters_2018} and \cite{agarwal_rlliable_2022}, we report confidence intervals across a handful of seeds.
We run all experiments across 3 different runs, and use the aggregate metrics proposed by \cite{agarwal_rlliable_2022}.

We train all agents with PPO~\cite{schulman_ppo_2017}.
Early experiments show that PPO gives good results, stable learning curves and good sample efficiency, which is in line with results reported by \cite{andrychowicz_empirical_2020}.
Furthermore, we use the same hyperparameters as those outlined in \cite{cobbe_procgen_2020} (Table~\ref{tab:hyperparameters}).
We find that using many parallel environments stabilizes the training.
As suggested by \cite{andrychowicz_empirical_2020}, we initialize the policy output weights so that their mean is 0 and their standard deviation is low (\(10^{-2}\)).

\begin{table}
    \centering
    \caption{Hyperparameters used during training.}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        \(\gamma\) & .999 \\
        \(\lambda\) & .95 \\
        \(\text{time steps per rollout}\) & 256 \\
        \(\text{epochs per rollout}\) & 3 \\
        \(\text{mini-batches per epoch}\) & 8 \\
        \(\text{entropy bonus}\) & 0.01 \\
        \(\text{clip range}\) & .2 \\
        \(\text{reward normalization}\) & yes \\
        \(\text{learning rate}\) & \(5 \times 10^{-4}\) \\
        \(\text{parallel environments}\) & 64 \\
        \(\text{total time steps}\) & \(25 \times 10^6\) \\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparameters}
\end{table}

We normalize the reward to limit the scale of the error derivatives.
Early experiments show that this stabilizes training.
Similar results have been reported by \cite{andrychowicz_empirical_2020} and \cite{mnih_atari_2013}.
They use reward clipping instead which diminishes the meaning of reward magnitude.

% discuss tuning in discussion

\section{Implementation}

The environment is implemented with Gym~\cite{brockman_gym_2016}. The agent is implemented and RL algorithms are implemented with PyTorch~\cite{paszke_pytorch_nodate} for automatic differentiation of the computation graphs.

Proximal policy optimization~\cite{schulman_ppo_2017} was implemented following the official implementation by OpenAI.
Some necessary modifications were made to allow for recurrent policies.

All experiments are conducted on an Intel Core i9-10900X CPU and an NVIDIA GeForce RTX 2080 Ti GPU.

The source code for environments, agents and RL algorithms is available at \dots
\todo{Add link.}

% learning actions, recognition and localization simultaneously feels in itself interesting
% how do we interpret such a model? are there papers for visualizing reinforcement learning weights

% discuss state representation
% need more than the image
% - either memory or position or both

% also same configuration possibly?
% more parallel environments seem to lead to stable training (relatively slow increase in reward though...)

% an interesting question is how large search windows we can handle
% could be an alternative to zoom
% a large search window presumably requires attention


% memory is needed for (a) remembering previous locations, and (b) integrating features over time


% PPO
% PPG
