\chapter{Method}
\label{cha:method}

In this chapter, the method used is described.
Section \ref{sec:problem} formalizes the problem solved.
Section \ref{sec:environments} details the environment used to evaluate solutions.
Section \ref{sec:baseline} describes the baseline learning method.
Section \ref{sec:approach} describes the approach used to solve the problem with a learning agent.
Section \ref{sec:experiments} describes the experiments conducted to answer research questions \ref{itm:rq2} and \ref{itm:rq3}.

\section{Problem Statement}
\label{sec:problem}

We can now formally define the problem of searching for targets in unknown environments,
adopting the CMDP formalism \cite{kirk_survey_2022}.
Let the task be a contextual POMDP \(\mathcal{M}\) where the state includes the context \(c\), which we refer to as the \textit{seed}.
The state includes a space \(S \subset \mathbb{R}^d\) which we refer to as the \textit{scene}.
In the scene, there is a set of \(N\) targets \(T = \{t_0, t_1, \dots t_N\}\) such that \(t_i \in S\).
At each time step, the agent perceives a subspace \(V \subset S\) of the environment which we refer to as the \textit{view}.
If \(T \cap V \neq \varnothing\) there are \(\left\lvert T \cap V \right\rvert\) targets in view.

The actions \(a \in \mathcal{A}\) transform the view into a different subset of the scene.
With a final action, the agent can indicate that there is one or more target in the view.
The observations \(o \in \Omega\) are tuples \(o = \left\langle x, p \right\rangle\).
Here, \(x \in \mathbb{R}^{3 \times W \times H}\) is an RGB image representing the current view, and \(p \in S\) is the position of the agent which uniquely identifies the view.
The goal of the agent is to select actions that bring each target into view and indicate that they are visible, while minimizing the number of total steps.
There is no inherent reward \(\mathcal{R}\) for this problem, so it has to be designed.
Finally, seed \(c\) determines the initial view \(V\), the location of the targets \(T\), the initial position \(p_0\) as well as the image observations \(x_t\) at each position \(p_t\).

\section{Environments}
\label{sec:environments}

To train an test an agent for the problem, we use three different environments with similar observation spaces, action spaces and reward signal.
Each environment have different characteristics that test the applicability of the evaluated approaches to different types search problems.
In all environments, the appearance of the scenes and the location of targets are correlated to some degree.
This means that the agent should be able to learn common characteristics of each environment and use those to search more efficiently.

As \cite{cobbe_procgen_2020} and \cite{mnih_asynchronous_2016}, we leverage procedural generation in all environments.
This gives us control over the difficulty of the environments as well as the number of training and test samples the agent is exposed to.
A seed determines the appearance of the scene, the location of the targets and the initial position of the agent.

Each episode is terminated when all targets have been found, or after 1000 time steps
Terminating episodes early this way is common to speed up training~\cite{pardo_timelimits_2022}.

\subsection{Observations, Actions and Reward}

All three environments use the same observation space, action space and reward signal.
The position and image observations are

\begin{align}
    o_t & = \left\langle x_t, p_t \right\rangle \text{, where} \\
    x_t & \in \mathbb{R}^{3 \times 64 \times 64} \text{, and} \\
    p_t & \in \{0, \dots, 9\} \times \{0, \dots, 9\}
\end{align}

All image observations are \(64 \times 64\) RGB images.
The agent moves in a \(10 \times 10\) grid, and we assume the presence of some oracle that provides the agent with its position.
In many realistic scenarios it is possible to determine the global position of an agent (GPS, pan/tilt, etc.).
If how each action moves the agent is well-defined, the relative position can be used instead of the absolute one.

The action space is the same in all environments:

\begin{equation}
    a_t \in \left\lbrace \mathtt{UP}, \mathtt{DOWN}, \mathtt{LEFT}, \mathtt{RIGHT}, \mathtt{INDICATE} \right\rbrace,
\end{equation}

where \(\mathtt{UP}\), \(\mathtt{DOWN}\), \(\mathtt{LEFT}\), and \(\mathtt{RIGHT}\) move the view one step in each direction.
This action space is realistic for many real-world search tasks,
such as search and rescue with a UAV, where the actions correspond to translations in each cardinal direction,
and surveillance with a pan-tilt-camera, where the actions correspond to pitch and yaw rotations.
The final action, \(\mathtt{INDICATE}\), is used to indicate that a target is in view.

The reward signal should be designed so that the agent learns a policy that achieves the goal of finding all targets with a minimal number of actions.
We use the following reward signal:

\begin{equation}
    r_t = h - 0.01 + 0.005d + 0.005e
\end{equation}

Here, \(h = \left\lvert T \cap V \right\rvert\) if \(a_t = \mathtt{INDICATE}\), and \(h = 0\) otherwise.
This term is equal to the number of targets that were found at this time step.
The constant penalty of \(-0.01\) ensures that the agent is rewarded for quick episode completion.
The term \(d = 1\) if \(p_t\) is closer to the nearest target than \(p_{t-1}\), otherwise \(d = 0\).
Similarly, \(e = 1\) if \(p_t\) has not been explored previously and \(e = 0\) otherwise.

Through experimentation, we find that a larger time penalty than \(-0.01\) dominates the reward for finding targets.
This is potentially related to the episode time -- with a time penalty thats too large, the reward \(h\) for finding targets has a relatively small impact on episode return for long episodes.
The two next terms are bonus rewards, intended to speed up learning by further encouraging desired behavior.
Actions that move the agent towards the nearest target and move the view to previously unseen regions are desirable.
Importantly, the sum of these bonuses is not larger than the magnitude of the time penalty.
This is to ensure that exploration and moving towards targets does not seem more important to the agent than finishing the episode quickly.
Therefore, the bonuses may guide the agent towards finding targets but do not cause it to steer away from the underlying goal.

\subsection{Gaussian Environment}

The first environment is the simplest environment, where the correlation between scene appearance and target probability is clear.
During each episode reset, a \(1024 \times 1024\) RGB image is generated conditioned on the seed.
The image contains three blue bumps, whose intensity is highest towards their center and wears off radially outwards.
The intensity wears off as an (approximate) Gaussian function.
The sum of the intensity in the blue channel in the image is used as a probability density function to select the location of three targets.
Targets are characterized by red \(8 \times 8\) squares.
The higher the intensity in the blue channel, the more likely that there is a target there.

The image is divided into a grid of \(10 \times 10\) steps, one per position \(p_t\).
The image \(x_t\) is the \(64 \times 64\) sub-image at the current position in the grid.
Each moving action translates the agent one step in corresponding cardinal direction in the grid.

The idea with this environment is to test whether an agent is able to utilize scene characteristics to search quicker.
An efficient searcher in this environment should prioritize locations where the intensity in the blue channel is high.
It should be able to use the gradient of the underlying Gaussian function to move towards locations with higher target probabilities, while avoiding revisiting locations and intelligently planning its search path.

\begin{figure}
    \centering
    \input{figures/gaussian.pgf}
    \label{fig:gaussian}
    \caption[Gaussian environment]{Four samples of the first environment. There are three blue bumps and three red targets in each scene. Targets are more likely where the intensity of the bumps is high.}
\end{figure}

\subsection{Terrain Environment}

The second environment is similar to the first one, but intended to simulate a search scenario in realistic terrain.
Actions and observations behave the same as in the first environment.
At the start of each episode, a \(1024 \times 1024\) height map is generated using gradient noise.
The height map is used to determine the color of an image of the same size.
Lower heights are filled with blue ocean, and higher areas with green grass and brown mountains.
Three red targets are located with uniform probability along the shores, between mountains and water.
Green trees are also scattered around each scene, whose positions are sampled from the same distribution as that of the targets.

While this environment is similar to the first environment, it is less clear how to search efficiently in it.
There is higher variance between scene samples.
It is also less clear how the scene appearance is correlated to the probability of targets.
It is desirable that a searching agent should learn to not search oceans and mountains.
Instead it should prioritize searching along the edges of land masses.
For some scene samples, there are multiple islands with small patches of land between them.
These patches could be used to quickly prioritize land while avoiding water.
One can draw parallels to search-and-rescue scenarios with UAVs or fire detection.

\begin{figure}
    \centering
    \input{figures/terrain.pgf}
    \label{fig:terrain}
    \caption[Terrain environment]{Four samples of the second environment. Terrain seen from above with red targets scattered and green trees scattered along shores.}
\end{figure}

\subsection{Camera Environment}

The third environment is a three-dimensional version of the second one.
The height map is turned into a three-dimensional mesh, and the agent is placed at its center.
The agent observes the scene through a pan-tilt perspective camera.
Targets are, as before, placed along island edges.

The \texttt{LEFT} and \texttt{RIGHT} actions control the yaw of the camera, while \texttt{DOWN} and \texttt{UP} control its pitch.
The yaw is divided into 10 steps between 0 and 360 degrees.
The yaw angle wraps around, so that the agent can look around freely.
Similarly, the pitch is divided into 10 steps between 0 (straight forward) and -90 degrees (straight down), but without wrapping around.

Targets are always visible from at least one camera position.
They may be visible from multiple positions, and the agent is expected to indicate that they are visible only when they are as close to the center of the view image as possible.
This environment is intended to model more realistic scenarios where the image is more difficult to interpret.
In some scenarios, it can be important that objects are not only localized, but localized accurately.

\begin{figure}
    \centering
    \input{figures/camera.pgf}
    \label{fig:camera}
    \caption[Camera environment]{Four samples from the camera environment. Terrain seen from a pan-tilt camera. The pan and tilt of the camera can be adjusted to move the view around.}
\end{figure}

\section{Approach}
\label{sec:approach}

To design an agent that effectively solves the task, we draw inspiration from several previous works and adapt them to better suit this particular task.
The final agent should be able to recognize targets, regardless of where they appear in view.
It is likely important for the agent to have access to its location, especially for large search spaces.
As environments are procedurally generated, a partial image observation of the scene is not sufficient to ground the agent in it.
It should also be able to integrate features over time in order to remember which locations have been visited.
Remembering visual features of of these locations may also be of importance, as it can provide clues for what is in their proximity.

Due to the advantages described in Section \ref{sec:policy-value}, we limit our approaches to policy gradient methods.
Specifically, we employ an actor-critic approach that estimates a policy and value function with a multi-headed neural network.
The neural network architecture should reflect the aforementioned requirements.

\subsection{Architecture}

We design our neural network architecture as follows:
A CNN takes the observed image \(x_t\) and encodes it into a latent representation \(h_t\).
This allows the agent to extract translation invariant features in observed images.
The latent representation, as well as the current position of the agent \(p_t\) is used as input to an RNN.
Feeding both an image representation and the position of the agent to a recurrent step lets the agent remember visited locations and their appearance.
The output of the recurrent step is in turn is connected to an actor MLP head and a critic MLP head, which approximate the policy \(\pi\) and value function \(v\) respectively.
The architecture of the neural network is presented in Figure ~\ref{fig:architecture}.

\begin{figure}
    \centering
    \includegraphics{figures/architecture.pdf}
    \label{fig:architecture}
    \caption[Network architecture]{The neural network architecture used for estimating the policy and value functions from image and position observations.}
\end{figure}

For the CNN, we use the same architecture as \cite{mnih_human_2015}. % and more
The input image \(x_t \in \mathbb{R}^{3 \times 64 \times 64}\) is fed through three convolutional layers: the first layer convolves 32 filters of size \(8 \times 8\) with stride 4, the second convolves 64 filters of size \(4 \times 4\) with stride 2, and the third layer convolves 64 filters of size \(3 \times 3\) with stride 1.
This is followed by a final fully connected hidden layer with 512 outputs for the latent representation \(h_t \in \mathbb{R}^{512}\).

Two RNN architectures are compared: one temporal, and one spatial.
While a temporal memory can retain location and appearance information over time, it is not cleared how this information is stored and whether it can be utilized properly.
We hypothesize that agents using temporal memories may struggle with learning good policies in large search spaces and those that require scene understanding, such as reasoning over previously visited locations.
Several results from works in embodied visual navigation indicate that spatial memories can give better results than temporal ones~\cite{parisotto_salakhutdinov_2017,henriques_vedaldi_2018,gupta_cognitive_2019,chaplot_semantic_2020}.
Taking inspiration from this, we investigate whether a spatial memory can be more useful than a temporal one when searching for targets.

The temporal memory is a single LSTM~\cite{hochreiter_schmidhuber_lstm_1997} layer, as proposed by \cite{hausknecht_stone_2017} and used in \cite{mnih_asynchronous_2016}, \cite{mirowski_navigate_2017}, and \cite{gupta_cognitive_2019}.
As input to the LSTM, we use the latent image representation \(h_t\) concatenated with one-hot encodings of each dimension of \(p_t\).
The LSTM layer has 128 hidden cells so that \(y_t\) has 128 dimensions.
Finally, \(z_t\) contains the hidden and cell states.

The spatial memory is composed of a readable and writable feature map, similar to those in \cite{parisotto_salakhutdinov_2017}, \cite{henriques_vedaldi_2018}, \cite{gupta_cognitive_2019} and \cite{chaplot_semantic_2020}.
It is defined by the following set of operations:

\begin{align}
    \rho_t &= \text{CNN}_\text{read}(z_t) \\
    \omega_t &= \text{MLP}_\text{write}(\left\lbrack h_t, \rho_t, z_t^{(p_t)} \right\rbrack) \\
    q_t &= \text{MLP}_\text{position}(\text{onehot}(p_t)) \\
    y_t &= \left\lbrack \rho_t, \omega_t, q_t \right\rbrack
\end{align}

\begin{equation}    
    z_{t+1}^{(p')} =
    \begin{cases}
        \omega_t & \text{if } p' = p_t \\
        z_{t}^{(p')} & \text{if} p' \neq p_t
    \end{cases}
\end{equation}

Here, \(z_t \in \mathbb{R}^{64 \times 10 \times 10}\) is a feature map with one 64-dimensional feature vector per possible position \(p_t\).
At each time step, the agent reads from this map using a CNN with three convolutional layers.
Each layer has 32 filters of size \(3 \times 3\), and uses a padding of 1.
A final fully connected layer produces a 64-dimensional read vector \(\rho_t\).
An MLP takes the read vector concatenated with \(h_t\) and the feature vector stored in the map at the current position \(z_t^{(p_t)}\), and produces a 64-dimensional write vector \(\omega_{t}\).
Another MLP takes a one-hot encoding of the position as input and outputs a 64-dimensional vector \(q_t\).
The read vector \(\rho_t\), the write vector \(\omega_{t}\), and the position vector \(q_t\) are concatenated and used as input \(y_t\) to the value and policy networks.
This means that they can make use of visual features of previously explored locations, spatial relationships between these features, and the current position of the agent.
Finally, the feature map is updated so that \(z_{t+1}\) contains the write vector \(\omega_{t}\) at position \(p_t\).

\todo{Illustrate map updates.}

The hyperparameters for the two RNN architectures are chosen so that they have a comparable number of trainable parameters.
Both the policy and value network are fully connected networks.
The value network has one output for the value estimate.
The policy network has 5 outputs, the logits for each action.
Applying the softmax operation on this output gives the final action probabilities.
All network layers have ReLU activation functions, as suggested by \cite{henderson_matters_2018}.

% Inhibition of return could be done programmatically, but there are advantages to learning it:
% In some cases it is good to return to visited locations, such as when traversing a visited region to reach an area with high probability.
% Cite paper that advocated for having as little bias as possible (since removing it can lead to better learned strategies).

\section{Experiments}
\label{sec:experiments}

We train the network with the PPO~\cite{schulman_ppo_2017} algorithm described in Section~\ref{sec:ppo}.

To evaluate our approaches, we run four different experiments.
First, we train both the LSTM agent and the map agent i

Finally, to evaluate the quality of the learned policy, we compare both approaches to a set of baselines.

To compare our approach to the different baselines,
we train and test all agents on all three environments.
The agents are trained on the full distribution of environments,
We do this for all three reward signals \(R\), \(R'\) and \(R''\).
All agents are trained for 25 million time steps.
They are then tested on 1000 held out samples from each environment.

For each agent, environment, and reward signal we report the average return and episode length over time during training.
During testing, we compare the learning agents to random walk, exhaustive search and a human searcher with prior knowledge of the characteristics of the searched environments.
We report the SPL metric~\cite{anderson_evaluation_2018} for all agents, where the shortest path length is the optimal travel distance between the targets and the initial position of the agent.
The distance between two points is computed as the minimal number of actions to transform the view between the two.
Although finding the optimal path is not realistic in partially observable environments, SPL can still be useful when compared to that of a human.

During testing, we increase the maximum episode length from 1000 time steps to 5000 time steps.
This is to give more accurate metrics for episodes that do not terminate within 1000 time steps. 

Additionally, we conduct experiments to measure the generalization capabilities of the agents.
For this, we use the terrain environment only.
The training set size is varied from 100, 1000, 10 000, and 100 000 samples by limiting the seed pool used to generate the environments.
We test on held out samples from the full distribution, as done by \cite{cobbe_procgen_2020}.
This way, we can get a sense of how much data and simulation is required to apply the approach to real-world tasks.
For each training set size, we train the agents until convergence.
Once again, we compare all three reward signals.

Following the recommendations of \cite{henderson_matters_2018} and \cite{agarwal_rlliable_2022}, we report confidence intervals across a handful of seeds.
We run all experiments across 3 different runs, and use the aggregate metrics proposed by \cite{agarwal_rlliable_2022}.

All agents are trained with PPO~\cite{schulman_ppo_2017}.
Early experiments show that PPO gives good results, stable learning curves and good sample efficiency, which is in line with results reported by \cite{andrychowicz_empirical_2020}.
Furthermore, we use similar hyperparameters as in \cite{cobbe_procgen_2020} (Table~\ref{tab:hyperparameters}).
More parallel environments seem to both speed up and stabilize training.
%Furthermore, we reduce the number of steps per rollout to be much smaller than the episode length.
%This is recommended by \cite{schulman_ppo_2017}. 
As suggested by \cite{andrychowicz_empirical_2020}, we initialize the policy output weights so that their mean is 0 and their standard deviation is low (\(10^{-2}\)).
The Adam optimizer~\cite{kingma_ba_2017} is used in all experiments.

\begin{table}
    \centering
    \caption{Hyperparameters used during training.}
    \input{tables/hparams}
    \label{tab:hyperparameters}
\end{table}

We normalize the reward to limit the scale of the error derivatives.
Early experiments show that this stabilizes training.
Similar results have been reported by \cite{andrychowicz_empirical_2020} and \cite{mnih_atari_2013}.
Finally, we decay the learning rate linearly so that it is zero at the final time step.
We find that this helps the agent with finding a better local optimum.

%They use reward clipping instead which diminishes the meaning of reward magnitude.

%As scenes differ in appearance, the image is not enough to ground the agent.
%For large scenes, the position is likely to be needed as a way for the agent to orient itself.
%This could make it difficult for the agent to search exhaustively.
%With these three baselines, we can clearly see what role the observations and memory plays for the search task.

\subsection{Search Efficiency}

\subsection{Scene Size}

\subsection{Generalization}


\subsection{Baselines}
\label{sec:baseline}

To get a sense of quality of the learned policies, we compare our approaches to two common baselines.

\subsection{Ablations}
\label{sec:ablations}

\section{Implementation}

The environment is implemented with using the Gym~\cite{brockman_gym_2016} interface. The agent is implemented and RL algorithms are implemented with PyTorch~\cite{paszke_pytorch_nodate} for automatic differentiation of the computation graphs.
PPO~\cite{schulman_ppo_2017} was implemented following the official implementation by OpenAI,
and verified by testing on the benchmarks used in the original paper.
All experiments are conducted on an Intel Core i9-10900X CPU and an NVIDIA GeForce RTX 2080 Ti GPU.
The source code for environments, models and algorithms is available at \url{https://gitlab.liu.se/osklu414/tqdt33-masters-thesis}.
