\chapter{Method}
\label{cha:method}

In this chapter, the method used is described. Section \ref{sec:problem} formalizes the problem solved. Section \ref{sec:environment} details the environment used to train and test an agent. Section \ref{sec:algorithm} describes the algorithm used to train the agent.

\section{Problem Statement}
\label{sec:problem}

The problem of finding an optimal sequence of actions to find can be cast as a partially observable Markov decision process (POMDP). The environment's state is described by an \(n\)-dimensional matrix \(S\) whose members are drawn from some unknown distribution. In the environment, there are \(N\) targets \(\mathbb{T}\).

The agent observes a window \(\mathbb{W}\) of this matrix. The actions available to the agent change the members of the window \(\mathbb{W}\) but not the number of elements.

Every camera move is associated with a cost, and it is therefore important that the targets are identified as quickly as possible. % Reward signal.

The aim of this project is to learn to search efficiently in the visual domain with a pan-tilt-zoom camera. To make the problem more manageable, it is approximated as follows. The environment state is an RGB image of shape \((3, H, W)\). The window is fixed to \((3, 64, 64)\) and can be moved row-wise and column-wise in the image. This emulates the pan-tilt behaviour of a camera.

% todo: image

% "Learning to optimize" has some good formulation tips
% explains that policy search is intractable
% Also uses a decaying step size, could be interesting for us as well
% (although we have multiple targets...)

\section{Environment}
\label{sec:environment}

To train a learning agent, an environment for search was implemented. The searched environment is approximated by an RGB image with dimensions \((3, H, W)\). The behaviour of a pan-tilt-zoom camera is approximated by giving the   At any given time, only a \((3, 64, 64)\) subregion of the image is observable to the agent.

The environments to be searched are drawn from a distribution, with varying but similar appearance, target locations and appearances. For all environments, the appearance correlates to the probability of targets.

To incentivize finding targets quickly, the reward signal is set to -1 for each time step. Since viewing a window twice is redundant, such actions are punished by setting the reward to -2. If the agent selects the trigger action when a target overlaps with the window, the reward is set to 5. When all targets have been triggered, or when 1000 time steps have passed, the episode ends.

% section 17.4 in sutton is good!

% reward shaping: change reward signal as agent progresses
% could also shape the environment itself, make it more and more difficult
% each modification should be made so that the agent frequently receives reward with its current policy
% this is what we do when we train animals! ie give treats to dogs
% why isn't this working...

% reward normalization: 
% reward clipping: done in "playing atari with deep reinforcement learning": they state that since the scale of scores varies from game to game they fix all positive rewards to be one and negative to be -1. 0 rewards are left unchanged. this limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. could also affect agent's performance as it cannot distinguish between rewards of different magnitudes.
% could be important for us if the number of targets and the reward varies a lot

\section{Algorithm}
\label{sec:algorithm}

The 

The agent is trained with reinforcement learning 

% Actor critic

% Trained with PPO (possibly PPG)

% Feature extraction
% Shared Network
% Policy Head
% Value Head

\section{Implementation}

The environment is implemented with Gym~\cite{gym}, and the agent is implemented with PyTorch~\cite{pytorch}. 

\section{Experiments}

The first environment was used to determine a good observation space. By just observing the current window, the agent can never learn a suitable policy to solve the problem. This is because it cannot distinguish between equal windows at different locations. Experiments are run with several additional observation types: window position, .... Results of these experiments are presented for this environment only.

The agent was trained using the algorithm described in Section \ref{sec:algorithm} for 100 million time steps in all three environments. Hyperparameters are tuned with random search separately for each environment. For all experiments, the average return per episode is reported together with the theoretically optimal reward (obtained with an optimal path).

Additionally, experiments to eveluate the generalization capability of the agent were conducted. These were conducted on the procedurally generated terrain environment following the approach suggested in ~\cite{procgen}. During training, the seed pool size was fixed to various sizes to limit the training set size. The agent was trained for varying number of timesteps and then evaluated on the full distribution of environments.

All experiments are conducted on an Intel Core i9-10900X CPU and an NVIDIA GeForce RTX 2080 Ti GPU.

\subsection{Memory}

% learning actions, recognition and localization simultaneously feels in itself interesting
% how do we interpret such a model? are there papers for visualizing reinforcement learning weights

% detailed
% replicability

% pre-study
% implementation
% evaluation

% discuss state representation
% need more than the image
% - either memory or position or both

% seed + 1000 levels training
% >= s + ... for test
% do the same as procgen

% also same configuration possibly?
% more parallel environments seem to lead to stable training (relatively slow increase in reward though...)

% would be nice to have a varying number of targets
% also a done action
% measure false positives, false negatives etc.
% good replacement for zoom?
% relates to visual search literature
% the current setup is also the most natural for a computer vision system
% foveated vision is not very reasonable


% select one algorithm and clearly motivate why!
% probably PPO, but why?


% time to find targets could be measured in terms of execution time, 
% and number of timesteps. this way, it could be easier to incorporate 
% visual attention if it is deemed reasonable.

% how do we handle appearance of targets:
% can an agent learn to recognize anything out of the ordinary?
% maybe not the focus, but just the search

% an interesting question is how large search windows we can handle
% could be an alternative to zoom
% a large search window presumably requires attention


% memory is needed for (a) remembering previous locations, and (b) integrating features over time


% PPO
% PPG
