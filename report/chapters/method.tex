\chapter{Method}
\label{cha:method}

In this chapter, the method is described.

\section{Problem Statement}

% Formally define problem

% Given an n-dimensional signal with N i.i.d. targets located with some unknown distribution
% We assume that the targes correlate with the environment (signal).
% How should this be formally defined? 
% It *is* true in many cases.
% Although in general it might be that the target itself is the correlation?
% There is a certain cost involved with thinking that a point contains a target (processing and/or time)

% "Learning to optimize" has some good formulation tips
% explains that policy search is intractable
% Also uses a decaying step size, could be interesting for us as well
% (although we have multiple targets...)

\section{Environment}

The environments to be searched are drawn from a distribution, with varying but similar appearance, target locations and appearances. For all environments, the appearance correlates to the probability of targets.

\subsection{Action Space}

\subsection{Observation Space}

% window of signal

\subsection{Reward Signal}

% reward normalization: 
% reward clipping: done in "playing atari with deep reinforcement learning": they state that since the scale of scores varies from game to game they fix all positive rewards to be one and negative to be -1. 0 rewards are left unchanged. this limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. could also affect agent's performance as it cannot distinguish between rewards of different magnitudes.
% could be important for us if the number of targets and the reward varies a lot

\subsection{Algorithm}

% Actor critic

% Trained with PPO (possibly PPG)

\subsection{Feature extraction}

\subsection{Shared Network}

\subsection{Policy Head}

\subsection{Value Head}

\section{Experiments}

\subsection{Different Environments}

\subsection{Hyperparameters}

\subsection{Generalization}

\subsection{Extra Fields}

\subsection{Memory}

% learning actions, recognition and localization simultaneously feels in itself interesting
% how do we interpret such a model? are there papers for visualizing reinforcement learning weights

% detailed
% replicability

% pre-study
% implementation
% evaluation

% discuss state representation
% need more than the image
% - either memory or position or both

% seed + 1000 levels training
% >= s + ... for test
% do the same as procgen

% also same configuration possibly?
% more parallel environments seem to lead to stable training (relatively slow increase in reward though...)

% would be nice to have a varying number of targets
% also a done action
% measure false positives, false negatives etc.
% good replacement for zoom?
% relates to visual search literature
% the current setup is also the most natural for a computer vision system
% foveated vision is not very reasonable


% select one algorithm and clearly motivate why!
% probably PPO, but why?


% time to find targets could be measured in terms of execution time, 
% and number of timesteps. this way, it could be easier to incorporate 
% visual attention if it is deemed reasonable.

% how do we handle appearance of targets:
% can an agent learn to recognize anything out of the ordinary?
% maybe not the focus, but just the search

% an interesting question is how large search windows we can handle
% could be an alternative to zoom
% a large search window presumably requires attention


% memory is needed for (a) remembering previous locations, and (b) integrating features over time


% PPO
% PPG