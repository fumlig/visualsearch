\chapter{Method}
\label{cha:method}

In this chapter, we present our proposed methodology for answering the research questions.
The visual search problem is modelled as a sequential decision-making process.
A set of environments for the visual search problem is presented together with an approach for solving them with deep RL.
Finally, we describe a set of experiments to evaluate our approach.

%\section{Problem Statement}
%\label{sec:problem}

%The problem of searching for targets in unknown environments can be modelled as a sequential decision making process in which an agent alternates between receiving visual sensory input and guiding its attention.
%For the purposes of this thesis, we adopt the CMDP formalism \cite{kirk_survey_2022} and define the problem as follows:

%Let the task be a contextual POMDP \(\mathcal{M}\) where the environment's state includes the context \(c\), which we refer to as the \textit{seed}.
%The state also includes the searched space \(S \subset \mathbb{R}^d\) which we refer to as the \textit{scene}.
%In the scene, there is a set of \(N\) targets \(T = \{t_0, t_1, \dots t_N\}\) such that \(t_i \in S\).
%At each time step, the agent perceives a subspace \(V \subset S\) of the environment which we refer to as the \textit{view}.
%If \(T \cap V \neq \varnothing\) there are \(\left\lvert T \cap V \right\rvert\) targets in view.

%The actions \(a \in \mathcal{A}\) transform the view into a different subset of the scene.
%With a final action, the agent can indicate that there is one or more target in the view.
%The observations \(o \in \Omega\) are tuples \(o = \left\langle x, p \right\rangle\).
%Here, \(x \in \mathbb{R}^{3 \times w \times h}\) is an RGB image representing the current view, and \(p \in S\) is the position of the agent which uniquely identifies the view.
%The goal of the agent is to select actions that bring each target into view and indicate that they are visible, while minimizing the number of total steps.
%There is no inherent reward \(\mathcal{R}\) for this problem, so it has to be designed.
%Finally, the seed \(c\) determines the initial view \(V\), the location of the targets \(T\), the initial position \(p_0\) as well as the image observations \(x_t\) at each position \(p_t\).

\section{Environments}
\label{sec:environments}

To implement and evaluate agents that learn search to visually search for targets, we use three different simulated environments.
Each of the environments have different characteristics that test the applicability of the evaluated approach to different search scenarios.
In all environments, the appearance of the scenes and the location of targets are correlated to some degree.
This means that the agent should be able to learn common characteristics of each environment and use those to search more efficiently.

As \cite{cobbe_leveraging_2020} and \cite{mnih_asynchronous_2016}, we leverage procedural generation in all environments.
This gives us control over the difficulty of the environments as well as the number of training and test samples the agent is exposed to.
In the next sections, the visual search task that is tested by the environments is stated formally.
The particulars of each of the three environments are also described.

\subsection{Problem Statement}

The problem of searching for targets in unknown environments can be modelled as a sequential decision-making process in which an agent alternates between receiving visual sensory input and guiding its attention.
The goal of the agent is to locate all targets in a minimum amount of time by guiding its attention in the environment and actively detecting when targets are visible.
For the purposes of this thesis, we adopt the formalism in \cite{kirk_survey_2022} and let the task be a contextual POMDP \(\mathcal{M}\) with states \(\mathcal{S}\), actions \(\mathcal{A}\), reward \(\mathcal{R}\) and observations \(\Omega\).

\subsubsection{State Set}

The underlying state of the environment at each time step \(t\) is defined as follows:

\begin{equation}
    s_t = \left\langle c, S, T_t, V_t \right\rangle \in \mathcal{S}.
\end{equation}

Here, \(c \in \mathbb{M}\) is a \(seed\) that is used to generate samples of each environment.
\(S \subset \mathbb{R}^d\) is the searched space, which we refer to as the \textit{scene}.
A set of undetected targets \(T_t = \{t_0, t_1, \dots t_N\}\) are placed in the scene so that \(t_i \in S\).
The agent perceives a subspace \(V_t \subset S\) of the environment which we refer to as the \textit{view}.
If \(T_t \cap V_t \neq \varnothing\) there are \(\left\lvert T_t \cap V_t \right\rvert\) undetected targets in view.

% Finally, the \textit{seed} \(c \in \mathcal{N}\) determines the initial view \(V\) and the location of the targets \(T\)
% and the initial position \(p_0\) as well as the image observations \(x_t\) at each position \(p_t\).

%Each episode is terminated when all targets have been found, or after 1 000 time steps
%Terminating episodes early this way is common to speed up training~\cite{pardo_time_2018}.

%All three environments use the same observation space, action space and reward signal.

\subsubsection{Action Set}

The agent selects between the following set of actions:

\begin{equation}
    a_t \in \mathcal{A} = \left\lbrace \mathtt{INDICATE}, \mathtt{UP}, \mathtt{DOWN}, \mathtt{LEFT}, \mathtt{RIGHT} \right\rbrace
\end{equation}

The first action, \(\mathtt{INDICATE}\), is used by the agent to detect that a target is visible.
Once a target has been detected, it is removed from the set of targets in the next time step, \(T_{t+1}\).
The remaining actions are used to transform the view in the next time step, \(V_{t+1}\), to a new subspace of \(S\).
In our three environments, the agent may select between four moving actions, \(\mathtt{UP}\), \(\mathtt{DOWN}\), \(\mathtt{LEFT}\), and \(\mathtt{RIGHT}\), that move the view in each cardinal direction.

\subsubsection{Reward Signal}

The goal of the agent is to select actions that bring each target into view and detect them once they are visible, while minimizing the number of total time steps.
The reward signal should be designed so that the agent learns this behavior by maximizing the expected reward.
We use the following reward signal:

\begin{equation}
    r_t = \mathcal{R}(s_t, a_t) = h - 0.01 + 0.005d + 0.005e
\end{equation}

The reward signal consists of four different terms.
The first term is set to \(h = \left\lvert T_t \cap V_t \right\rvert\) if \(a_t = \mathtt{INDICATE}\), and \(h = 0\) otherwise.
This term is equal to the number of targets that were found at this time step.
The second term, \(-0.01\), is a constant time penalty ensures that the agent is rewarded for quick episode completion.
Through experimentation, we find that a larger time penalty than \(-0.01\) dominates the reward for finding targets.
This is potentially related to the number of time steps per episode -- with a time penalty that is too large, the reward \(h\) for finding targets has a relatively small impact on episode return for long episodes.

The two next terms are bonus rewards, intended to speed up learning by further encouraging desired behavior.
The term \(d = 1\) if \(a_t\) moves the view \(V_t\) is closer to then nearest target in \(T_t\) than \(V_{t-1}\) and \(d = 0\) otherwise.
Similarly, \(e = 1\) if \(a_t\) moves the view \(V_t\) to a previously unexplored subspace of \(S\), otherwise \(e = 0\).
Importantly, the coefficients of \(0.005\) on these bonuses ensure that their sum is not larger than the magnitude of the time penalty.
This is to ensure that exploration and moving towards targets does not seem more important to the agent than finishing the episode quickly.
Therefore, the bonuses may guide the agent towards finding targets but do not cause it to steer away from the underlying goal.

\subsubsection{Observation Set}

The observations that the agent receives at each time step are:

\begin{align}
    o_t & = \left\langle x_t, p_t \right\rangle \in \Omega \text{, where} \\
    x_t & \in \mathbb{R}^{3 \times 64 \times 64} \text{, and} \\
    p_t & = \left\langle p_{t,0}, p_{t,1} \right\rangle \in \{0, \dots, H-1\} \times \{0, \dots, W-1\}.
\end{align}

The first part of each observation, \(x_t\), is a \(64 \times 64\) RGB image of the current view \(V_t\) as it is perceived by the agent.
The second part of each observation, \(p_t\), is the current position of the agent that uniquely identifies the view \(V_t\).
For each possible view \(V\) there is a position \(p\) that represents the position of the camera.
In our environments, the camera can take any position in a two-dimensional grid.
As the seed determines the initial view \(V_0\), it also determines the initial position \(p_0\).
Finally, the seed determines the appearance of the searched scene and therefore the image observations that the agent perceives.

\subsection{Gaussian Environment}

The first environment is the simplest environment, where the correlation between scene appearance and target probability is clear.
During each episode reset, a \(1024 \times 1024\) RGB image is generated conditioned on the seed.
The seed determines the center of three (approximate) two-dimensional Gaussian distributions in the image.
The normalized sum of the distributions is used to determine the intensity of the blue channel of the image.
It is also used as a probability density function from which the three target locations are sampled, meaning that the higher the intensity in the blue channel, the more likely it is that a target will be there.
Targets are characterized by red \(8 \times 8\) squares.

The image is divided into a grid of \(10 \times 10\) steps, one per position \(p_t\).
The image \(x_t\) is the \(64 \times 64\) sub-image at the current position in the grid.
Each moving action translates the agent one step in corresponding cardinal direction in the grid.

The idea with this environment is to test whether an agent is able to utilize scene characteristics to search quicker.
An efficient searcher in this environment should prioritize locations where the intensity in the blue channel is high.
It should be able to use the gradient of the underlying Gaussian function to move towards locations with higher target probabilities, while avoiding revisiting locations and intelligently planning its search path.

\begin{figure}
    \centering
    \input{figures/gaussian.pgf}
    \caption[Gaussian environment]{Four samples of the first environment. Agent observes \(64 \times 64\) image at position \((2, 6)\). There are three blue spots and three red targets in each scene. Targets are more likely where the intensity of the spots is high.}
    \label{fig:gaussian}
\end{figure}

\subsection{Terrain Environment}

The second environment is similar to the first one, but intended to simulate a search scenario in realistic terrain.
Actions and observations behave the same as in the first environment.
At the start of each episode, a \(1024 \times 1024\) height map is generated using gradient noise.
The height map is used to determine the color of an image of the same size.
Lower heights are filled with blue ocean, and higher areas with green grass and brown mountains.
Three red targets are located with uniform probability along the shores, between mountains and water.
Green trees are also scattered around each scene, whose positions are sampled from the same distribution as that of the targets.

While this environment is similar to the first environment, it is less clear how to search efficiently in it.
There is higher variance between scene samples.
It is also less clear how the scene appearance is correlated to the probability of targets.
Although targets are never located in oceans or on mountains, these may still have to be traversed during searches.
For instance, if two targets are on separate islands the agent must travel over ocean to find both.
One can draw parallels between this environment and search-and-rescue scenarios with UAVs.

\begin{figure}
    \centering
    \input{figures/terrain.pgf}
    \caption[Terrain environment]{Four samples of the second environment. Terrain seen from above with red targets scattered and green trees scattered along shores. Agent observes \(64 \times 64\) image at position \((4, 3)\).}
    \label{fig:terrain}
\end{figure}

\subsection{Camera Environment}

The third environment is a three-dimensional version of the second one.
The height map is turned into a three-dimensional mesh, and the agent is placed at its center.
Targets are, as before, placed along island edges.

The agent observes the scene through a pan-tilt perspective camera.
Moving actions rotate the camera instead of translating it.
The \texttt{LEFT} and \texttt{RIGHT} actions control the yaw of the camera, while \texttt{DOWN} and \texttt{UP} control its pitch.

The yaw angle is divided into 20 steps between 0 and 360 degrees.
This angle wraps around, so that the agent can look around freely.
Similarly, the pitch angle is divided into 10 steps between 0 (straight forward) and -90 degrees (straight down), but without wrapping around.
This means that the camera can take \(20 \times 10 = 200\) different positions.

Targets are always visible from at least one camera position.
They may be visible from multiple positions, and the agent is expected to use the \texttt{INDICATE} action only when a target is as close to the center of the view image as possible.
This environment is intended to model more realistic scenarios where the image is more difficult to interpret.
In some scenarios it can be important that objects are not only localized, but localized accurately.

\begin{figure}
    \centering
    \input{figures/camera.pgf}
    \caption[Camera environment]{Four samples from the camera environment. Terrain seen from a pan-tilt camera. Each image shows the image observation at the initial position. The pan and tilt of the camera can be adjusted to move the view around.}
    \label{fig:camera}
\end{figure}

\section{Agents}
\label{sec:agents}

To design a reinforcement learning agent that effectively solves the task, we draw inspiration from several previous works and adapt them to better suit this particular task.
The final agent should be able to recognize targets, regardless of where they appear in view.
As environments are procedurally generated, a partial image observation of the scene is not sufficient for the agent to localize itself in it.
It is likely important for the agent to have access to its location, especially for large search spaces.
It should also be able to integrate features over time in order to remember which locations have been visited.
Remembering visual features of these locations may also be of importance, as it can provide clues for what is in their proximity.

Due to the advantages described in Section \ref{sec:policy-value}, we limit ourselves to policy gradient methods.
Specifically, we employ an actor-critic approach that estimates a policy and value function with a multi-headed neural network.
The neural network architecture should reflect the aforementioned requirements.

\subsection{Architecture}

We design our neural network architecture as follows:
A CNN takes the observed image \(x_t\) and encodes it into a latent representation \(y_t\).
This allows the agent to extract translation invariant features from observed images.
The latent representation, as well as the current position of the agent \(p_t\) is used as input to an RNN with recurrent state \(z_t\) and output \(h_t\).
Feeding both an image representation and the position of the agent to a recurrent step lets the agent remember visited locations and their appearance.
The output of the recurrent step is in turn is connected to an actor MLP head and a critic MLP head, which approximate the policy \(\pi\) and value function \(v\) respectively.
The architecture of the neural network is presented in Figure~\ref{fig:architecture}.

\begin{figure}
    \centering
    \includegraphics{figures/architecture.pdf}
    \caption[Neural network architecture]{The neural network architecture used for estimating the policy and value functions from image and position observations.}
    \label{fig:architecture}
\end{figure}

For the CNN, we use the same architecture as \cite{mnih_human-level_2015}.
The input image \(x_t \in \mathbb{R}^{3 \times 64 \times 64}\) is fed through three convolutional layers: the first layer convolves 32 filters of size \(8 \times 8\) with stride 4, the second convolves 64 filters of size \(4 \times 4\) with stride 2, and the third layer convolves 64 filters of size \(3 \times 3\) with stride 1.
This is followed by a final fully connected hidden layer with 512 outputs for the latent representation \(y_t \in \mathbb{R}^{512}\).

Both the policy and value network are fully connected networks.
The value network has one output for the value estimate.
The policy network has 5 outputs, the logits for each action.
Applying the softmax operation on this output gives the final action probabilities.
All network layers have ReLU activation functions, as suggested by \cite{henderson_deep_2018}.

Two agents with different RNN architectures are compared: one temporal, and one spatial.
While a temporal memory can retain location and appearance information over time, it is not cleared how this information is stored and whether it can be utilized properly.
We hypothesize that agents using temporal memories may struggle with learning good policies in large search spaces and those that require scene understanding, such as reasoning over previously visited locations.
Several results from works in embodied visual navigation indicate that spatial memories can give better results than temporal ones~\cite{parisotto_neural_2017,henriques_mapnet_2018,gupta_cognitive_2019,chaplot_object_2020}.
Taking inspiration from this, we investigate whether a spatial memory can be more useful than a temporal one when searching for targets.

\subsubsection{Temporal Memory}

The temporal memory is a single LSTM~\cite{hochreiter_long_1997} layer as proposed by \cite{hausknecht_deep_2017} and used in \cite{mnih_asynchronous_2016}, \cite{mirowski_learning_2017}, and \cite{gupta_cognitive_2019}.
%We refer to this memory as \(\text{RNN}_\text{lstm}\).
As input to the LSTM, we use the latent image representation \(y_t\) concatenated with one-hot encodings of each dimension of \(p_t\).
The LSTM layer has 128 hidden cells so that \(h_t\) has 128 dimensions.
Finally, \(z_t\) contains the hidden state \(h_t\) and cell state \(c_t\) of the LSTM.

\begin{equation}
    z_t = \left\langle h_t, c_t \right\rangle = \text{LSTM}(\left\lbrack y_t, \text{onehot}(p_{t,0}), \text{onehot}(p_{t,1}) \right\rbrack, h_{t-1}, c_{t-1})
\end{equation}

\subsubsection{Spatial Memory}

The spatial memory is composed of a readable and writable feature map.
This is a simplified version of the architecture suggested by \cite{parisotto_neural_2017}, similar in spirit to the architectures in \cite{henriques_mapnet_2018}, \cite{gupta_cognitive_2019} and \cite{chaplot_object_2020}.
It is defined by the following set of operations:

\begin{align}
    \rho_t &= \text{CNN}_\text{read}(z_t) \\
    \omega_t &= \text{MLP}_\text{write}(\left\lbrack y_t, \rho_t, z_t^{(p_t)} \right\rbrack) \\
    q_t &= \text{MLP}_\text{position}(\left\lbrack \text{onehot}(p_{t,0}), \text{onehot}(p_{t,1}) \right\rbrack) \\
    h_t &= \left\lbrack \rho_t, \omega_t, q_t \right\rbrack \\
    z_{t+1}^{(p')} &=
    \begin{cases}
        \omega_t & \text{if } p' = p_t \\
        z_{t}^{(p')} & \text{if } p' \neq p_t
    \end{cases}
\end{align}

Here, \(z_t \in \mathbb{R}^{64 \times 10 \times 10}\) is a feature map with one 64-dimensional feature vector per possible position \(p_t\).
The initial feature map is \(z_0 = 0\).
At each time step, the agent reads from this map using a CNN with three convolutional layers.
Each layer has 32 filters of size \(3 \times 3\), and uses a padding of 1.
A final fully connected layer produces a 64-dimensional read vector \(\rho_t\).
An MLP takes the read vector concatenated with \(y_t\) and the feature vector stored in the map at the current position \(z_t^{(p_t)}\), and produces a 64-dimensional write vector \(\omega_{t}\).
Another MLP takes a one-hot encoding of the position as input and outputs a 64-dimensional vector \(q_t\).
The read vector \(\rho_t\), the write vector \(\omega_{t}\), and the position vector \(q_t\) are concatenated and used as input \(h_t\) to the value and policy networks.
Finally, the feature map is updated so that \(z_{t+1}\) contains the write vector \(\omega_{t}\) at position \(p_t\).

By storing a representation of the observed image at its corresponding position in the feature map, the agent builds up an internal representation of the explored scene while searching\footnote{An illustration of the contents of the feature map \(z_t\) of a trained agent after a finished search can be found in Appendix~\ref{app:memory}.}.
Passing this structured representation through a set of convolutional layers yields an encoding of the entire explored scene which retains structured visual information.
This means that they can make use of visual features of previously explored locations, spatial relationships between these features, and the current position of the agent when approximating the policy and value function.
Since writes to the feature map are dependent on the previously stored feature vector at each position, the agent is also able to store new information every time it visits a position.

% Inhibition of return could be done programmatically, but there are advantages to learning it:
% In some cases it is good to return to visited locations, such as when traversing a visited region to reach an area with high probability.
% Cite paper that advocated for having as little bias as possible (since removing it can lead to better learned strategies).

\subsection{Training}

We train our neural networks with PPO~\cite{schulman_proximal_2017}, as described in Section~\ref{sec:ppo}.
Early experiments show that PPO gives good results, stable learning curves and good sample efficiency, which is in line with results reported by \cite{andrychowicz_what_2020}.
Furthermore, we use similar hyperparameters to \cite{cobbe_leveraging_2020}.
These are presented in Table~\ref{tab:hyperparameters}.
Many parallel environments seem to both speed up and stabilize training.
As suggested by \cite{andrychowicz_what_2020}, we initialize the policy output weights so that their mean is 0 and their standard deviation is low (\(10^{-2}\)).

We normalize the reward using a moving average.
This is done to limit the scale of the error derivatives.
Early experiments show that not normalizing the reward destabilized learning. % Likely due to time penalty and large variance in rewards
Similar results have been reported by \cite{andrychowicz_what_2020} and \cite{mnih_playing_2013}.
The Adam optimizer~\cite{kingma_adam_2017} is used in all experiments.
Finally, we decay the learning rate linearly so that it is zero at the final time step.
We find that this helps the agent with finding a better local optimum.

\begin{table}
    \centering
    \caption[PPO hyperparameters]{PPO hyperparameters used during training.}
    \input{tables/hyperparameters}
    \label{tab:hyperparameters}
\end{table}

\section{Experiments}
\label{sec:experiments}

We conduct three different experiments to evaluate our architectures and answer the research questions in \ref{sec:questions}.
Following the recommendations of \cite{henderson_deep_2018,colas_hitchhikers_2019,agarwal_deep_2022}, we report mean and standard deviation across a handful of seeds.

\subsection{Search Performance and Behavior}

To evaluate the quality of the learned policy, we compare the policies learned by our two architectures to a set of baseline policies:
one random, one greedy, and one exhaustive.
The random policy (Algorithm~\ref{alg:random}), selects random moving actions.
The greedy policy (Algorithm~\ref{alg:greedy}) keeps track of visited positions, and greedily selects actions that explore new positions.
If no such actions exist, it selects a random moving action.
The exhaustive policy (Algorithm~\ref{alg:exhaustive}) behaves similarly to the second one.
Instead of sampling exploring actions randomly, it selects exploring actions from a predetermined order.
In rectangular search spaces without obstacles, this policy leads to search paths that cover the whole search space with a small amount number of revisited positions.

All baselines automatically indicate when a target is visible.
We feel that offloading the detection from the baselines is reasonable.
Having the baselines randomly indicate that targets are visible would lead to excessively long search paths in moderately large search spaces.
By giving this advantage to the baselines, the emphasis of the comparison is put on the search path planning.

\begin{algorithm}
    \caption{Random baseline policy.}
    \label{alg:random}
    \input{algorithms/random}
\end{algorithm}

\begin{algorithm}
    \caption{Greedy baseline policy.}
    \label{alg:greedy}
    \input{algorithms/greedy}
\end{algorithm}

\begin{algorithm}
    \caption{Exhaustive baseline policy.}
    \label{alg:exhaustive}
    \input{algorithms/exhaustive}
\end{algorithm}

To get a better sense of how our agents compare to handcrafted search algorithms, use an additional baseline agent in the Gaussian environment.
This handcrafted baseline (Algorithm~\ref{alg:handcrafted}) selects actions that move the agent towards bluer regions.

\begin{algorithm}
    \caption{Handcrafted baseline policy (Gaussian environment).}
    \label{alg:handcrafted}
    \input{algorithms/handcrafted}
\end{algorithm}

For each of the three environments, we train our learning agents on the full distribution of samples.
Using a fixed set of 100 held out test samples, we measure the search path length and success rate of each learning agent and baseline.
When measuring the average search path length, we only include successful searches.
Finally, we investigate how the search performance of our approach compares to that of a human with prior knowledge of each environment.

We also report the SPL metric~\cite{anderson_evaluation_2018} for all agents, where the shortest path length is the optimal travel distance between the targets and the initial position of the agent.
The distance between two points is computed as the minimal number of actions to transform the view between the two.
Although finding the optimal path is not realistic in partially observable environments, SPL can still be useful when compared to that of a human.
For each metric and agent, we report both the mean and standard deviation across three runs.

\subsection{Size of Search Space}

The number of steps required to locate all targets in a scene is dependent on the size of the search space.
In small search spaces, the difference in performance between an intelligent searcher and a less intelligent one may be difficult to quantify.
Furthermore, a large scene is more difficult to search intelligently - more capacity is needed to remember visited locations and plan future steps.

To investigate how well each architecture scales to different search space sizes, we train them on three variants of the Gaussian environment.
The scene image is scaled up so that there are \(10 \times 10\), \(15 \times 15\) and \(20 \times 20\) possible camera positions in each environment.
We train and test both agents on the full distribution of samples from all three search space sizes respectively.
By varying the search space size in each environment, we can get a feel for how the size affects training times and the quality of the learned policies.
For each agent and search space size, we report how the average length and success rate changes while learning.

\subsection{Number of Training Samples}

In realistic scenarios, training agents on an unlimited number of samples is not possible.
It is more likely that there is a limited set of samples for an agent to learn from.
For this reason, it is interesting to quantify how the number of training samples influences generalization to the full distribution.

We do this by training our two agents on 500, 1000, 5000 and 10000 samples of the terrain environment.
Both agents are tested on held out samples from the full distribution of samples, as suggested by \cite{cobbe_leveraging_2020}.
This should illustrate how many samples are needed to generalize, as well as whether overfitting occurs.
For each agent and training set size, we report how the average length and success rate changes while learning.

\section{Implementation}

The environment is implemented with the Gym~\cite{brockman_openai_2016} interface.
The agents and RL algorithms are implemented in Python, with PyTorch~\cite{paszke_pytorch_2019} for automatic differentiation of the computation graphs.
PPO~\cite{schulman_proximal_2017} was implemented following the official implementation by the original authors,
and verified by testing on the benchmarks used in the original paper.
All experiments are conducted on an Intel Core i9-10900X CPU and an NVIDIA GeForce RTX 2080 Ti GPU.
Source code for environments, models and algorithms has been made publicly available~\footnote{\url{https://github.com/fumlig/visualsearch}}.
