\chapter{Method}
\label{cha:method}

In this chapter, the method used to answer the research questions in Section~\ref{sec:questions}
Section \ref{sec:problem} formalizes the problem solved.
Section \ref{sec:environments} details the environment used to evaluate solutions.
Section \ref{sec:approach} describes the approach used to solve the problem with a learning agent.
Section \ref{sec:experiments} describes the experiments conducted to answer research questions \ref{itm:rq2} and \ref{itm:rq3}.

\section{Problem Statement}
\label{sec:problem}

We can now formally define the problem of searching for targets in unknown environments,
adopting the CMDP formalism \cite{kirk_survey_2022}.
Let the task be a contextual POMDP \(\mathcal{M}\) where the state includes the context \(c\), which we refer to as the \textit{seed}.
The state includes a space \(S \subset \mathbb{R}^d\) which we refer to as the \textit{scene}.
In the scene, there is a set of \(N\) targets \(T = \{t_0, t_1, \dots t_N\}\) such that \(t_i \in S\).
At each time step, the agent perceives a subspace \(V \subset S\) of the environment which we refer to as the \textit{view}.
If \(T \cap V \neq \varnothing\) there are \(\left\lvert T \cap V \right\rvert\) targets in view.

The actions \(a \in \mathcal{A}\) transform the view into a different subset of the scene.
With a final action, the agent can indicate that there is one or more target in the view.
The observations \(o \in \Omega\) are tuples \(o = \left\langle x, p \right\rangle\).
Here, \(x \in \mathbb{R}^{3 \times w \times h}\) is an RGB image representing the current view, and \(p \in S\) is the position of the agent which uniquely identifies the view.
The goal of the agent is to select actions that bring each target into view and indicate that they are visible, while minimizing the number of total steps.
There is no inherent reward \(\mathcal{R}\) for this problem, so it has to be designed.
Finally, seed \(c\) determines the initial view \(V\), the location of the targets \(T\), the initial position \(p_0\) as well as the image observations \(x_t\) at each position \(p_t\).

\section{Environments}
\label{sec:environments}

To train an test an agent for the problem, we use three different environments with similar observation spaces, action spaces and reward signal.
Each environment have different characteristics that test the applicability of the evaluated approaches to different types search problems.
In all environments, the appearance of the scenes and the location of targets are correlated to some degree.
This means that the agent should be able to learn common characteristics of each environment and use those to search more efficiently.

As \cite{cobbe_procgen_2020} and \cite{mnih_asynchronous_2016}, we leverage procedural generation in all environments.
This gives us control over the difficulty of the environments as well as the number of training and test samples the agent is exposed to.
A seed determines the appearance of the scene, the location of the targets and the initial position of the agent.

Each episode is terminated when all targets have been found, or after 1000 time steps
Terminating episodes early this way is common to speed up training~\cite{pardo_timelimits_2022}.

\subsection{Observations, Actions and Reward}

All three environments use the same observation space, action space and reward signal.
The position and image observations are

\begin{align}
    o_t & = \left\langle x_t, p_t \right\rangle \text{, where} \\
    x_t & \in \mathbb{R}^{3 \times 64 \times 64} \text{, and} \\
    p_t & \in \{0, \dots, H-1\} \times \{0, \dots, W-1\}
\end{align}

All image observations are \(64 \times 64\) RGB images.
The agent moves in a \(H \times W\) grid, and we assume the presence of some oracle that provides the agent with its position.
In many realistic scenarios it is possible to determine the global position of an agent (GPS, pan/tilt, etc.).
If how each action moves the agent is well-defined, the relative position can be used instead of the absolute one.

The action space is the same in all environments:

\begin{equation}
    a_t \in \left\lbrace \mathtt{UP}, \mathtt{DOWN}, \mathtt{LEFT}, \mathtt{RIGHT}, \mathtt{INDICATE} \right\rbrace,
\end{equation}

where \(\mathtt{UP}\), \(\mathtt{DOWN}\), \(\mathtt{LEFT}\), and \(\mathtt{RIGHT}\) move the view one step in each direction.
This action space is realistic for many real-world search tasks,
such as search and rescue with a UAV, where the actions correspond to translations in each cardinal direction,
and surveillance with a pan-tilt-camera, where the actions correspond to pitch and yaw rotations.
The final action, \(\mathtt{INDICATE}\), is used to indicate that a target is in view.

The reward signal should be designed so that the agent learns a policy that achieves the goal of finding all targets with a minimal number of actions.
We use the following reward signal:

\begin{equation}
    r_t = h - 0.01 + 0.005d + 0.005e
\end{equation}

Here, \(h = \left\lvert T \cap V \right\rvert\) if \(a_t = \mathtt{INDICATE}\), and \(h = 0\) otherwise.
This term is equal to the number of targets that were found at this time step.
The constant penalty of \(-0.01\) ensures that the agent is rewarded for quick episode completion.
The term \(d = 1\) if \(p_t\) is closer to the nearest target than \(p_{t-1}\), otherwise \(d = 0\).
Similarly, \(e = 1\) if \(p_t\) has not been explored previously and \(e = 0\) otherwise.

Through experimentation, we find that a larger time penalty than \(-0.01\) dominates the reward for finding targets.
This is potentially related to the episode time -- with a time penalty thats too large, the reward \(h\) for finding targets has a relatively small impact on episode return for long episodes.
The two next terms are bonus rewards, intended to speed up learning by further encouraging desired behavior.
Actions that move the agent towards the nearest target and move the view to previously unseen regions are desirable.
Importantly, the sum of these bonuses is not larger than the magnitude of the time penalty.
This is to ensure that exploration and moving towards targets does not seem more important to the agent than finishing the episode quickly.
Therefore, the bonuses may guide the agent towards finding targets but do not cause it to steer away from the underlying goal.

\subsection{Gaussian Environment}

The first environment is the simplest environment, where the correlation between scene appearance and target probability is clear.
During each episode reset, a \(1024 \times 1024\) RGB image is generated conditioned on the seed.
The image contains three blue bumps, whose intensity is highest towards their center and wears off radially outwards.
The intensity wears off as an (approximate) Gaussian function.
The sum of the intensity in the blue channel in the image is used as a probability density function to select the location of three targets.
Targets are characterized by red \(8 \times 8\) squares.
The higher the intensity in the blue channel, the more likely that there is a target there.

The image is divided into a grid of \(10 \times 10\) steps, one per position \(p_t\).
The image \(x_t\) is the \(64 \times 64\) sub-image at the current position in the grid.
Each moving action translates the agent one step in corresponding cardinal direction in the grid.

The idea with this environment is to test whether an agent is able to utilize scene characteristics to search quicker.
An efficient searcher in this environment should prioritize locations where the intensity in the blue channel is high.
It should be able to use the gradient of the underlying Gaussian function to move towards locations with higher target probabilities, while avoiding revisiting locations and intelligently planning its search path.

\begin{figure}
    \centering
    \input{figures/gaussian.pgf}
    \label{fig:gaussian}
    \caption[Gaussian environment]{Four samples of the first environment. There are three blue bumps and three red targets in each scene. Targets are more likely where the intensity of the bumps is high.}
\end{figure}

\subsection{Terrain Environment}

The second environment is similar to the first one, but intended to simulate a search scenario in realistic terrain.
Actions and observations behave the same as in the first environment.
At the start of each episode, a \(1024 \times 1024\) height map is generated using gradient noise.
The height map is used to determine the color of an image of the same size.
Lower heights are filled with blue ocean, and higher areas with green grass and brown mountains.
Three red targets are located with uniform probability along the shores, between mountains and water.
Green trees are also scattered around each scene, whose positions are sampled from the same distribution as that of the targets.

While this environment is similar to the first environment, it is less clear how to search efficiently in it.
There is higher variance between scene samples.
It is also less clear how the scene appearance is correlated to the probability of targets.
It is desirable that a searching agent should learn to not search oceans and mountains.
Instead it should prioritize searching along the edges of land masses.
For some scene samples, there are multiple islands with small patches of land between them.
These patches could be used to quickly prioritize land while avoiding water.
One can draw parallels to search-and-rescue scenarios with UAVs or fire detection.

\begin{figure}
    \centering
    \input{figures/terrain.pgf}
    \label{fig:terrain}
    \caption[Terrain environment]{Four samples of the second environment. Terrain seen from above with red targets scattered and green trees scattered along shores.}
\end{figure}

\subsection{Camera Environment}

The third environment is a three-dimensional version of the second one.
The height map is turned into a three-dimensional mesh, and the agent is placed at its center.
The agent observes the scene through a pan-tilt perspective camera.
Targets are, as before, placed along island edges.

The \texttt{LEFT} and \texttt{RIGHT} actions control the yaw of the camera, while \texttt{DOWN} and \texttt{UP} control its pitch.
The yaw is divided into 20 steps between 0 and 360 degrees.
The yaw angle wraps around, so that the agent can look around freely.
Similarly, the pitch is divided into 10 steps between 0 (straight forward) and -90 degrees (straight down), but without wrapping around.
This means that the camera can take 200 different positions.

Targets are always visible from at least one camera position.
They may be visible from multiple positions, and the agent is expected to use the \texttt{INDICATE} action only when a target is as close to the center of the view image as possible.
This environment is intended to model more realistic scenarios where the image is more difficult to interpret.
In some scenarios it can be important that objects are not only localized, but localized accurately.

\begin{figure}
    \centering
    \input{figures/camera.pgf}
    \label{fig:camera}
    \caption[Camera environment]{Four samples from the camera environment. Terrain seen from a pan-tilt camera. The pan and tilt of the camera can be adjusted to move the view around.}
\end{figure}

\section{Approach}
\label{sec:approach}

To design an agent that effectively solves the task, we draw inspiration from several previous works and adapt them to better suit this particular task.
The final agent should be able to recognize targets, regardless of where they appear in view.
It is likely important for the agent to have access to its location, especially for large search spaces.
As environments are procedurally generated, a partial image observation of the scene is not sufficient to ground the agent in it.
It should also be able to integrate features over time in order to remember which locations have been visited.
Remembering visual features of of these locations may also be of importance, as it can provide clues for what is in their proximity.

Due to the advantages described in Section \ref{sec:policy-value}, we limit our approaches to policy gradient methods.
Specifically, we employ an actor-critic approach that estimates a policy and value function with a multi-headed neural network.
The neural network architecture should reflect the aforementioned requirements.

\subsection{Architecture}

We design our neural network architecture as follows:
A CNN takes the observed image \(x_t\) and encodes it into a latent representation \(h_t\).
This allows the agent to extract translation invariant features from observed images.
The latent representation, as well as the current position of the agent \(p_t\) is used as input to an RNN.
Feeding both an image representation and the position of the agent to a recurrent step lets the agent remember visited locations and their appearance.
The output of the recurrent step is in turn is connected to an actor MLP head and a critic MLP head, which approximate the policy \(\pi\) and value function \(v\) respectively.
The architecture of the neural network is presented in Figure ~\ref{fig:architecture}.

\begin{figure}
    \centering
    \includegraphics{figures/architecture.pdf}
    \label{fig:architecture}
    \caption[Network architecture]{The neural network architecture used for estimating the policy and value functions from image and position observations.}
\end{figure}

For the CNN, we use the same architecture as \cite{mnih_human_2015}.
The input image \(x_t \in \mathbb{R}^{3 \times 64 \times 64}\) is fed through three convolutional layers: the first layer convolves 32 filters of size \(8 \times 8\) with stride 4, the second convolves 64 filters of size \(4 \times 4\) with stride 2, and the third layer convolves 64 filters of size \(3 \times 3\) with stride 1.
This is followed by a final fully connected hidden layer with 512 outputs for the latent representation \(h_t \in \mathbb{R}^{512}\).

Both the policy and value network are fully connected networks.
The value network has one output for the value estimate.
The policy network has 5 outputs, the logits for each action.
Applying the softmax operation on this output gives the final action probabilities.
All network layers have ReLU activation functions, as suggested by \cite{henderson_matters_2018}.

Two RNN architectures are compared: one temporal, and one spatial.
While a temporal memory can retain location and appearance information over time, it is not cleared how this information is stored and whether it can be utilized properly.
We hypothesize that agents using temporal memories may struggle with learning good policies in large search spaces and those that require scene understanding, such as reasoning over previously visited locations.
Several results from works in embodied visual navigation indicate that spatial memories can give better results than temporal ones~\cite{parisotto_salakhutdinov_2017,henriques_vedaldi_2018,gupta_cognitive_2019,chaplot_semantic_2020}.
Taking inspiration from this, we investigate whether a spatial memory can be more useful than a temporal one when searching for targets.
The hyperparameters for the two RNN architectures are chosen so that they have a comparable number of trainable parameters.

\subsubsection{Temporal Memory}

The temporal memory is a single LSTM~\cite{hochreiter_schmidhuber_lstm_1997} layer, as proposed by \cite{hausknecht_stone_2017} and used in \cite{mnih_asynchronous_2016}, \cite{mirowski_navigate_2017}, and \cite{gupta_cognitive_2019}.
As input to the LSTM, we use the latent image representation \(h_t\) concatenated with one-hot encodings of each dimension of \(p_t\).
The LSTM layer has 128 hidden cells so that \(y_t\) has 128 dimensions.
Finally, \(z_t\) contains the hidden and cell states.

\subsubsection{Spatial Memory}

The spatial memory is composed of a readable and writable feature map, similar to those in \cite{parisotto_salakhutdinov_2017}, \cite{henriques_vedaldi_2018}, \cite{gupta_cognitive_2019} and \cite{chaplot_semantic_2020}.
It is defined by the following set of operations:

\begin{align}
    \rho_t &= \text{CNN}_\text{read}(z_t) \\
    \omega_t &= \text{MLP}_\text{write}(\left\lbrack h_t, \rho_t, z_t^{(p_t)} \right\rbrack) \\
    q_t &= \text{MLP}_\text{position}(\text{onehot}(p_t)) \\
    y_t &= \left\lbrack \rho_t, \omega_t, q_t \right\rbrack \\
    z_{t+1}^{(p')} &=
    \begin{cases}
        \omega_t & \text{if } p' = p_t \\
        z_{t}^{(p')} & \text{if} p' \neq p_t
    \end{cases}
\end{align}

Here, \(z_t \in \mathbb{R}^{64 \times 10 \times 10}\) is a feature map with one 64-dimensional feature vector per possible position \(p_t\).
At each time step, the agent reads from this map using a CNN with three convolutional layers.
Each layer has 32 filters of size \(3 \times 3\), and uses a padding of 1.
A final fully connected layer produces a 64-dimensional read vector \(\rho_t\).
An MLP takes the read vector concatenated with \(h_t\) and the feature vector stored in the map at the current position \(z_t^{(p_t)}\), and produces a 64-dimensional write vector \(\omega_{t}\).
Another MLP takes a one-hot encoding of the position as input and outputs a 64-dimensional vector \(q_t\).
The read vector \(\rho_t\), the write vector \(\omega_{t}\), and the position vector \(q_t\) are concatenated and used as input \(y_t\) to the value and policy networks.
This means that they can make use of visual features of previously explored locations, spatial relationships between these features, and the current position of the agent.
Finally, the feature map is updated so that \(z_{t+1}\) contains the write vector \(\omega_{t}\) at position \(p_t\).

\todo{Illustrate map updates?}

% Inhibition of return could be done programmatically, but there are advantages to learning it:
% In some cases it is good to return to visited locations, such as when traversing a visited region to reach an area with high probability.
% Cite paper that advocated for having as little bias as possible (since removing it can lead to better learned strategies).

\subsection{Training}

We train both agents with PPO~\cite{schulman_ppo_2017}, as described in Section~\ref{sec:ppo}.
Early experiments show that PPO gives good results, stable learning curves and good sample efficiency, which is in line with results reported by \cite{andrychowicz_empirical_2020}.
Furthermore, we use similar hyperparameters to \cite{cobbe_procgen_2020}.
These are presented in Table~\ref{tab:hyperparameters}.
Many parallel environments seem to both speed up and stabilize training. 
As suggested by \cite{andrychowicz_empirical_2020}, we initialize the policy output weights so that their mean is 0 and their standard deviation is low (\(10^{-2}\)).

We normalize the reward using a moving average.
This is done to limit the scale of the error derivatives.
Early experiments show that not normalizing the reward destabilized learning. % Likely due to time penalty and large variance in rewards
Similar results have been reported by \cite{andrychowicz_empirical_2020} and \cite{mnih_atari_2013}.
The Adam optimizer~\cite{kingma_ba_2017} is used in all experiments.
Finally, we decay the learning rate linearly so that it is zero at the final time step.
We find that this helps the agent with finding a better local optimum.

\begin{table}
    \centering
    \caption[PPO hyperparameters]{PPO hyperparameters used during training.}
    \input{tables/hyperparameters}
    \label{tab:hyperparameters}
\end{table}

\section{Experiments}
\label{sec:experiments}

We conduct four different experiments to evaluate our approaches and answer the research questions in \ref{sec:questions}. 
Following the recommendations of \cite{henderson_matters_2018}, \cite{colas_hitchhiker_2019} and \cite{agarwal_rlliable_2022}, we report mean and standard deviation across a handful of seeds.

\subsection{Quality of Search Behavior}

To evaluate the quality of the learned policy, we compare our approaches to a set of baseline policies: one random, one greedy, and one exhaustive.
The random policy (Algorithm ~\ref{alg:random}), selects random moving actions.
The greedy policy (Algorithm ~\ref{alg:greedy}) keeps track of visited positions, and greedily selects actions that explore new positions.
If no such actions exist, it selects a random moving action.
The exhaustive policy (Algorithm ~\ref{alg:exhaustive}) behaves similarly to the second one.
Instead of sampling exploring actions randomly, it selects exploring actions from a predetermined order.
In rectangular search spaces without obstacles, this policy leads to search paths that cover the whole search space with a small amount number of revisited positions.

All baselines automatically indicate when a target is visible.
We feel that offloading the detection from the baselines is reasonable.
Having the baselines randomly indicate that targets are visible would lead to excessively long search paths in moderately large search spaces.
By giving this advantage to the baselines, the emphasis of the comparison is put on the search path planning.

\begin{algorithm}
    \label{alg:random}
    \caption{Random Baseline Policy}
    \begin{algorithmic}
        \If{there is a target at \(p_t\)}
            \State \(a_t \leftarrow \mathtt{INDICATE}\)
        \Else
            \State \(\mathcal{A}_{\text{move}} \leftarrow \{\mathtt{LEFT}, \mathtt{UP}, \mathtt{RIGHT}, \mathtt{DOWN}\}\)
            \State \(a_t \leftarrow \text{sample from } \mathcal{A}_{\text{move}}\)
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \label{alg:greedy}
    \caption{Greedy Baseline Policy}
    \begin{algorithmic}
        \If{there is a target at \(p_t\)}
            \State \(a_t \leftarrow \mathtt{INDICATE}\)
        \Else
            \State \(\mathcal{V} \leftarrow \{p_t, p_{t-1}, \dots p_0\}\)
            \State \(\mathcal{A}_{\text{move}} \leftarrow \{\mathtt{LEFT}, \mathtt{UP}, \mathtt{RIGHT}, \mathtt{DOWN}\}\)
            \State \(\mathcal{A}_{\text{explore}} \leftarrow \{a | a \in \mathcal{A}_{\text{move}} \text{ moves the agent to an unvisited location } p \notin \mathcal{V}\}\) 
            \If{\(\mathcal{A}_{\text{explore}} \neq \varnothing\)}
                \State \(a_t \leftarrow \text{sample from } \mathcal{A}_{\text{explore}}\)
            \Else
                \State \(a_t \leftarrow \text{sample from } \mathcal{A}_{\text{move}}\)
            \EndIf
        \EndIf
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \label{alg:exhaustive}
    \caption{Exhaustive Baseline Policy}
    \begin{algorithmic}
        \If{there is a target at \(p_t\)}
            \State \(a_t \leftarrow \mathtt{INDICATE}\)
        \Else
            \State \(\mathcal{V} \leftarrow \{p_t, p_{t-1}, \dots p_0\}\)
            \State \(\mathcal{A}_{\text{move}} \leftarrow \{\mathtt{LEFT}, \mathtt{UP}, \mathtt{RIGHT}, \mathtt{DOWN}\}\)
            \State \(\mathcal{A}_{\text{explore}} \leftarrow \{a | a \in \mathcal{A}_{\text{move}} \text{ moves the agent to an unvisited location } p \notin \mathcal{V}\}\) 
            \If{\(\mathcal{A}_{\text{explore}} \neq \varnothing\)}
                \State \(a_t \leftarrow \text{the first } a \in \mathcal{A}_{\text{explore}} \text{ in } \langle \mathtt{LEFT}, \mathtt{UP}, \mathtt{RIGHT}, \mathtt{DOWN} \rangle\)
            \Else
                \State \(a_t \leftarrow \text{sample from } \mathcal{A}_{\text{move}}\)
            \EndIf
        \EndIf
    \end{algorithmic}
\end{algorithm}

For each of the three environments, we train our approaches on the full distribution of samples.
Using a fixed set of 100 held out test samples, we measure the search path length and success rate of each approach and baseline.
When measuring the average search path length, we only include successful searches.
Finally, we investigate how the search performance of our approach compares to that of a human with prior knowledge of each environment.
\todo{Recollect test results...}

We also report the SPL metric~\cite{anderson_evaluation_2018} for all agents, where the shortest path length is the optimal travel distance between the targets and the initial position of the agent.
The distance between two points is computed as the minimal number of actions to transform the view between the two.
Although finding the optimal path is not realistic in partially observable environments, SPL can still be useful when compared to that of a human.
For each metric and agent, we report both the mean and standard deviation across three runs.

\subsection{Size of Search Space}

The number of steps required to locate all targets in a scene is dependent on the size of the search space.
In small search spaces, the difference in performance between an intelligent searcher and a less intelligent one may be difficult to quantify.
Furthermore, a large scene is more difficult to search intelligently - more capacity is needed to remember visited locations and plan future steps.

To investigate how well our approach scales to different search space sizes, we train our approaches on three variants of the gaussian environment.
The scene image is scaled up so that there are \(10 \times 10\), \(15 \times 15\) and \(20 \times 20\) possible camera positions in each environment.
We train and test both approaches on the full distribution of samples from all three search space sizes respectively.
By varying the search space size in each environment, we can get a feel for how the size affects training times and the quality of the learned policies.
For each agent and search space size, we report how the average length and success rate changes while learning.

\subsection{Number of Training Samples}

In realistic scenarios, training agents on an unlimited number of samples is not possible.
It is more likely that there is a limited set of samples for an agent to learn from.
For this reason, it is interesting to quantify how the number of training samples affects generalization to the full distribution.

We do this by training our two agents on 500, 1000, 5000, 10000 and an unlimited number samples of the terrain environment.
Both agents are tested on held out samples from the full distribution of samples, as suggested by \cite{cobbe_procgen_2020}.
This should illustrate how many samples are needed to generalize, and if overfitting is an issue.
For each agent and training set size, we report how the average length and success rate changes while learning.

\subsection{Ablations}
\label{sec:ablations}

Finally, we investigate which aspects of our architecture are important for good search behavior through a set of ablation studies.
In the gaussian environment, we train three additional variants of our agent architecture.
One without the recurrent step, one without image observations, and one without position observations.
We compare the achieved performance to that of our full architectures.

\section{Implementation}

The environment is implemented with using the Gym~\cite{brockman_gym_2016} interface. The agent is implemented and RL algorithms are implemented with PyTorch~\cite{paszke_pytorch_nodate} for automatic differentiation of the computation graphs.
PPO~\cite{schulman_ppo_2017} was implemented following the official implementation by OpenAI,
and verified by testing on the benchmarks used in the original paper.
All experiments are conducted on an Intel Core i9-10900X CPU and an NVIDIA GeForce RTX 2080 Ti GPU.
The source code for environments, models and algorithms is available at \url{https://gitlab.liu.se/osklu414/tqdt33-masters-thesis}.
