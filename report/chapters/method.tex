\chapter{Method}
\label{cha:method}

% ~10p

% In this chapter, the method is described in a way which shows how the
% work was actually carried out. The description must be precise and
% well thought through. Consider the scientific term
% replicability. Replicability means that someone reading a scientific
% report should be able to follow the method description and then carry
% out the same study and check whether the results obtained are
% similar. Achieving replicability is not always relevant, but precision
% and clarity is.
% 
% Sometimes the work is separated into different parts, e.g.  pre-study,
% implementation and evaluation. In such cases it is recommended that
% the method chapter is structured accordingly with suitable named
% sub-headings.

In this chapter, the method used is described.
Section \ref{sec:problem} formalizes the problem solved.
Section \ref{sec:environments} details the environment used to evaluate solutions.
Section \ref{sec:baseline} describes the baseline learning method.
Section \ref{sec:approach} describes the approach used to solve the problem with a learning agent.
Section \ref{sec:experiments} describes the experiments conducted to answer research questions \ref{itm:rq2} and \ref{itm:rq3}.

\section{Problem Statement}
\label{sec:problem}

% Maybe keep the POMDP stuff in environment section

We can now formally define the problem of searching for targets in unknown environments,
adopting the CMDP formalism \cite{kirk_survey_2022}.

Let the task be a contextual POMDP \(\mathcal{M}\) where the state includes the context \(c\), which we refer to as the \textit{seed}.
The state includes a space \(S \subset \mathbb{R}^d\) which we refer to as the \textit{scene}.
In the scene, there is a set of \(N\) targets \(T = \{t_0, t_1, \dots t_N\}\) such that \(t_i \in S\).
At each time step, the agent perceives a subspace \(V \subset S\) of the environment which we refer to as the \textit{view}.
If \(T \cap V \neq \varnothing\) there are \(h = \left\lvert T \cap V \right\rvert\) targets in view.

The actions in \(\mathcal{A}\) transform the view into a different subset of the scene.
With a final trigger action, the agent can indicate that there is one or more target in the view.
The observations \(o \in \Omega\) are tuples \(o = \left\langle x, p \right\rangle\).
Here, \(x \in \mathbb{R}^{3 \times W \times H}\) is an RGB image representing the current view, and \(p \in S\) is the position of the agent which uniquely identifies the view.
The seed \(c\) determines the initial view \(V\), the location of the targets \(T\), the initial position \(p_0\) as well as the image observations \(x_t\) at each position \(p_t\).

The goal of the agent is to select actions that bring each target into view and indicate that they are visible with the trigger action, while minimizing the number of total steps.

%\todo{Could reformalize so that targets are volumes instead.}

\section{Approach}
\label{sec:approach}

Due to time constraints and the advantages described in Section \ref{sec:policy-value}, we limit our approaches to policy gradients.
Specifically, we use an actor-critic method.
The policy and value function are approximated using a multi-headed neural network.
The neural network of the agent is split into four parts:
A feature extraction network is connected to a recurrent network.
The recurrent network is in turn is connected to an actor network head and a critic network head, which approximate the policy and value function respectively.

To design an agent that effectively solves the task, we draw inspiration from several previous works and adapt them to better suit this particular task.
The final agent should be able to recognize targets, regardless of where they appear in view.
It should also be able to integrate features over time in order to remember which locations have been visited.
Remembering features of of these locations may also be of importance, as it can provide clues for what is in their proximity. 

With this in mind, we use an episodic spatial memory that stores the agent's belief state.

\begin{itemize}
    \item One memory slot for each possible position \(p_t\).
    \item The agent writes a representation of \(x_t\) and potentially other features like \(a_{t-1}\) to the memory at position \(p_t\).
    \item The memory can be read with a CNN which creates a representation \(h_t\) of the whole memory that encodes spatial relations.
    \item The representation is used to approximate the policy and value.
    \item This way, the agent can associate information with visited.
\end{itemize}

The architecture of the neural network is presented in Figure ~\ref{fig:approach}.

\todo{Describe approach once it is fully decided. Will be a (spatial) memory that allows for reasoning over the whole explored scene and history of observations.}

\begin{figure}
    \centering
    %\input{figures/approach}
    \label{fig:approach}
    \caption[Approach architecture]{The architecture.}
\end{figure}


% inhibition of return could be done programmatically, but there are advantages to learning it:
% in some cases it is good to return to visited locations, such as when traversing a visited region to reach an area with high probability
% cite paper that advocated for having as little bias as possible (since removing it can lead to better learned strategies)

\section{Baselines}
\label{sec:baseline}

\todo{Alternatively, we can use just one RL baseline and perform ablation studies.}

We compare our approach to three different baselines.
All three baselines use the same architecture for the policy and value heads as our approach.

The first baseline is the agent from \cite{mnih_human_2015}, modified to be trainable with an actor-critic algorithm.
This has previously been used as a baseline in \cite{mirowski_navigate_2017} and \cite{cobbe_procgen_2020}.
The agent receives only image observations, and uses a CNN with three layers for feature extraction.

The second baseline is a recurrent version of the first baseline, the same architecture used in \cite{mnih_asynchronous_2016}, \cite{mirowski_navigate_2017}, and \cite{gupta_cognitive_2019}.
After the initial CNN there is an LSTM layer with 256 cells,
which lets the agent integrate visual information over time.

The third baseline also uses the position of the agent, which is encoded as one one-hot vector per dimension. 
The encoded positions are concatenated together with the output of the CNN and used as input to the LSTM layer.
Including position should help the agent avoid revisiting previous locations and understand which parts of the scene are yet to be explored. 

As scenes differ in appearance, the image is not enough to ground the agent.
For large scenes, the position is likely to be needed as a way for the agent to orient itself.
This could make it difficult for the agent to search exhaustively.
With these three baselines, we can clearly see what role the observations and memory plays for the search task.

\section{Environments}
\label{sec:environments}

To train an test an agent for the problem, we use three different environments.
The three environments have different characteristics to test the applicability of the evaluated approaches to different search problems.
In each environment there is a scene with a background of distractors and a foreground of targets.
The first two environments approximate the problem of searching with a pan-tilt camera with a 2D environment.
The third environment uses a simulated 3D environment with a perspective projection camera. 

As \cite{cobbe_procgen_2020,mnih_asynchronous_2016}, we leverage procedural generation in all environments.
The seed determines the appearance of the scene, the location of the targets and the initial position of the agent.
Furthermore, the appearance of the scenes and the location of targets are correlated to some degree in all environments.
This means that the agent should be able to search more efficiently using knowledge from the scene.

The position and image observations are

\begin{align*}
    x & \in \mathbb{R}^{3 \times 64 \times 64} \text{, and} \\
    p & \in \{0, \dots, 15\} \times \{0, \dots, 15\}
\end{align*}

The agent always moves in a \(16 \times 16\) grid.
For the position part of the observations, we assume the presence of some oracle.
In many realistic scenarios it is possible to determine the global position of an agent (GPS, pan/tilt, etc.).
If how each action moves the agent is well-defined, the relative position can be used instead of the absolute one.

The action space is the same in all environments:

\begin{equation*}
    \mathcal{A} = \left\lbrace \mathtt{UP}, \mathtt{DOWN}, \mathtt{LEFT}, \mathtt{RIGHT}, \mathtt{TRIGGER} \right\rbrace,
\end{equation*}

\(\mathtt{UP}\), \(\mathtt{DOWN}\), \(\mathtt{LEFT}\), and \(\mathtt{RIGHT}\) move the view one step in each direction.
This action space is realistic for many real-world search tasks,
such as search and rescue with a UAV, where the actions correspond to translations in each cardinal direction,
and surveillance with a pan-tilt-camera, where the actions correspond to pitch and yaw rotations.
\(\mathtt{TRIGGER}\) is used to indicate that a target is in view, and functions as the done signal recommended by \cite{anderson_evaluation_2018}.

We experiment with three rewards signals.
The first reward signal is defined as

\begin{equation*}
    \mathcal{R}(s_t, a_t) =
    \begin{cases}
        100h & \text{if \(a_t = \mathtt{TRIGGER}\) and a target is in view,} \\
        -1   & \text{otherwise.}
    \end{cases}
\end{equation*}

% For bad policies, finding targets should be important, and the episode length not so much.
% When the policy has managed to learn how to localize targets, the episode length should become more important.
% How do we balance these two? Finding targets is implicitly important as it shortens the episode length.

with \(h = \left\lvert T \cap V \right\rvert\). 
Optimizing for this reward will lead to desirable behavior as the agent is rewarded for finding all targets quickly.
Early experiments show that a constant penalty \(r_t = -1\) that simply incentivize the agent to complete the episode as quickly as possible is too slow to train for moderately large scenes.
The reward for finding a target speeds up training without deviating from the goal of the task -
targets should be triggered when in view, but triggers when targets are out of view should be penalized.
The constant penalty of \(-1\) in all other cases assures that the agent is rewarded for quick episode completion.
It seems to be important that the magnitude of the reward is similar to the return obtained in an unfinished episode.
We suspect that the return when only hitting one target must be substantially higher than the return for not finding any targets,
or the agent does not understand the benefit of hitting targets without finishing the episode. 

In practice, early experiments show that even this reward might be too sparse for large search spaces.
To speed up training, we experiment with two extensions to the reward:

\begin{equation*}
    \mathcal{R}'(s_t, a_t) =
    \begin{cases}
        1 & \text{if \(a_t \neq \mathtt{TRIGGER}\) moves the view closer to the nearest target, and} \\
        \mathcal{R}(s_t, a_t) & \text{otherwise}.
    \end{cases}
\end{equation*}

\begin{equation*}
    \mathcal{R}''(s_t, a_t) =
    \begin{cases}
        1 & \text{if \(a_t \neq \mathtt{TRIGGER}\) moves the view to a previously unseen subspace, and} \\
        \mathcal{R}(s_t, a_t) & \text{otherwise}.
    \end{cases}
\end{equation*}

These three reward signals are interesting to compare for a few reasons.
\(R\) does not clearly mediate to the agent what actions are desireable until a target is found.
It may therefore lead to slow learning, but it also does not steer away from the goal of finding targets quickly.
\(R'\) uses the supervised distance between targets and the agents, which is available during training.
This is similar to the reward used by \cite{caicedo_active_2015} and \cite{ghesu_artificial_2016}.
In addition to speeding up learning, this reward may help the agent pick up correlations between scene appearance and target probability.
However, it can never yield policies that search exhaustively as such actions are never rewarded.
It may therefore perform worse during testing where the reward is not available to the agent.
It will also not learn to take the shortest paths in the general case, as selecting waypoints greedily does not yield optimal paths.
\(R''\) strikes a balance between the two other signals by instead encouraging exploration.
This should cause the agent to learn to search the environment exhaustively.

Each episode is terminated when all targets have been found, or after 1000 time steps
Terminating episodes early this way is common to speed up training~\cite{pardo_timelimits_2022}.

\subsection{Gaussian Environment}

The first environment is the simplest environment. 
The scene's appearance is given by a \(1024 \times 1024\) RGB image.
The agent observes a \(64 \times 64\) sub-image at each time step.
Which sub-image is viewed is determined by the position of the agent.
Each non-trigger action move the agent one step in the corresponding cardinal direction.

In the image there are three Gaussian kernels with random positions.
The height of the kernel is indicated by a higher intensity in the blue channel.
Three targets are scattered around the environment, visualized as red squares.
The larger the sum of the gaussian kernels, the more likely it is that there is a target present.

The idea with this environment is to test that the method learns what we want it to learn.
There is a clear correlation between observations and desirable actions.
It is also easy to determine whether the agent acts well in this environment.
% Our feeling is that this is something that previous similar works has not done. % cite

%To investigate the effect the scene size has on the agent's ability to learn a good policy,
%we use three different versions of the environment.

\begin{figure}
    \centering
    \input{figures/gaussian.pgf}
    \label{fig:gaussian}
    %\vspace*{-1cm}
    \caption[Gaussian environment]{Four samples of the first environment. There are three gaussian kernels in the environment, whose heights are indicated by the intensity of the blue channel. There are three targets in the environment, whose location is sampled from the distribution defined by the sum of the three gaussian kernels. The white border indicates the current view.}
\end{figure}

\subsection{Terrain Environment}

The second environment is similar to the first one, but intended to look like realistic terrain.
It roughly represents a UAV search-and-rescue scenario.
To generate the image, gradient noise is generated and used as a height map.
The height map determines the color of the image, which is picked to resemble water, sand, grass and rock.
The height also correlates to the probability of targets.
Specifically, three targets are located with uniform probability between shores and mountain bases.
It is desirable that a searching agent should learn to not search oceans and lakes, and instead prioritize searching along the edges of land masses.

\begin{figure}
    \centering
    \input{figures/terrain.pgf}
    \label{fig:terrain}
    %\vspace*{-1cm}
    \caption[Terrain environment]{Four samples of the terrain environment. Terrain seen from above with red targets scattered along island edges.}
\end{figure}

\subsection{Camera Environment}

The third environment is a three-dimensional version of the second one.
The height map is turned into a three-dimensional mesh, and the agent is placed at its center.
The agent observes the scene through a pan-tilt perspective camera.
Targets are, as before, placed along island edges.
The agent is therefore expected to avoid searching the skies and oceans.
This environment is intended to model more realistic scenarios where the image is more difficult to interpret.
The \texttt{LEFT} and \texttt{RIGHT} actions control the yaw of the camera, while \texttt{DOWN} and \texttt{UP} control its pitch.

\begin{figure}
    \centering
    \input{figures/camera.pgf}
    \label{fig:camera}
    %\vspace*{-1cm}
    \caption[Camera environment]{Four samples from the camera environment. Terrain seen from a pan-tilt camera. The pan and tilt of the camera can be adjusted to move the view around. In the left image, a target can be seen.}
\end{figure}

\section{Experiments}
\label{sec:experiments}

\todo{Could remove baseline without recurrent step and instead do ablation studies.}

To compare our approach to the different baselines,
we train and test all agents on all three environments.
The agents are trained on the full distribution of environments,
We do this for all three reward signals \(R\), \(R'\) and \(R''\).
All agents are trained for 25 million time steps.
They are then tested on 1000 held out samples from each environment.

For each agent, environment, and reward signal we report the average return and episode length over time during training.
During testing, we compare the learning agents to random walk, exhaustive search and a human searcher with prior knowledge of the characteristics of the searched environments.
We report the SPL metric for all agents, where the shortest path length is the optimal travel distance between the targets and the initial position of the agent.
The distance between two points is computed as the minimal number of actions to transform the view between the two.
Although finding the optimal path is not realistic in partially observable environments, SPL can still be useful when compared to that of a human.

During testing, we increase the maximum episode length from 1000 time steps to 5000 time steps.
This is to give more accurate metrics for episodes that do not terminate within 1000 time steps. 

Additionally, we conduct experiments to measure the generalization capabilities of the agents.
For this, we use the terrain environment only.
The training set size is varied from 100, 1000, 10 000, and 100 000 samples by limiting the seed pool used to generate the environments.
We test on held out samples from the full distribution, as done by \cite{cobbe_procgen_2020}.
This way, we can get a sense of how much data and simulation is required to apply the approach to real-world tasks.
For each training set size, we train the agents until convergence.
Once again, we compare all three reward signals.

Following the recommendations of \cite{henderson_matters_2018} and \cite{agarwal_rlliable_2022}, we report confidence intervals across a handful of seeds.
We run all experiments across 3 different runs, and use the aggregate metrics proposed by \cite{agarwal_rlliable_2022}.

All agents are trained with PPO~\cite{schulman_ppo_2017}.
Early experiments show that PPO gives good results, stable learning curves and good sample efficiency, which is in line with results reported by \cite{andrychowicz_empirical_2020}.
Furthermore, we use similar hyperparameters as in \cite{cobbe_procgen_2020} (Table~\ref{tab:hyperparameters}).
More parallel environments seem to both speed up and stabilize training.
%Furthermore, we reduce the number of steps per rollout to be much smaller than the episode length.
%This is recommended by \cite{schulman_ppo_2017}. 
As suggested by \cite{andrychowicz_empirical_2020}, we initialize the policy output weights so that their mean is 0 and their standard deviation is low (\(10^{-2}\)).
The Adam optimizer~\cite{kingma_ba_2017} is used in all experiments.

\begin{table}
    \centering
    \caption{Hyperparameters used during training.}
    \input{tables/hparams}
    \label{tab:hyperparameters}
\end{table}

We normalize the reward to limit the scale of the error derivatives.
Early experiments show that this stabilizes training.
Similar results have been reported by \cite{andrychowicz_empirical_2020} and \cite{mnih_atari_2013}.
Finally, we decay the learning rate linearly so that it is zero at the final time step.
We find that this helps the agent with bettering its policy towards the end of training.
%They use reward clipping instead which diminishes the meaning of reward magnitude.

\section{Implementation}

The environment is implemented with Gym~\cite{brockman_gym_2016}. The agent is implemented and RL algorithms are implemented with PyTorch~\cite{paszke_pytorch_nodate} for automatic differentiation of the computation graphs.
Proximal policy optimization~\cite{schulman_ppo_2017} was implemented following the official implementation by OpenAI,
and verified by testing on the benchmarks used in the original paper.
All experiments are conducted on an Intel Core i9-10900X CPU and an NVIDIA GeForce RTX 2080 Ti GPU.
The source code for environments, agents and RL algorithms is available at \dots
\todo{Add link.}

% learning actions, recognition and localization simultaneously feels in itself interesting
% how do we interpret such a model? are there papers for visualizing reinforcement learning weights

% discuss state representation
% need more than the image
% - either memory or position or both

% also same configuration possibly?
% more parallel environments seem to lead to stable training (relatively slow increase in reward though...)

% an interesting question is how large search windows we can handle
% could be an alternative to zoom
% a large search window presumably requires attention


% memory is needed for (a) remembering previous locations, and (b) integrating features over time

% PPO
% PPG
