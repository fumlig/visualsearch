\chapter{Method}
\label{cha:method}

% ~10p

% In this chapter, the method is described in a way which shows how the
% work was actually carried out. The description must be precise and
% well thought through. Consider the scientific term
% replicability. Replicability means that someone reading a scientific
% report should be able to follow the method description and then carry
% out the same study and check whether the results obtained are
% similar. Achieving replicability is not always relevant, but precision
% and clarity is.
% 
% Sometimes the work is separated into different parts, e.g.  pre-study,
% implementation and evaluation. In such cases it is recommended that
% the method chapter is structured accordingly with suitable named
% sub-headings.

In this chapter, the method used is described.
Section \ref{sec:problem} formalizes the problem solved.
Section \ref{sec:environment} details the environment used to evaluate solutions.
Section \ref{sec:baseline} describes the baseline learning method.
Section \ref{sec:approach} describes the approach used to solve the problem with a learning agent.
Section \ref{sec:experiments} describes the experiments conducted to answer research questions \ref{itm:rq2} and \ref{itm:rq3}.

\section{Problem Statement}
\label{sec:problem}

% Maybe keep the POMDP stuff in environment section

We can now formally define the problem of searching for targets in unknown environments.

We denote the task by \(\langle \mathcal{M}, \mathcal{T}_0 \rangle\), where \(\mathcal{M}\) is a POMDP and \(\mathcal{T}_0\) is the probability distribution on the initial states.
The state is defined by a (Euclidean) space \(S \subset \mathbb{R}^d\) which we refer to as the \textit{scene}.
At each timestep, the agent observes a subspace \(V \subset S\) of the environment which we refer to as the \textit{view}.
The actions in \(\mathcal{A}\) transform the view.
In the scene, there is a set of \(N\) targets \(T = \{t_0, t_1, \dots t_N | t_i \in S\}\).
With a final trigger action, the agent can indicate that there is one or more target in the view.
The goal of the agent is to select actions that bring each target into view and indicate that they are visible with the trigger action, while minimizing the number of actions taken.
The observations \(o \in \Omega\) are tuples \(o = \left\langle x, p \right\rangle\),
where \(x \in \mathbb{R}^{3 \times W \times H}\) is an RGB image representing the current view, and \(p \in S\) is the position of the agent.
If \(T \cap V \neq \varnothing\) there are \(h = \left\lvert T \cap V \right\rvert\) targets in view.

\section{Environments}
\label{sec:environments}

To train an test an agent for the problem, we use three different environments.
The three environments have different characteristics to test the applicability of the evaluated approaches.
In each environment there is a scene with a background of distractors and a foreground of targets.
The scenes are drawn from some unknown distribution.
The background and foreground are assumed to be correlated.
This means that by looking at the background, an agent should sometimes be able to deduce a suitable action.

For the position, we assume the presense of some oracle.
In many realistic scenarios this is the case (GPS, pan/tilt, etc.).
If how each action moves the agent is well-defined, we do not need the position at all.
We can use relative positions instead of absolute ones.
Some of the baselines do not use the position.

The action space is the same in all environments:

\[
    \mathcal{A} = \lbrace \mathtt{UP}, \mathtt{DOWN}, \mathtt{LEFT}, \mathtt{RIGHT}, \mathtt{TRIGGER} \rbrace,
\]

\(\mathtt{UP}\), \(\mathtt{DOWN}\), \(\mathtt{LEFT}\), and \(\mathtt{RIGHT}\) translate the view and \(\mathtt{TRIGGER}\) indicates that a target is in view.
% rotates in third environment though

We experiment with three rewards signals.
The first reward signal is defined as

\[
    \mathcal{R}(s_t, a_t) =
    \begin{cases}
        10h & \text{if \(a_t = \mathtt{TRIGGER}\) and a target is in view,} \\
        -1  & \text{otherwise.}
    \end{cases}
\]

with \(h = \left\lvert T \cap V \right\rvert\). 
We argue that this reward provides a suitable inductive bias for the task at hand.
Early experiments show that a constant reward of \(r_t = -1\) that simply incentivizes the agent to complete the episode as quickly as possible converge too slowly for large state spaces.
The reward for finding a target speeds up training without deviating from the goal of the task -
targets should be triggered when in view, but triggers when targets are out of view should be penalized.
The constant penalty of \(-1\) in all other cases assures that the agent is rewarded for quick episode completion.

In practice, early experiments show that even this reward might be too sparse.
To speed up training, we experiment with two extensions to the reward:

\[
    \mathcal{R}'(s_t, a_t) =
    \begin{cases}
        1 & \text{if \(a_t \neq \mathtt{TRIGGER}\) moves the view closer to the nearest target, and} \\
        \mathcal{R}(s_t, a_t) & \text{otherwise}.
    \end{cases}
\]

\[
    \mathcal{R}''(s_t, a_t) =
    \begin{cases}
        1 & \text{if \(a_t \neq \mathtt{TRIGGER}\) moves the view to a previously unseen subspace, and} \\
        \mathcal{R}(s_t, a_t) & \text{otherwise}.
    \end{cases}
\]


These three reward signals are interesting to compare for a few reasons.
\(R\) does not clearly mediate to the agent what actions are desireable until a target is found.
It may therefore lead to slow learning, but it also does not steer away from the goal of finding targets quickly.
\(R'\) uses the supervised distance between targets and the agents, which is available during training.
This is similar to the reward used by \cite{caicedo_active_2015} and \cite{ghesu_multi_scale_2019}.
In addition to speeding up learning, we hypothesize that this reward may help the agent pick up correlations between scene appearance and target probability.
However, it can never yield policies that search exhaustively as such actions are never rewarded.
It may therefore perform worse during testing where the reward is not available to the agent.
It will also not learn to take the shortest paths in the general case, as selecting waypoints greedily does not yield optimal paths.
\(R''\) strikes a balance between the two other signals by instead incentivizing exploration.
This should cause the agent to learn to search the environment exhaustively.
% tabu search
% optimal paths

The episode is terminated when all targets have been found, or when 1000 time steps have passed.
Terminating episodes early this way is common to speed up training~\cite{pardo_timelimits_2022}.

% Averaged over all possible samples, the probability of targets should be uniform over the scene.
% to make comparison with exhaustive search fair
% I think?

\subsection{Gaussian Environment}
% environment 1: easy, understandable

The first environment is the simplest environment. 
The scene is described by a 256x256 RGB image.
The agent observes a 64x64 sub-image at each time step.
In the image there are three Gaussian kernels with random positions.
The height of the kernel is indicated by a higher intensity in the blue channel.
Targets are 1x1 pixels in the red channel.
The locations of the targets are randomized weighted by the height of the Gaussian kernels.
This means that the more intense the blue channel, the higher the probability of a target.
The idea with this environment is to test that the method learns what we want it to learn.
There is a clear correlation between observations and desirable actions.
It is also easy to determine whether the agent acts well in this environment.
Our feeling is that this is something that previous similar works has not done. % cite

\begin{figure}
    \centering
    \input{figures/gaussian.pgf}
    \label{fig:gaussian}
    %\vspace*{-1cm}
    \caption[Gaussian environment]{Four samples of the first environment. There are three gaussian kernels in the environment, whose height is visualized with the blue channel. There are three targets in the environment, whose location is sampled from the distribution defined by the sum of the three gaussian kernels.}
\end{figure}

\subsection{Terrain Environment}
% environment 2: medium, procedural and configurable

The second environment is intended to look like realistic terrain.
The scene's appearance is given by a 512x512 RGB image.
The agent's view is a 64x64 sub-image.
Gradient noise is generated and used as a height map.
The height map determines the color of the terrain.
The height also correlates to the probability of targets.
Specifically, targets are located between shores and mountain bases.
There are 10 targets in each scene.

The environment roughly corresponds to a UAV search-and-rescue scenario.
It is desirable that a searching agent should learn to not search oceans and lakes.
The agent should prioritize searching along the edges of land masses.
The appearance of the environment is highly configurable and has high variance.
We use this environment for evaluating the generalization capabilities of agents.

\begin{figure}
    \centering
    \input{figures/terrain.pgf}
    \label{fig:terrain}
    %\vspace*{-1cm}
    \caption[Terrain environment]{Three samples of the terrain environment. Terrain seen from above with red targets scattered along island edges. The white border indicates the agent's current view.}
\end{figure}

\subsection{Camera Environment}
% environment 3: hard, realistic

The third environment is a three-dimensional version of the second one.
The scene 

This environment is intended to model more realistic scenarios where the image is more difficult to interpret.

\begin{figure}
    \centering
    \input{figures/camera.pgf}
    \label{fig:camera}
    %\vspace*{-1cm}
    \caption[Camera environment]{Three samples from the camera environment. Terrain seen from a pan-tilt-zoom camera. The pan and tilt of the camera can be adjusted to move the view around. In the mid-right image, a target can be seen.}
\end{figure}

\section{Approach}
\label{sec:approach}


To design an agent that effectively solves the task, we draw inspiration from several previous works and adapt them to better suit this particular task.
Considering 

Due to time constraints and the advantages described in Section \ref{sec:policy-value}, we limit our approaches to policy gradients.
Specifically, we use an actor-critic method.
The policy and value function are approximated using a multi-headed neural network.
The neural network of the agent is split into four parts:
A feature extraction network is connected to a recurrent network.
The recurrent network is in turn is connected to an actor network head and a critic network head, which approximate the policy and value function respectively.


The network is trained with PPO~\cite{schulman_ppo_2017}.
Early experiments show that PPO gives stable learning curves and good sample efficiency, which is in line with results reported by ~\cite{}.
We 

\cite{andrychowicz_empirical_2020}


% should we rescale to 84x84 instead?

% https://www.reddit.com/r/reinforcementlearning/comments/ogwct6/how_to_deal_with_catastrophic_forgetting/
% hopefully reward normalization fixes it
% someone here does not recommend to clip value loss 

% https://arxiv.org/pdf/2006.05990.pdf
% cite this one...


% save latent representation of map?
% As in "Cognitive Mapping and Planning for Visual Navigation"

% self-supervised pretraining?
% https://arxiv.org/pdf/2003.14323.pdf


The agent is trained with reinforcement learning using Proximal Policy Optimization~\cite{proxim} with function approximation using neural networks.

% section 17.4 in sutton is good!

% https://xbpeng.github.io/projects/DeepMimic/index.html

% reward shaping: change reward signal as agent progresses
% could also shape the environment itself, make it more and more difficult
% each modification should be made so that the agent frequently receives reward with its current policy
% this is what we do when we train animals! ie give treats to dogs
% why isn't this working...

% reward normalization: 
% reward clipping: done in "playing atari with deep reinforcement learning": they state that since the scale of scores varies from game to game they fix all positive rewards to be one and negative to be -1. 0 rewards are left unchanged. this limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. could also affect agent's performance as it cannot distinguish between rewards of different magnitudes.
% could be important for us if the number of targets and the reward varies a lot

% "explicit episodic (semantic) memory"
% visible region, visited regions
% other semantic information if available
% feature extraction with convolutions
% split the scene into discrete grid
% C channels with different features
% can even be applied to dynamic scenes with a last-seen mask

% could be multiple channels for view as well
% maybe call them "global" and "local"

% resnets?
% https://efficientdl.com/how-to-train-a-resnet-efficiently/

% should use a constant network architecture

The architecture of the neural network is presented in Figure X.
The network is split into three main section: the feature extraction, the shared network, and the policy and value heads.
In order to be able to use the same extractor for all environments we resize the RGB image to \(64 \times 64\) pixels.


% Actor critic

% Feature extraction
% Shared Network
% Policy Head
% Value Head

% we should clearly explain the thought process
% connect back to local search problems
% we need a good feature extraction
% and then something for a belief state
% finally, what is the idea behind policy and value (should be in theory instead)

% we could give some pseudocode here

\section{Baselines}
\label{sec:baseline}

We compare our approach to three different well-studied baselines.
The first baseline is the agent from \cite{mnih_human_2015}, which has previously been used as a baseline in \dots.
The agent receives only image observations, and \dots
The second baseline is a recurrent version of that agent
As \cite{mirowski_navigate_2017}, we use a 

This way, we can clearly see the effect memory, image observations and position observations have on performance for the tasks.

\section{Experiments}
\label{sec:experiments}

% train on the full distribution of environments?
% compare and evaluate all agents on all three environments
% report training reward, episode length, SPL, ...

% generalization
% limit training set size to 100, 1000, 10000
% train until convergence
% test on held out levels (how many?)

The agent was trained using the algorithm described in Section \ref{sec:approach} for 100 million time steps in all three environments using PPO, PPG and A2C.
Hyperparameters are tuned with random search separately for each environment.
For all experiments, the average return per episode is reported together with the theoretically optimal reward (obtained with an optimal path).
The agent was trained and tested on the full distribution of environments.

We compare the two reward signals on the first and second environment\dots

% rq1
The baselines and our approach was\dots

% rq2
Additionally, experiments to eveluate the generalization capability of the agent were conducted.
These were conducted on the procedurally generated terrain environment following the approach suggested in ~\cite{procgen}.
During training, the seed pool size was fixed to various sizes to limit the training set size.
The agent was trained for varying number of timesteps and then tested on the full distribution of environments.
This way, we can get a sense of how much data and simulation is required to use the approach for real-world tasks.

% rq3
We compare the approach to random walk, exhaustive search and a human searcher with prior knowledge of the characteristics of the searched environments.

% eval

Following the recommendations of \cite{henderson_matters_2018} and \cite{agarwal_rlliable_2022}, we report confidence intervals across a handful of seeds.
We run all experiments across 3 different seeds.
We use the aggregate metrics proposed by \cite{agarwal_rlliable_2022}, namely\dots

For each trained agent, we report the average return and episode length across all parallell environments. We train

During testing, we increase the maximum episode length from 1000 time steps to 5000 time steps.
This is to 

\section{Implementation}

The environment is implemented with Gym~\cite{brockman_gym_2016}. The agent is implemented and RL algorithms are implemented with PyTorch~\cite{paszke_pytorch_nodate} for automatic differentiation of the computation graphs.

Proximal policy optimization was implemented following the official implementation by OpenAI.
Some necessary modifications were made to allow for recurrent policies.

All experiments are conducted on an Intel Core i9-10900X CPU and an NVIDIA GeForce RTX 2080 Ti GPU.


% a little short...


% learning actions, recognition and localization simultaneously feels in itself interesting
% how do we interpret such a model? are there papers for visualizing reinforcement learning weights

% detailed
% replicability

% pre-study
% implementation
% evaluation

% discuss state representation
% need more than the image
% - either memory or position or both

% seed + 1000 levels training
% >= s + ... for test
% do the same as procgen

% also same configuration possibly?
% more parallel environments seem to lead to stable training (relatively slow increase in reward though...)

% would be nice to have a varying number of targets
% also a done action
% measure false positives, false negatives etc.
% good replacement for zoom?
% relates to visual search literature
% the current setup is also the most natural for a computer vision system
% foveated vision is not very reasonable


% select one algorithm and clearly motivate why!
% probably PPO, but why?


% time to find targets could be measured in terms of execution time, 
% and number of timesteps. this way, it could be easier to incorporate 
% visual attention if it is deemed reasonable.

% how do we handle appearance of targets:
% can an agent learn to recognize anything out of the ordinary?
% maybe not the focus, but just the search

% an interesting question is how large search windows we can handle
% could be an alternative to zoom
% a large search window presumably requires attention


% memory is needed for (a) remembering previous locations, and (b) integrating features over time


% PPO
% PPG
