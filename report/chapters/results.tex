\chapter{Results}
\label{cha:results}

This chapter presents the results for each of the experiments described in Section~\ref{sec:experiments}.
In addition to the results here, we present learning curves for all environments in Appendix~\ref{app:learning} and some search path examples for our approaches and baselines in Appendix~\ref{app:paths}.

% todo: We can visualize attention as in "A critical investigation of deep reinforcement learning for navigation"

\section{Search Performance and Behavior}

Table~\ref{tab:metrics} shows the average search path length, success rate and SPL metric on a fixed set of 100 levels from each environment.
The average search path length is computed from episodes in which the agents successfully finds all targets.
These metrics are presented for our two approaches trained on the full distribution of environments, as well as for the baselines baselines and human searchers.

Overall, our two approaches and human searchers achieve similar scores.
Our spatial memory approach achieves the most competitive search path lengths and SPL scores in the gaussian and terrain environments.
Our temporal memory agent select search paths that are on average longer than the paths chosen by our spatial memory approach and human searchers.
In the camera environment, the temporal memory agent is better both in average length and SPL score.
Interestingly, the temporal memory agent achieves SPL scores comparable to that of the spatial memory agent despite choosing much longer path lengths.

Human searchers seemed to be successful in utilizing using environment cues to guide search, but frequently forgot which positions had been visited.
In the camera environment, human searchers found it difficult to identify when targets should be indicated,
as the discretization of the search space did not always align targets closely to the center of the screen.
This is more of a side-effect of the implementation than an indication of poor search performance.

The random baseline policy achieves the worst SPL, success rate and average search path length in all three environments.
The greedy baseline policy is closer to the remaining agents.
The exhaustive baseline policy achieves an average search path length that is lower than our temporal memory approach.
Its SPL score, however, is only higher than the those of the other two baselines.
The handcrafted baseline for the gaussian environment performed better than all other agents in this environment. % todo

\begin{table}
    \centering
    \label{tab:metrics}
    \caption[Performance metrics for each environment.]{SPL, success rate and average search path length on successful episodes from three runs on a fixed set of a 100 samples from each environment.}
    Gaussian environment\par\vspace{0.5em}
    \input{tables/metrics-gaussian}
    \par\vspace{1em}Terrain environment\par\vspace{0.5em}
    \input{tables/metrics-terrain}
    \par\vspace{1em}Camera environment\par\vspace{0.5em}
    \input{tables/metrics-camera}
\end{table}

\section{Size of Search Space}
\label{sec:shape}

The results of the search space experiments in the gaussian environment are presented in Figure~\ref{fig:shape}.
Results were collected across four different runs, and the plots show the mean and standard deviation.
For the search space of \(10 \times 10\), both architectures initially improve their policy quickly.
Past a certain time step, they keep improving at a reduced pace.
At the end of training, the spatial memory architecture has reached a policy that seems to find targets quicker than the temporal memory.
Both seem to be able to find the all three targets in every episode.

For the larger search space sizes with \(15 \times 15\) and \(20 \times 20\) the difference between the two architectures is greater.
While the spatial memory seems to consistently find targets in a number of steps that is comparable to the number of positions in the search space,
the search paths of the agent with the temporal memory are substantially longer.
Furthermore, the variance across runs increases with the search space size.

\begin{figure}
    \centering
    \(10 \times 10\)
    \input{figures/shape-10.pgf}
    \(15 \times 15\)
    \input{figures/shape-15.pgf}
    \(20 \times 20\)
    \input{figures/shape-20.pgf}
    \caption[Learning curves for different search space sizes.]{Episode length and success rate curves during training for three different search space sizes. Mean and standard deviation across 4 runs.}
    \label{fig:shape}
\end{figure}

\section{Number of Training Samples}
\label{sec:sample}

Figure~\ref{fig:sample} shows how the average length and success rate in the terrain environment is affected by the number of samples seen during training.
These metrics are presented for the limited training set and unlimited testing set respectively.

It seems like both architectures can overfit to training sets of as many as 1000 samples,
For smaller training set sizes, search path lengths on the test get get gradually worse past a certain time step.
Interestingly the LSTM architecture seems to overfit more severely to small training sets.
Its performance on the test set decreases substantially past a certain time step.
The spatial memory architecture does not overfit as severely.
For both approaches, as many as 10000 training samples are needed to generalize to the full distribution of scenes and achieve the same performance on the training and test sets. 

\begin{figure}
    \centering
    \(500\) samples
    \input{figures/sample-500.pgf}
    \(1000\) samples
    \input{figures/sample-1000.pgf}
    \(5000\) samples
    \input{figures/sample-5000.pgf}
    \(10000\) samples
    \input{figures/sample-10000.pgf}
    \caption[Learning curves for varying training set sizes.]{Episode length and success rate curves for different training set sizes in terrain environment. Mean and standard deviation across 3 seeds.}
    \label{fig:sample}
\end{figure}
