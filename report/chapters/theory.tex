\chapter{Theory}
\label{cha:theory}

% The main purpose of this chapter is to make it obvious for
% the reader that the report authors have made an effort to read
% up on related research and other information of relevance for
% the research questions. It is a question of trust. Can I as a
% reader rely on what the authors are saying? If it is obvious
% that the authors know the topic area well and clearly present
% their lessons learned, it raises the perceived quality of the
% entire report.
% 
% After having read the theory chapter it shall be obvious for          <--
% the reader that the research questions are both well                  <--
% formulated and relevant.                                              <--
% 
% The chapter must contain theory of use for the intended
% study, both in terms of technique and method. If a final thesis
% project is about the development of a new search engine for
% a certain application domain, the theory must bring up related
% work on search algorithms and related techniques, but also
% methods for evaluating search engines, including
% performance measures such as precision, accuracy and
% recall.
% 
% The chapter shall be structured thematically, not per author.
% A good approach to making a review of scientific literature
% is to use \emph{Google Scholar} (which also has the useful function
% \emph{Cite}). By iterating between searching for articles and reading
% abstracts to find new terms to guide further searches, it is
% fairly straight forward to locate good and relevant
% information, such as \cite{test}.
% 
% Having found a relevant article one can use the function for
% viewing other articles that have cited this particular article,
% and also go through the articleâ€™s own reference list. Among
% these articles on can often find other interesting articles and
% thus proceed further.
% 
% It can also be a good idea to consider which sources seem
% most relevant for the problem area at hand. Are there any
% special conference or journal that often occurs one can search
% in more detail in lists of published articles from these venues
% in particular. One can also search for the web sites of
% important authors and investigate what they have published
% in general.
% 
% This chapter is called either \emph{Theory, Related Work}, or
% \emph{Related Research}. Check with your supervisor.

This chapter introduces relevant theory and related work.
Section ~\ref{sec:activevision} gives some background on active vision.
Section ~\ref{sec:visualsearch} describes the problem of visual searching for targets.


% describe how animal vision translates to camera vision?
% animals have a large field of view, but also require foveated vision: processing is expensive.
% computers do not naturally have this constraint and most CV algorithms do not use foveated vision
% (although visual attention models exist)

\section{Artificial Neural Networks}

An artificial neural network (ANN) is a type of universal function approximator.
ANNs are based on 

% https://davidstutz.de/illustrating-convolutional-neural-networks-in-latex-with-tikz/

\subsection{Feed-forward Neural Network}

A feed-forward neural network, also called multi-layer perceptron (MLP) defines a mapping \(y = f(x; \theta)\).
The value of the parameter \(\theta\) is learned  

~\cite{goodfellow_deep_2016}

\subsection{Convolutional Neural Network}

Convolutional Neural Networks (CNNs) are designed specifically for processing data with a known grid-like topology, such as images.
As the name implies, CNNs employ the convolution operator

\[
    s(t) 
\]

~\cite{goodfellow_deep_2016}

% https://viso.ai/deep-learning/mask-r-cnn/#:~:text=Mask%20R%2DCNN%20is%20a,segmentation%20mask%20for%20each%20instance.

\subsection{Recurrent Neural Network}

% https://colah.github.io/posts/2015-08-Understanding-LSTMs/
Recurrent neural networks (RNNs) are networks designed to process sequential data.
By sharing parameters across different parts of a network, it is possible to generalize to 

A commonly used RNN architecture is long short-term memory (LSTM)~\cite{hochreiter_schmidhuber_lstm_1997}.

\section{Reinforcement Learning}

Reinforcement learning (RL)~\cite{sutton_reinforcement_2018} is a subfield of machine learning concerned with learning from interaction how to achieve a goal.
This section introduces the fundamental concepts of RL.

\subsection{Partially Observable Markov Decision Processes}

The problem of learning from interaction to achieve some goal is often framed as a (finite) Markov decision process (MDP).
A learning \textit{agent} interacts continually with its \textit{environment}.
The agent takes the \textit{state} of the environment as input, and select an \textit{action} to take.
This action updates the state of the environment and gives the agent a scalar \textit{reward}.
It is assumed that the next state and reward depend only on the previous state and the action taken.
This is referred to as the \textit{Markov} property.~\cite{kaelbling_pomdp_1998}

In an MDP, the agent can perceive the state of the environment with full certainty.
For many problems, including the one we consider here, this is not the case.
The agent can only perceive a partial representation of the environment's state.
Such a process is referred to as a partially observable Markov decision process (POMDP).
A POMDP is formally defined as a 7-tuple \(\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \Omega, \mathcal{O}, \gamma \rangle\), where

\begin{itemize}
    \item \(\mathcal{S}\) is a finite set of states,
    \item \(\mathcal{A}\) is a finite set of actions,
    \item \(\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \Pi(\mathcal{S})\) is a state-transition function,
    \item \(\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) is a reward function,
    \item \(\Omega\) is a finite set of observations,
    \item \(\mathcal{O}: \mathcal{S} \times \mathcal{A} \rightarrow \Pi(\Omega)\) is an observation function, and
    \item \(\gamma \in [0, 1]\) is a discount factor.
\end{itemize}

Assume that the environment is in state \(s \in \mathcal{S}\), and the agent selects action \(a \in \mathcal{A}\).
Then, \(T(s, a, s^\prime)\) is the probability of ending in state \(s^\prime\) and \(R(s, a)\) is the expected reward gained by the agent.
The agent also receives an observation \(o \in \Omega\) with probability \(\mathcal{O}(s^\prime, a, o)\).
The agent and environment interact over a sequence of discrete time steps \(t = 0, 1, \dots, T\), giving rise to an \textit{episode} of length \(T\).
The goal of the agent is to maximize the expected \textit{discounted return} \(\mathbb{E} \lbrack \Sigma_{t=0}^\infty \gamma^t r_t \rbrack\).~\cite{kaelbling_pomdp_1998}

Figure \ref{fig:pomdp} illustrates the interaction between agent and environment.

\begin{figure}
    \centering
    \input{figures/pomdp}
    \label{fig:pomdp}
    \caption{Partially observable Markov decision process.}
\end{figure}

Since the agent receives partial observations of the environment's state, it has to act under uncertainty.
Planning in a POMDP is undecidable, and solving them is often computationally intractable.
Approximate solutions are more common, where the agent usually maintains an internal \textit{belief state} \(b\) that summarizes its previous experience.

% belief state
% markov property

\subsection{Policies and Value Functions}

Most RL algorithms estimate both a \textit{value function} that tells the agent how good it is to be in a given state.
\cite{sutton_reinforcement_2018}

\subsection{Exploration and Exploitation Trade-off}


\section{Related Work}

\subsection{Deep Reinforcement Learning}

% Mnih et al., Atari, DQN

A long-standing challenge in RL is to learn good policies from high-dimensional sensory inputs like vision.
Mnih et al. (2013)~\cite{mnih_atari_2013} address this by combining deep learning with RL.
A CNN is trained to estimate the 

Mnih et al. (2015)~\cite{mnih_human_2015}\dots % real breakthrough, human level control, origin of NatureCNN
% 84x84x4 input image produced by preprocessing map
% 1st: 32 filters of 8x8 with stride 4 and ReLU
% 2nd: 64 filters of 4x4 with stride 2 and ReLU
% 3rd: 64 filters of 3x3 with stride 1 and ReLU
% 4th: Hidden fully connected kayer of 512 ReLU
% 5th: Output layer with one output for each valid action

% deep learning
% experience replay
% value iteration
% deep Q-learning
% refer briefly to the whole imagenet thing

Hausknecht and Stone (2017)~\cite{hausknecht_stone_2017} investigates the effects of adding recurrency to a DQN in order to tackle POMDPs.
A LSTM is added after the initial 

% PPO, others

\subsection{Active Vision}
\label{sec:activevision}

% Marr?

Much of past and present research in machine perception involves a passive observer.
Images are passively sampled and perceived.
Animal perception, however, is active.
We do not only see things, but look for them.
One might ask why this is the case, if there is any advantage that an active observer has over a passive one.
Aloimonos and Weiss (1988)~\cite{aloimonos_active_1988} introduce the paradigm called \textit{active vision}, and prove that an active observer can solve several basic vision problems in a more efficient way than a passive one.

Bajcsy (1988)~\cite{bajcsy_1988} defines active vision, and perception in general, as a problem of intelligent data acquisition.
An active observer needs to define and measure parameters and errors from its scene and feed then back to control the data acquisition process.
Bajscy states that one of the difficulties of this problem is that they are scene and context dependent.
A thorough understanding of the data acquisition parameters and the goal of the visual processing is needed.
One view lacks information that may be present with multiple views.
Multiple views also add the time dimension into the problem.

In a re-visitation of active perception, Bajcsy, Aloimonos and Tsotsos (2018)~\cite{bajcsy_aloimonos_tsotsos_2018} stress that despite recent successes in robotics, artificial intelligence and computer vision, an intelligent agent must include active perception:

\begin{quote}
    An agent is an active perceiver if it knows why it wishes to sense, and then chooses what to perceive, and determines how, when and where to achieve that perception
\end{quote}~\cite{bajcsy_aloimonos_tsotsos_2018}

% see conclusion in that paper:
% - mirror neurons, same system responsible for generating and interpreting actions at a high level 

\subsection{Visual Search}
\label{sec:visualsearch}

The perceptual task of searching for something in a visual environment is usually referred to as \textit{visual search}.
The searched object or feature is the \textit{target}, and the other objects or features in the environment are the \textit{distractors}.
This task has been studied extensively in psychology and neuroscience.

Wolfe (2021)~\cite{wolfe_guided_2021} describes a model of visual search

% general things
% - foveated vision
% - covert and overt attention
% ...

Eckstein (2011)~\cite{eckstein_visual_2011} reviews efforts from various subfields and identifies a set of mechanisms used to achieve efficient visual search.
Knowledge about the target, distractor, background statistical properties, location probabilities, contextual cues, rewards and target prevalence are all identified as useful.
This is motivated with evidence from psychology as well as neural correlates.

Visual search is not always instant, and can in fact often be slow.
This is in part due to processing: our visual system cannot process the entire visual field and 

% saliency, center-surround organization
% covert attention and eye movement

Wolfe and Horowitz (2017)~\cite{wolfe_horowitz_2017} identify and measure a set of factors that guide attention in visual search.
One of these is bottom-up guidance, in which some visual properties of the scene draw more attention than others.
Another is top-down guidance, which is user driven and directed to objects with known features of desired targets.
Scene guidance is also identified, in which attributes of the scene guide attention to areas likely to contain targets. 

These works ground the task considered in this project in psychology.

% can we build a system that exhibits all of these?

\subsection{Object Detection}

% active object detection

A similar problem can be found in the computer vision literature under \textit{object detection}.
The goal of object detection is to, given an input image, detect instances of semantic objects in it.
This includes assigning a bounding box to the objects, and classifying the object.
The input image is usually passively sampled, and the whole scene is visible at once.

Caicedo and Lazebnik (2015)~\cite{caicedo_active_2015} propose to use deep reinforcement learning for active object localization in images where the object to be localized is fully visible.
An agent is trained to successively improve a bounding box using translating and scaling transformations.
They use a reward signal that is proportional to how well the current box covers the target object.
An action that improves the region is rewarded with +1, and -1 otherwise.
Without this quantization, the difference was small enough to confuse the agent.
Binary rewards communicate more clearly which transformations keep the object inside the box and which take the box away from the target.
When there is no action that improves the bounding box, the agent may select a trigger action (which would be the only action that does not give a negative reward) which resets the box.
This way the agent may select additional bounding boxes.
Each trigger modifies the environment by marking it so that the agent may learn to not select the same region twice.
This is referred to as an inhibition-of-return mechanism, and is widely used in visual attention models~\cite{[16] in caicedo_active_2015}.
This method has a few shortcomings for the problem considered in this project.
The object may not be visible in the initial frame so the agent cannot act in the same way. 

A separate field is active object search, which is perhaps most closely related to the problem we consider in this work.
In active object search, % explain
% difference is tht this is the first deep dive into these particular aspects
% write about the work that uses object co-occurences though
% we should stress that we do not explicitly model the environment guidance, but are rather interested in whether it can be learned efficiently by the system

% Visual Navigation is another thing to call it...

% 

A similar work by Ghesu et al. (2016)~\cite{ghesu_artificial_2016} present an agent for anatomical landmark detection trained with DRL.
Different from \cite{caicedo_active_2015} is that the entire scene is not visible at once.
The agent sees a limited region of interest in an image, with its center representing the current position of the agent.
The actions available to the agent translate the view up, down, left and aright.
A reward is given to the agent that is equal to the supervised relative distance-change to the landmark after each action.
Three datasets of 891 anatomical images are used.
The agent starts at random positions in the image close to the target landmark and is tasked with moving to the target location.
While achieving strong results (90\% success rate), the scenes and targets are all drawn from a distribution with low variance.
Most real-world search tasks exhibit larger variance than anatomical images of the human body.

Zhu et al. (2016)~\cite{zhu_target_driven_2016} create a model for target-driven visual navigation in indooor scenes with DRL.
An observer is given a partial image of its scene as well as an image of the target object, and is tasked with navigating to the object n the scene with a minimal number of steps.
The agent moves forwards, backwards, and turns left and right at constant step lengths.
They use a reward signal with a small time penalty to incentivize task completion in few steps.
They compare their approach to random walk and the shortest path and achieve promising results.
This setup is quite similar to the one considered in this report, but the authors make a few assumptions that we do not.
They a set of 32 scenes, each of which contain a fixed number of object instances.
They focus on learning spatial relationships between objects in these specific scenes, and have scene-specific layers to achieve this.
Thus, while they show that they can adapt a trained network to a new scene, their approach is unable to zero-shot generalize to new scenes.

A similar work by Ye et al. (2018)~\cite{ye_active_2018} integrates an object recognition module with a deep reinforcement learning based visual navigation module.
They experiment with a set of reward functions and find that constant time penalizing rewards can be problematic and lead to slow convergence.
Their experiments make the same assumptions as \cite{zhu_target_driven} - the scenes and targets used during testing have all been seen during training.


% CT scan material: there is way less variance, otherwise similar. Big difference is that we look at more variance in many aspects?

% CV material: different in that the object is assumed to be visible


\subsection{Visual Attention}

% this article is very good!
% note: reinforcement learning part, soft and hard attention
% https://shairozsohail.medium.com/a-survey-of-visual-attention-mechanisms-in-deep-learning-1043eb25f343

\subsection{Navigation}

% Point navigation, Object navigation, semantic navigation

Mnih et al. (2016)~\cite{mnih_asynchronous_2016} use a recurrent policy with only RGB images to navigate in a labyrinth.
3D labyrinths are randomly generated, and an agent is tasked with finding objects in them.
The same architecture as in \cite{mnih_human_2015} is used, but with 256 LSTM cells after the final hidden layer.
% https://aihabitat.org/challenge/2020/#task-1-pointnav

% https://ieeexplore.ieee.org/document/9102361

% \subsection{Object Navigation}

% https://aihabitat.org/challenge/2020/#task-2-objectnav
% https://arxiv.org/pdf/1709.06158.pdf

% \subsection{Semantic Navigation}

% https://arxiv.org/pdf/2007.00643.pdf
% proposes a semantic map which is apparently very similar to my proposed method...
% "explicit episodic (semantic) memory"

Henriques and Vedaldi (2018)~\cite{henriques_vedaldi_2018} use a spatial memory...

Gupta et al. (2019)~\cite{gupta_cognitive_2019} use a latent spatial memory.
The 
They also use a planner that can plan paths given partial information of the environment.
This allows the agent to take appearance of visited locations into account when deciding where to look next.
The RGB observation is fed through an encoder network that\dots
Planning in this fashion s
% has a list of baselines
% among them is an lstm baseline

Object (Goal) Navigation bears resemblance to the task. 
% evaluation:
% https://arxiv.org/pdf/1807.06757.pdf
% https://arxiv.org/pdf/2006.13171.pdf

\subsection{Generalization in Deep Reinforcement Learning}

% reality is dynamic
% agents need to be robust to variation
% capability to transfer and adapt to unseen but similar environments
% most current research works on benchmarks that do not test this (MuJoCo, Arcade learning environment)

% refer to survey
% specifically IID (train_dist = test_dist) and OOD environments (train_dist != test_dist)

% how do they handle training and test set?

Kobbe et al. (2020)~\cite{cobbe_procgen_2020} study generalization in deep RL. They introduce a benchmark of procedurally generated i.i.d. environments, and find that this is essential to 

Zhang et al. (2018)~\cite{zhang_overfitting_2018} study overfitting in deep RL.

\subsection{Local Search}
% Artificial Intelligence - A modern approach
% Page 134
% Should cover belief states?
% ~\cite{russell_artificial_2021}
% https://en.wikipedia.org/wiki/Tabu_search
% tabu search is mentioned in russel
% could be interesting as inspiration for reward function
% and also observation space
% bring in "learning to optimize" here

% an annealed step size could be interesting...
% more general and there are certainly parallells to other areas.

% local search
% https://en.wikipedia.org/wiki/Local_search_(optimization)
% https://en.wikipedia.org/wiki/Guided_Local_Search

% "learning local search"
% "learning to optimize"

% Reinforcement Learning for Local Search

% Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics
% Levine et al. seems like it could be useful

% we could draw inspiration from multiple local search approaches

% it is essentially a combinatorial optimization problem (I think)

%Schuurman & Southey propose three measures of effectiveness for local search (depth, mobility, and coverage):[2]
%
%    depth: the cost of the current (best) solution;
%    mobility: the ability to rapidly move to different areas of the search space (whilst keeping the cost low);
%    coverage: how systematically the search covers the search space, the maximum distance between any unexplored assignment and all visited assignments.


\subsection{Evaluation of Agents}

Anderson et al. (2018)~\cite{anderson_evaluation_2018} discuss problem statements and evaluation measures for embodied navigation agents.
They...

Batra et al. (2020)~\cite{batra_evaluation_2020}

% make point of following these in method
A problem in RL is reproducibility.
There is often non-determinism, both in the methods and environments used.
This has meant that reproducing state-of-the-art deep RL results is difficult.~\cite{henderson_matters_2018}
Henderson et al. (2018) suggest to make future results in deep RL more reproducible.
% what do they find

A similar work by Agarwal et al. (2022)~\cite{agarwal_rlliable_2022} criticises the heavy use of point estimates of aggregate performance.
They advocate for a set of performance metrics that take uncertainty in results into account. 