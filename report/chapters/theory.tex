\chapter{Theory}
\label{cha:theory}

% ~10p

% The main purpose of this chapter is to make it obvious for
% the reader that the report authors have made an effort to read
% up on related research and other information of relevance for
% the research questions. It is a question of trust. Can I as a
% reader rely on what the authors are saying? If it is obvious
% that the authors know the topic area well and clearly present
% their lessons learned, it raises the perceived quality of the
% entire report.
% 
% After having read the theory chapter it shall be obvious for          <--
% the reader that the research questions are both well                  <--
% formulated and relevant.                                              <--
% 
% The chapter must contain theory of use for the intended
% study, both in terms of technique and method. If a final thesis
% project is about the development of a new search engine for
% a certain application domain, the theory must bring up related
% work on search algorithms and related techniques, but also
% methods for evaluating search engines, including
% performance measures such as precision, accuracy and
% recall.
% 
% The chapter shall be structured thematically, not per author.
% A good approach to making a review of scientific literature
% is to use \emph{Google Scholar} (which also has the useful function
% \emph{Cite}). By iterating between searching for articles and reading
% abstracts to find new terms to guide further searches, it is
% fairly straight forward to locate good and relevant
% information, such as \cite{test}.
% 
% Having found a relevant article one can use the function for
% viewing other articles that have cited this particular article,
% and also go through the article’s own reference list. Among
% these articles on can often find other interesting articles and
% thus proceed further.
% 
% It can also be a good idea to consider which sources seem
% most relevant for the problem area at hand. Are there any
% special conference or journal that often occurs one can search
% in more detail in lists of published articles from these venues
% in particular. One can also search for the web sites of
% important authors and investigate what they have published
% in general.
% 
% This chapter is called either \emph{Theory, Related Work}, or
% \emph{Related Research}. Check with your supervisor.

This chapter introduces relevant theory and related work.
Section ~\ref{sec:activevision} gives some background on active vision.
Section ~\ref{sec:visualsearch} describes the problem of visual searching for targets.


% describe how animal vision translates to camera vision?
% animals have a large field of view, but also require foveated vision: processing is expensive.
% computers do not naturally have this constraint and most CV algorithms do not use foveated vision
% (although visual attention models exist)

\section{Artificial Neural Network}

An artificial neural network (ANN) is a type of universal function approximator.
Inspired by biological neurons found in the brains of animals, ANNs are collections of connected nodes.
Each node receives a real-valued signal, processes it and signals connected nodes.

% https://www.asimovinstitute.org/neural-network-zoo/

This section will introduce three classes of ANNs that are relevant to this work:
feed-forward, convolutional, and recurrent neural networks.

\subsection{Feed-forward Neural Network}

A feed-forward neural network, also called multi-layer perceptron (MLP) defines a mapping \(y = f(x; \theta)\).
It consists of multiple layers of computational units
The value of the parameter \(\theta\) is learned  

\cite{goodfellow_deep_2016}

\subsection{Convolutional Neural Network}

Convolutional Neural Networks (CNNs) are designed specifically for processing data with a known grid-like topology, such as images.
As the name implies, CNNs employ the convolution operator

\[
    s(t) 
\]

\cite{goodfellow_deep_2016}

% https://davidstutz.de/illustrating-convolutional-neural-networks-in-latex-with-tikz/
% https://viso.ai/deep-learning/mask-r-cnn/#:~:text=Mask%20R%2DCNN%20is%20a,segmentation%20mask%20for%20each%20instance.

\subsection{Recurrent Neural Network}

% https://colah.github.io/posts/2015-08-Understanding-LSTMs/
Recurrent neural networks (RNNs) are networks designed to process sequential data.
RNNs extend ANNs by also including \textit{hidden state} in the network.
Information from previous inputs is stored in the hidden state and used to influence the current input and output.
This means that a single network can be used for sequences of variable length.

Formally, RNNs operate on a sequence \(x^{(1), \dots, x^\{(\tau)}\).
For each value \(x_t\) in the sequence, the hidden state is computed as \(h^{(t)} = f(h^{(t-1)}, x^{(t)}; \theta)\).
The hidden state is used as a summary of all previous items in the sequence 

% figure 10.2 in deep learning book


In practice, conventional RNNs struggle with learning long-term dependencies.
During back-propagation, gradients can tend to zero for long sequences, something known as the vanishing gradient problem~\cite{goodfellow_deep_2016}.
An architecture that addresses this issue is long short-term memory (LSTM)~\cite{hochreiter_schmidhuber_lstm_1997}.
LSTMs have \textit{cells} in the hidden layers of the network which have an input gate, an output gate and a forget gate.
These gates control how information flows in the network.
By allowing 


This makes LSTMs particularly useful for learning over long sequences.

\section{Reinforcement Learning}

% {arulkumaran_survey_2017} also gives a good introduction to algorithms

% https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf
% https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html
% https://sites.ualberta.ca/~szepesva/rlbook.html

Reinforcement learning (RL)~\cite{sutton_reinforcement_2018} is a subfield of machine learning concerned with learning from interaction how to achieve a goal.
This section introduces the fundamental concepts of RL.

\subsection{Partially Observable Markov Decision Processes}

The problem of learning from interaction to achieve some goal is often framed as a Markov decision process (MDP).
A learning \textit{agent} interacts continually with its \textit{environment}.
The agent takes the \textit{state} of the environment as input, and select an \textit{action} to take.
This action updates the state of the environment and gives the agent a scalar \textit{reward}.
It is assumed that the next state and reward depend only on the previous state and the action taken.
This is referred to as the \textit{Markov} property.~\cite{kaelbling_pomdp_1998}

In an MDP, the agent can perceive the state of the environment with full certainty.
For many problems, including the one we consider here, this is not the case.
The agent can only perceive a partial representation of the environment's state.
Such a process is referred to as a partially observable Markov decision process (POMDP).
A POMDP is formally defined as a 7-tuple \(\langle \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \Omega, \mathcal{O}, \gamma \rangle\), where

\begin{itemize}
    \item \(\mathcal{S}\) is a finite set of states,
    \item \(\mathcal{A}\) is a finite set of actions,
    \item \(\mathcal{T}: \mathcal{S} \times \mathcal{A} \rightarrow \Pi(\mathcal{S})\) is a state-transition function,
    \item \(\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) is a reward function,
    \item \(\Omega\) is a finite set of observations,
    \item \(\mathcal{O}: \mathcal{S} \times \mathcal{A} \rightarrow \Pi(\Omega)\) is an observation function, and
    \item \(\gamma \in [0, 1]\) is a discount factor.
\end{itemize}

Assume that the environment is in state \(s_t \in \mathcal{S}\), and the agent selects action \(a_t \in \mathcal{A}\).
Then, \(T(s_t, a_t, s_{t+1})\) is the probability of ending in state \(s_{t+1}\) and \(r_t = R(s_t, a_t)\) is the expected reward gained by the agent.
The agent also receives an observation \(o_t \in \Omega\) with probability \(\mathcal{O}(s_{t+1}, a_t, o_t)\).~\cite{kaelbling_pomdp_1998}
The agent and environment interact over a sequence of discrete time steps \(t = 0, 1, \dots, T\), giving rise to an \textit{episode} of length \(T\).
At each time step \(t\), the goal of the agent is to select the action that maximizes the expected \textit{discounted return}:

\[ 
    \mathbb{E} \left[ \sum_{k=0}^T \gamma^{k-t-1} r_k \right]
\]

Figure \ref{fig:pomdp} illustrates the interaction between agent and environment.

\begin{figure}
    \centering
    \input{figures/pomdp}
    \label{fig:pomdp}
    \caption[Partially observable Markov decision process]{Partially observable Markov decision process.}
\end{figure}

Since the agent receives partial observations of the environment's state, it has to act under uncertainty.
Planning in a POMDP is undecidable, and solving them is often computationally intractable.
Approximate solutions are more common, where the agent usually maintains an internal \textit{belief state}~\cite{kaelbling_pomdp_1998} which is acts on.
The belief state summarizes the agent's previous experience and is therefore dependent on the previous belief state, observation and action.
It does not need to summarize the whole history, but generally only the information that helps the agent maximize the expected reward.
From here on we will use the belief state and the environment state \(s\) interchangably. 

\subsection{Policies and Value Functions}
\label{sec:policies-values}

The behaviour of the agent is described by its \textit{policy}.
A policy \(\pi\) is a mapping from perceived environment states to actions.
Policies are often stochastic and specify probabilities for each action, with \(\pi(a|s)\) denoting the probability of taking action \(a\) in state \(s\).
Most RL solutions methods also approximate a \textit{value function}.
A value function \(v_\pi\) estimates how good it is to be in a state.
The value function \(v_\pi(s)\) is the expected (discounted) return when starting at state \(s\) and following policy \(\pi\) until the end of the episode.
With a good value function estimate, the policy can select actions so as to maximize the value function.
This is referred to as an \textit{action-value} method.~\cite{sutton_reinforcement_2018}
% quality functions, advantage functions

For problems with large state and action spaces, it is common to represent value functions with \textit{function approximation}.
In such cases, it is common to encounter states that have never been encountered before.
This makes it important that the estimated value function can generalize from seen to unseen states.
With examples from the true value function, an approximation can be made with supervised learning methods.
We write \(\hat{v}(s,w) \approx v_\pi(s)\) for the approximate value of state \(s\) with the some weight vector \(w \in \mathbb{R}^d\).

An alternative to action-value methods is to approximate the policy itself.
\textit{Policy gradient methods}~\cite{sutton_policygrad_1999} learn a parametrized policy that select actions without a value function.
We denote a parametrized policy as \(\pi(a|s,\theta)\) with \(\theta \in \mathbb{R}^{d^\prime}\) as the parameters to the policy.
The policy parameters are usually learned based on the gradient of some performance measure \(J(\theta)\).
As long as \(\pi(a|s,\theta)\) is differentiable with respect to its parameters, the parameters can be updated with \textit{gradient ascent} in \(J\):

\[
    \theta_{t+1} = \theta_t + \alpha \hat{\nabla J(\theta_t)}) 
\]

Advantages of policy parametrization over action-value methods include stronger convergence guarantees~\cite{sutton_policygrad_1999} and more flexibility in parametrization~\cite{sutton_reinforcement_2018}. In practice, value functions are often still used to learn the policy parameter, but they are not needed for action selection.
Such methods are called \textit{actor-critic} methods, with actor referring to the learned policy and critic referring to the learned value function.
In these cases, there might also be some overlap between the weights \(w\) of the value function estimate and \(\theta\) of the policy estimate. 

% We might imagine an artificial neural network (ANN) in which the last layer is split into multiple parts, or heads, each working on a di↵erent task. One head might produce the approximate value function for the main task (with reward as its cumulant) whereas the others would produce solutions to various auxiliary tasks. All heads could propagate errors by stochastic gradient descent into the same body—the shared preceding part of the network—which would then try to form representations, in its next-to-last layer, to support all the heads. Researchers have experimented with auxiliary tasks such as predicting change in pixels, predicting the next time step’s reward, and predicting the distribution of the return. In many cases this approach has been shown to greatly accelerate learning on the main task (Jaderberg et al., 2017). Multiple predictions have similarly been repeatedly proposed as a way of directing the construction of state estimates (see Section 17.3). (Sutton)

\subsection{Challenges in Reinforcement Learning}

% we can illustrate the challenges in terms of our problem



\begin{itemize}
    \item Exploration and Exploitation
    \item Sparse Rewards
    \item Credit Assignment~\cite{minsky_cap_1961}
\end{itemize}

% https://ai.stackexchange.com/questions/12908/what-is-the-credit-assignment-problem
% https://medium.com/@m.k.daaboul/dealing-with-sparse-reward-environments-38c0489c844d
% reward shaping

\section{Related Work}

\subsection{Deep Reinforcement Learning}

% Mnih et al., Atari, DQN

% problem with classical

As mentioned in Section~\ref{sec:policies-values}, policy and value functions are often approximated.
Neural networks have good properties for function approximation and have been used for RL with success.
One early example is TD-Gammon~\cite{tesauro1995tdgammon}, a neural network trained with RL that reached expert Backgammon performance in the 90s.
More recently, the use of deep neural networks has shown tremendous success in RL, giving birth to the field of deep RL.

\cite{arulkumaran_survey_2017} % survey

\cite{tesauro1995tdgammon} % TD gammon

% deep learning

% combination

Breakthroughs in computer vision like 
Mnih et al. (2013)~\cite{mnih_atari_2013} address this by combining deep learning with RL.
A CNN is trained to estimate the 

Mnih et al. (2015)~\cite{mnih_human_2015}\dots
% real breakthrough, human level control, origin of NatureCNN
% or is it from the 2013 paper?
% 84x84x4 input image produced by preprocessing map
% 1st: 32 filters of 8x8 with stride 4 and ReLU
% 2nd: 64 filters of 4x4 with stride 2 and ReLU
% 3rd: 64 filters of 3x3 with stride 1 and ReLU
% 4th: Hidden fully connected kayer of 512 ReLU
% 5th: Output layer with one output for each valid action

% deep learning
% experience replay
% value iteration
% deep Q-learning
% refer briefly to the whole imagenet thing

Hausknecht and Stone (2017)~\cite{hausknecht_stone_2017} investigates the effects of adding recurrency to a DQN in order to tackle POMDPs.
A LSTM is added after the initial 

\subsection{Proximal Policy Optimization}

Proximal policy optimization algorithms\dots~\cite{schulman_ppo_2017}.

\subsection{Active Vision}
\label{sec:activevision}

% Marr?

Much of past and present research in machine perception involves a passive observer.
Images are passively sampled and perceived.
Animal perception, however, is active.
We do not only see things, but look for them.
One might ask why this is the case, if there is any advantage that an active observer has over a passive one.
Aloimonos and Weiss (1988)~\cite{aloimonos_active_1988} introduce the paradigm called \textit{active vision}, and prove that an active observer can solve several basic vision problems in a more efficient way than a passive one.

Bajcsy (1988)~\cite{bajcsy_1988} defines active vision, and perception in general, as a problem of intelligent data acquisition.
An active observer needs to define and measure parameters and errors from its scene and feed then back to control the data acquisition process.
Bajscy states that one of the difficulties of this problem is that they are scene and context dependent.
A thorough understanding of the data acquisition parameters and the goal of the visual processing is needed.
One view lacks information that may be present with multiple views.
Multiple views also add the time dimension into the problem.

In a re-visitation of active perception, Bajcsy, Aloimonos and Tsotsos (2018)~\cite{bajcsy_aloimonos_tsotsos_2018} stress that despite recent successes in robotics, artificial intelligence and computer vision, an intelligent agent must include active perception:

\begin{quote}
    An agent is an active perceiver if it knows why it wishes to sense, and then chooses what to perceive, and determines how, when and where to achieve that perception
\end{quote}~\cite{bajcsy_aloimonos_tsotsos_2018}

% see conclusion in that paper:
% - mirror neurons, same system responsible for generating and interpreting actions at a high level 

\subsection{Visual Search}
\label{sec:visualsearch}

% todo: should maybe remove the psychology and keep that in introduction only
% "kuriosa"

The perceptual task of searching for something in a visual environment is usually referred to as \textit{visual search}.
The searched object or feature is the \textit{target}, and the other objects or features in the environment are the \textit{distractors}.
This task has been studied extensively in psychology and neuroscience.
% the reason for introducing this is to be able to use the same terminology later

Wolfe (2021)~\cite{wolfe_guided_2021} describes a model of visual search

% general things
% - foveated vision
% - covert and overt attention
% ...

Eckstein (2011)~\cite{eckstein_visual_2011} reviews efforts from various subfields and identifies a set of mechanisms used to achieve efficient visual search.
Knowledge about the target, distractor, background statistical properties, location probabilities, contextual cues, rewards and target prevalence are all identified as useful.
This is motivated with evidence from psychology as well as neural correlates.

Visual search is not always instant, and can in fact often be slow.
This is in part due to processing: our visual system cannot process the entire visual field and 

% saliency, center-surround organization
% covert attention and eye movement

Wolfe and Horowitz (2017)~\cite{wolfe_horowitz_2017} identify and measure a set of factors that guide attention in visual search.
One of these is bottom-up guidance, in which some visual properties of the scene draw more attention than others.
Another is top-down guidance, which is user driven and directed to objects with known features of desired targets.
Scene guidance is also identified, in which attributes of the scene guide attention to areas likely to contain targets. 

These works ground the task considered in this project in psychology.

% can we build a system that exhibits all of these?

\subsection{Object Detection}

% active object detection

A similar problem can be found in the computer vision literature under \textit{object detection}.
The goal of object detection is to, given an input image, detect instances of semantic objects in it.
This includes assigning a bounding box to the objects, and classifying the object.
The input image is usually passively sampled, and the whole scene is visible at once.

Caicedo and Lazebnik (2015)~\cite{caicedo_active_2015} propose to use deep reinforcement learning for active object localization in images where the object to be localized is fully visible.
An agent is trained to successively improve a bounding box using translating and scaling transformations.
They use a reward signal that is proportional to how well the current box covers the target object.
An action that improves the region is rewarded with +1, and -1 otherwise.
Without this quantization, the difference was small enough to confuse the agent.
Binary rewards communicate more clearly which transformations keep the object inside the box and which take the box away from the target.
When there is no action that improves the bounding box, the agent may select a trigger action (which would be the only action that does not give a negative reward) which resets the box.
This way the agent may select additional bounding boxes.
Each trigger modifies the environment by marking it so that the agent may learn to not select the same region twice.
This is referred to as an inhibition-of-return mechanism, and is widely used in visual attention models~\cite{[16] in caicedo_active_2015}.
This method has a few shortcomings for the problem considered in this project.
The object may not be visible in the initial frame so the agent cannot act in the same way. 

A separate field is active object search, which is perhaps most closely related to the problem we consider in this work.
In active object search, % explain
% difference is tht this is the first deep dive into these particular aspects
% write about the work that uses object co-occurences though
% we should stress that we do not explicitly model the environment guidance, but are rather interested in whether it can be learned efficiently by the system

% Visual Navigation is another thing to call it...

% 

A similar work by Ghesu et al. (2016)~\cite{ghesu_artificial_2016} present an agent for anatomical landmark detection trained with DRL.
Different from \cite{caicedo_active_2015} is that the entire scene is not visible at once.
The agent sees a limited region of interest in an image, with its center representing the current position of the agent.
The actions available to the agent translate the view up, down, left and aright.
A reward is given to the agent that is equal to the supervised relative distance-change to the landmark after each action.
Three datasets of 891 anatomical images are used.
The agent starts at random positions in the image close to the target landmark and is tasked with moving to the target location.
While achieving strong results (90\% success rate), the scenes and targets are all drawn from a distribution with low variance.
Most real-world search tasks exhibit larger variance than anatomical images of the human body.

Zhu et al. (2016)~\cite{zhu_target_driven_2016} create a model for target-driven visual navigation in indooor scenes with DRL.
An observer is given a partial image of its scene as well as an image of the target object, and is tasked with navigating to the object n the scene with a minimal number of steps.
The agent moves forwards, backwards, and turns left and right at constant step lengths.
They use a reward signal with a small time penalty to incentivize task completion in few steps.
They compare their approach to random walk and the shortest path and achieve promising results.
This setup is quite similar to the one considered in this report, but the authors make a few assumptions that we do not.
They a set of 32 scenes, each of which contain a fixed number of object instances.
They focus on learning spatial relationships between objects in these specific scenes, and have scene-specific layers to achieve this.
Thus, while they show that they can adapt a trained network to a new scene, their approach is unable to zero-shot generalize to new scenes.

A similar work by Ye et al. (2018)~\cite{ye_active_2018} integrates an object recognition module with a deep reinforcement learning based visual navigation module.
They experiment with a set of reward functions and find that constant time penalizing rewards can be problematic and lead to slow convergence.
Their experiments make the same assumptions as \cite{zhu_target_driven} - the scenes and targets used during testing have all been seen during training.

% CT scan material: there is way less variance, otherwise similar. Big difference is that we look at more variance in many aspects?

% CV material: different in that the object is assumed to be visible

\subsection{Visual Attention}

% this article is very good!
% note: reinforcement learning part, soft and hard attention
% https://shairozsohail.medium.com/a-survey-of-visual-attention-mechanisms-in-deep-learning-1043eb25f343

% closely connected with object detection

\cite{minut_mahadevan_2001}

\subsection{Coverage Path Planning}

\cite{galceran_carreras_2013} % survey of CPP

\cite{krishna_tetromino_2020} % an approach with RL

\subsection{Visual Navigation}

The task we consider bears resemblance to many navigation tasks in robotics.
Point navigation is the task of \dots
Object navigation is the task of \dots
Semantic navigation is the task of \dots

Mnih et al. (2016)~\cite{mnih_asynchronous_2016} use a recurrent policy with only RGB images to navigate in a labyrinth.
3D labyrinths are randomly generated, and an agent is tasked with finding objects in them.
The same architecture as in \cite{mnih_human_2015} is used, but with 256 LSTM cells after the final hidden layer.

% https://aihabitat.org/challenge/2020/#task-1-pointnav
% https://aihabitat.org/challenge/2020/#task-2-objectnav
% https://arxiv.org/pdf/1709.06158.pdf
% https://ieeexplore.ieee.org/document/9102361


Mirowski et al. (2017)~\cite{mirowski_navigate_2017}...

Henriques and Vedaldi (2018)~\cite{henriques_vedaldi_2018} use a spatial memory...

Gupta et al. (2019)~\cite{gupta_cognitive_2019} use a latent spatial memory.
The 
They also use a planner that can plan paths given partial information of the environment.
This allows the agent to take appearance of visited locations into account when deciding where to look next.
The RGB observation is fed through an encoder network that\dots
Planning in this fashion s
% has a list of baselines
% among them is an lstm baseline

Dhiman et al. (2019)~\cite{dhiman_critical_2019} critically investigate deep RL for navigation.
They ask whether DRL algorithms are inherently able to gather and exploit environmental information for during navigation.
Experimentally, they find that an agent is able to exploit environment information when trained and tested on the same map.
However, when trained and tested on differen maps, it cannot do so succesfully.
They further find that, with a single decision point whose correct.
% could be due to their inductive biases
% the agent does not have access to its location and can therefore not determine its location
% their setup:
% - random maze
% - when agent finds goal, it respawns and the goal is at the same location (+10)
% - smaller rewards scattered in area to incentivize exploration (+1)
% - wall penalty (-2)
% - respawn either static or random
% - randomly textured walls

Chaplot et al. (2020)~\cite{chaplot_semantic_2020} build on the idea of an explicit memory by including environment semantics\dots

\subsection{Inductive Biases, Overfitting and Generalization in Deep Reinforcement Learning}

% first introduce the reason why generalization is important
% reality is dynamic
% agents need to be robust to variation
% capability to transfer and adapt to unseen but similar environments
% most current research works on benchmarks that do not test this (MuJoCo, Arcade learning environment)

Kirk et al. (2021)~\cite{kirk_survey_2022} survey generalization in deep RL.

% refer to survey
% specifically IID (train_dist = test_dist) and OOD environments (train_dist != test_dist)

While deep neural networks have proved to be effective function approximators for RL, they are also prone to \textit{overfitting}.
High-capacity models trained over a long time may memorize the distribution seen during training rather than general patterns.
While studied in supervised learning, overfitting is generally been neglected in deep RL.
Training and evaluation stages are typically not separated.
Instead, the final return on the training environments is used as a measure of agent performance.

Zhang et al. (2018)~\cite{zhang_overfitting_2018} study overfitting and generalization in deep RL.
With experiments, they show that RL agents are capable of memorizing training data, even when completely random.
When the number of training samples exceeds the capacity of the agent, they overfit to them.
When exposed to new but statistically similar environments during testing, test performance could vary significantly despite consistent training performance.
The authors argue that good generalization requires that the \textit{inductive bias} of the algorithms is compatible with the bias of the problems.
The inductive bias refers to a priori algorithmic preferences, like neural network architecture.
When comparing MLPs with CNNs, they find that MLPs tend to be better at fitting the training data are worse at generalizing.
When rewards are spatially invariant, CNNs generalize much better than MLPs.
The authors advocate for carefully designed testing protocols for detecting overfitting.
The effectiveness of stochastic-based evaluation depends on the properties of the task.
Agents could still learn to overfit to random training data. 
For this reason, they recommend isolation of statistically tied training and test sets.

In a similar spirit, Cobbe et al. (2019)~\cite{cobbe_generalization_2019} construct distinct training and test sets to measure generalization in RL.
They find that agents can overfit to surprisingly large training sets, and that deep convolutional architectures can improve generalization.
Methods from supervised learning, like L2 regularization, dropout, data augmentation and batch normalization are also shown to aid with generalization.

Many current deep RL agents do not optimize the true objective that they are evaluated against,
but rather a handcrafted objective that incorporates biases to simplify learning.
Stronger biases can lead to faster learning, while weaker biases potentially lead to more general agents.
Hessel et al. (2019)~\cite{hessel_inductive_2019} investigate the trade-off between generality and performance from the perspective of inductive biases.
Through experimentation with common reward sculpting techniques, they find that learned solutions are competitive with domain heuristics like handcrafted objectives.
Learned solutions also seem to be better at generalizing to unseen domains.
For this reason, they argue for removing biases determined with domain knowledge in future research.


% what does this mean for "oracle" reward signals, such as supervised improvement in distance? 
% it is essentially reward shaping.
% they introduce human bias into the possible policies.
% the agent may fail to discover optimal policies.
% the problem with our reward is that I am not sure it will lead to minimizing time.
% should run experiments with time penalty and trigger reward again...

% in this spirit
Cobbe et al. (2020)~\cite{cobbe_procgen_2020} introduce a benchmark for sample efficiency and generalization in RL.
They make use of procedural generalization to decide many parameters of the initial state of the environment.
This forces agents to learn policies that are robust variation and avoid overfitting.
To evaluate sample efficiency of agents in the benchmark, they train and test on the full distribution of states.
To evaluate generalization, they fix the number of training samples and then test on held out levels.
When an episode ends, a new sample is drawn from the training set.
Agents may train for arbitrarily many time steps.
The number of training samples required to generalize is dependent on the particulars and difficulty of the environment.
The authors choose the training set size to be near the region when generalization begins to take effect.
Empirically they find that larger model architectures improve both sample efficiency and generalization.
Agents strongly overfit to small training sets and need many samples to generalize.
Interestingly, training performance improves as the training set grows past a certain threshold.
The authors attribute this to the implicit curriculum of the distribution of levels.

\subsection{Evaluation of Deep Reinforcement Learning Agents}

Anderson et al. (2018)~\cite{anderson_evaluation_2018} discuss problem statements and evaluation measures for embodied navigation agents.
They...

Batra et al. (2020)~\cite{batra_evaluation_2020} do something similar\dots

% make point of following these in method
A problem in RL is reproducibility.
There is often non-determinism, both in the methods and environments used.
This has meant that reproducing state-of-the-art deep RL results is difficult.~\cite{henderson_matters_2018}
Henderson et al. (2018) suggest to make future results in deep RL more reproducible.
% what do they find

A similar work by Agarwal et al. (2022)~\cite{agarwal_rlliable_2022} criticises the heavy use of point estimates of aggregate performance.
They advocate for a set of performance metrics that take uncertainty in results into account. 