\chapter{Theory}
\label{cha:theory}

This chapter introduces relevant theory and related work 

\section{Active Vision and Visual Search}

An \textit{active vision} system is a system that can manipulate the viewpoint of the camera in order to investigate the environment and get better information from it.



\section{Reinforcement Learning}

Reinforcement learning (RL) is a subfield of machine learning concerned with learning from interaction how to achieve a goal. An \textit{agent} and its \textit{environment} interact continually over discrete time steps. At each time step the agent selects some \textit{action} that updates the state of the environment, and gives it a \textit{reward}. The agent selects actions using a stochastic \textit{policy} with the goal of maximizing the \textit{return} which is usually defined as the discounted sum of future rewards.

\subsection{Markov Decision Process}

The RL setup is usually formalized as a (finite) Markov decision process (MDP).

The problem of learning from interaction to achieve a goal is usually framed as a (finite) Markov Decision Process (MDP). For regular MDPs it is assumed that the learning agent has access to some representation of the underlying \textit{state} of the environment which it uses to select \textit{actions}. For many problems this is not true. A partially observable Markov decision process (POMDP) is a generalization of an MDP in which it is assumed that the environment has some well defined underlying latent state, but the agent only perceives a partial \textit{observation} of it from the environment. 

A POMDP is formally defined as a 7-tuple \(\langle \mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{R}, \mathcal{T}, \Omega, \gamma \rangle\), where

\begin{itemize}
    \item \(\mathcal{S}\) is a finite set of states,
    \item \(\mathcal{A}\) is a finite set of actions,
    \item \(\mathcal{T}\) is a set of conditional state transition probabilities,
    \item \(\mathcal{R} : S \times A \rightarrow \mathbb{R}\) is a reward function,
    \item \(\Omega\) is a finite set of observations,
    \item \(\mathcal{O}\) is a set of conditional observation probabilities, and
    \item \(\gamma \in [0, 1]\) is a discount factor.
\end{itemize}

\begin{figure}
    \centering
    \input{figures/pomdp}
    \label{fig:pomdp}
    \caption{Partially observable Markov decision process.}
\end{figure}

The agent interacts with the environment at discrete time steps \(t = 0, 1, 2, \dots T\). At each time step \(t\), the agent receives an observation of the environment's state \(O_t \in \Omega\) and selects some action \(A_t \in \mathcal{A}\). In the next time step the agent receives a reward



action \(a \in \mathcal{A}\) which causes the environment to transition to state \(s^\prime\) with probability \(\mathcal{T}(s^\prime | s, a)\).
It receives an observation \(o \in \Omega\) with probability \(\mathcal{O}(o | s^\prime, a)\), as well as a reward \(r\) given by \(\mathcal{R}(s, a)\).

This interaction is repeated until the end of the episode at time step \(T\). The goal of the agent is to maximize the \textit{discounted return}, defined as the discounted sum of future rewards \(G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1} R_{k}\) where \(\gamma\) reflects the uncertainty of the environment.


%An MDP satisfies the Markov property: the process is memoryless. A POMDP is not memoryless, as observations only convey part of the underlying state. However, the \textit{history} \(H_t = A_0, O_1, R_1, \dots, A_{t-1}, O_t, R_t\) does.



\subsection{Policies and Value Functions}

Most RL algoerithms estimate both a \textit{value function} that tells the agent how good it is to be in a given state, and a 

\subsection{Policy Optimization}

This work will focus on policy optimization algorithms.

\subsection{Taxonomy of Algorithms}

\begin{itemize}
    \item Model-free vs. model-based
    \item 
\end{itemize}

\subsection{Generalization}

% reality is dynamic
% agents need to be robust to variation
% capability to transfer and adapt to unseen but similar environments
% most current research works on benchmarks that do not test this (MuJoCo, Arcade learning environment)

% refer to survey
% specifically IID (train_dist = test_dist) and OOD environments (train_dist != test_dist)