@article{agarwal_deep_2022,
  title        = {Deep Reinforcement Learning at the Edge of the Statistical Precipice},
  url          = {http://arxiv.org/abs/2108.13264},
  abstractnote = {Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a ﬁnite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few-run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the ﬁeld. We illustrate this point using a case study on the Atari 100k benchmark, where we ﬁnd substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the ﬁeld’s conﬁdence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance proﬁles to account for the variability in results, as well as present more robust and efﬁcient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our ﬁndings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable2, to prevent unreliable results from stagnating the ﬁeld.},
  note         = {arXiv: 2108.13264},
  journal      = {arXiv:2108.13264 [cs, stat]},
  author       = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G.},
  year         = {2022},
  month        = {Jan}
}

@article{aloimonos_active_1988,
  title        = {Active vision},
  volume       = {1},
  issn         = {0920-5691, 1573-1405},
  url          = {http://link.springer.com/10.1007/BF00133571},
  doi          = {10/cn4mdc},
  pages        = {333--356},
  number       = {4},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vision},
  author       = {Aloimonos, John and Weiss, Isaac and Bandyopadhyay, Amit},
  urldate      = {2022-02-07},
  date         = {1988-01},
  langid       = {english},
  note         = {705 citations (Crossref) [2022-02-07]},
  file         = {Active vision:/home/oslund/Zotero/storage/SFZQUV5N/aloimonos1988.pdf.pdf:application/pdf}
}

@article{anderson_evaluation_2018,
  title        = {On Evaluation of Embodied Navigation Agents},
  url          = {http://arxiv.org/abs/1807.06757},
  abstractnote = {Skillful mobile operation in three-dimensional environments is a primary topic of study in Artiﬁcial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task deﬁnitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking.},
  note         = {arXiv: 1807.06757},
  journal      = {arXiv:1807.06757 [cs]},
  author       = {Anderson, Peter and Chang, Angel and Chaplot, Devendra Singh and Dosovitskiy, Alexey and Gupta, Saurabh and Koltun, Vladlen and Kosecka, Jana and Malik, Jitendra and Mottaghi, Roozbeh and Savva, Manolis and Zamir, Amir R.},
  year         = {2018},
  month        = {Jul}
}


@article{andreopoulos_tsotsos_theory_2009,
  title        = {A Theory of Active Object Localization},
  abstractnote = {We present some theoretical results related to the problem of actively searching for a target in a 3D environment, under the constraint of a maximum search time. We deﬁne the object localization problem as the maximization over the search region of the Lebesgue integral of the scene structure probabilities. We study variants of the problem as they relate to actively selecting a ﬁnite set of optimal viewpoints of the scene for detecting and localizing an object. We do a complexity-level analysis and show that the problem variants are NP-Complete or NP-Hard. We study the tradeoffs of localizing vs. detecting a target object, using singleview and multiple-view recognition, under imperfect deadreckoning and an imperfect recognition algorithm. These results motivate a set of properties that efﬁcient and reliable active object localization algorithms should satisfy.},
  author       = {Andreopoulos, Alexander and Tsotsos, John K},
  pages        = {8},
  language     = {en}
}

@article{andrychowicz_what_2020,
  title        = {What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study},
  url          = {http://arxiv.org/abs/2006.05990},
  abstractnote = {In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [27]. As a step towards ﬁlling that gap, we implement >50 such “choices” in a uniﬁed on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250’000 agents in ﬁve continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.},
  note         = {arXiv: 2006.05990},
  journal      = {arXiv:2006.05990 [cs, stat]},
  author       = {Andrychowicz, Marcin and Raichuk, Anton and Stańczyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, Léonard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and Gelly, Sylvain and Bachem, Olivier},
  year         = {2020},
  month        = {Jun}
}


@article{arkin_lawnmowing_2000,
  title        = {Approximation algorithms for lawn mowing and milling},
  volume       = {17},
  issn         = {0925-7721},
  doi          = {10.1016/S0925-7721(00)00015-8},
  abstractnote = {We study the problem of finding shortest tours/paths for “lawn mowing” and “milling” problems: Given a region in the plane, and given the shape of a “cutter” (typically, a circle or a square), find a shortest tour/path for the cutter such that every point within the region is covered by the cutter at some position along the tour/path. In the milling version of the problem, the cutter is constrained to stay within the region. The milling problem arises naturally in the area of automatic tool path generation for NC pocket machining. The lawn mowing problem arises in optical inspection, spray painting, and optimal search planning. Both problems are NP-hard in general. We give efficient constant-factor approximation algorithms for both problems. In particular, we give a (3+ε)-approximation algorithm for the lawn mowing problem and a 2.5-approximation algorithm for the milling problem. Furthermore, we give a simple 65-approximation algorithm for the TSP problem in simple grid graphs, which leads to an 115-approximation algorithm for milling simple rectilinear polygons.},
  number       = {1},
  journal      = {Computational Geometry},
  author       = {Arkin, Esther M. and Fekete, Sándor P. and Mitchell, Joseph S. B.},
  year         = {2000},
  month        = {Oct},
  pages        = {25–50}
}


@article{arulkumaran_survey_2017,
  title        = {A Brief Survey of Deep Reinforcement Learning},
  volume       = {34},
  issn         = {1053-5888},
  doi          = {10.1109/MSP.2017.2743240},
  abstractnote = {Deep reinforcement learning is poised to revolutionise the ﬁeld of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general ﬁeld of reinforcement learning, then progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the ﬁeld.},
  note         = {arXiv: 1708.05866},
  number       = {6},
  journal      = {IEEE Signal Processing Magazine},
  author       = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year         = {2017},
  month        = {Nov},
  pages        = {26–38}
}

@article{aydemir_active_2013,
  title        = {Active Visual Object Search in Unknown Environments Using Uncertain Semantics},
  volume       = {29},
  issn         = {1941-0468},
  doi          = {10.1109/TRO.2013.2256686},
  abstractnote = {In this paper, we study the problem of active visual search (AVS) in large, unknown, or partially known environments. We argue that by making use of uncertain semantics of the environment, a robot tasked with finding an object can devise efficient search strategies that can locate everyday objects at the scale of an entire building floor, which is previously unknown to the robot. To realize this, we present a probabilistic model of the search environment, which allows for prioritizing the search effort to those parts of the environment that are most promising for a specific object type. Further, we describe a method for reasoning about the unexplored part of the environment for goal-directed exploration with the purpose of object search. We demonstrate the validity of our approach by comparing it with two other search systems in terms of search trajectory length and time. First, we implement a greedy coverage-based search strategy that is found in previous work. Second, we let human participants search for objects as an alternative comparison for our method. Our results show that AVS strategies that exploit uncertain semantics of the environment are a very promising idea, and our method pushes the state-of-the-art forward in AVS.},
  note         = {65 citations (Crossref) [2022-02-28]},
  number       = {4},
  journal      = {IEEE Transactions on Robotics},
  author       = {Aydemir, Alper and Pronobis, Andrzej and Göbelbecker, Moritz and Jensfelt, Patric},
  year         = {2013},
  month        = {Aug},
  pages        = {986–1002}
}


@inproceedings{aydemir_real_2011,
  address      = {Shanghai, China},
  title        = {Search in the real world: Active visual object search based on spatial relations},
  isbn         = {978-1-61284-386-5},
  url          = {http://ieeexplore.ieee.org/document/5980495/},
  doi          = {10.1109/ICRA.2011.5980495},
  abstractnote = {Objects are integral to a robot’s understanding of space. Various tasks such as semantic mapping, pick-andcarry missions or manipulation involve interaction with objects. Previous work in the ﬁeld largely builds on the assumption that the object in question starts out within the ready sensory reach of the robot. In this work we aim to relax this assumption by providing the means to perform robust and large-scale active visual object search. Presenting spatial relations that describe topological relationships between objects, we then show how to use these to create potential search actions. We introduce a method for efﬁciently selecting search strategies given probabilities for those relations. Finally we perform experiments to verify the feasibility of our approach.},
  note         = {48 citations (Crossref) [2022-02-28]},
  booktitle    = {2011 IEEE International Conference on Robotics and Automation},
  publisher    = {IEEE},
  author       = {Aydemir, A. and Sjoo, K. and Folkesson, J. and Pronobis, A. and Jensfelt, P.},
  year         = {2011},
  month        = {May},
  pages        = {2818–2824},
  language     = {en}
}


@article{bahdanau_neural_2016,
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  url          = {http://arxiv.org/abs/1409.0473},
  abstractnote = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  note         = {arXiv: 1409.0473},
  journal      = {arXiv:1409.0473 [cs, stat]},
  author       = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year         = {2016},
  month        = {May}
}

@article{bajcsy_active_1988,
  title        = {Active perception},
  volume       = {76},
  issn         = {1558-2256},
  doi          = {10.1109/5.5968},
  abstract     = {Active perception (active vision specifically) is defined as a study of modeling and control strategies for perception. Local methods are distinguished from global models by their extent of application in space and time. The local models represent procedures and parameters such as optical distortions of the lens, focal lens, spatial resolution, bandpass filter, etc, The global models, on the other hand, characterize the overall performance and make predictions on how the individual modules interact. The control strategies are formulated as a search of such sequences of steps that would minimize a loss function while still seeking the most information. Examples are shown as the existence proof of the proposed theory on obtaining range from focus and stereo/vergence on 2-D segmentation of an image and 3-D shape parameterization.{\textless}{\textgreater}},
  pages        = {966--1005},
  number       = {8},
  journaltitle = {Proceedings of the {IEEE}},
  author       = {Bajcsy, R.},
  date         = {1988-08},
  note         = {646 citations (Crossref) [2022-02-28]
                  Conference Name: Proceedings of the {IEEE}},
  keywords     = {Band pass filters, Focusing, Image segmentation, Lenses, Optical distortion, Optical losses, Predictive models, Shape, Spatial resolution},
  file         = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/FCL5ZG3R/5968.html:text/html;Active perception:/home/oslund/Zotero/storage/6BV9NPKM/bajcsy1988.pdf.pdf:application/pdf}
}

@article{bajcsy_revisiting_2018,
  title        = {Revisiting active perception},
  volume       = {42},
  issn         = {1573-7527},
  url          = {https://doi.org/10.1007/s10514-017-9615-3},
  doi          = {10.1007/s10514-017-9615-3},
  abstract     = {Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.},
  pages        = {177--196},
  number       = {2},
  journaltitle = {Autonomous Robots},
  shortjournal = {Auton Robot},
  author       = {Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K.},
  urldate      = {2022-03-13},
  date         = {2018-02-01},
  langid       = {english},
  note         = {86 citations (Crossref) [2022-03-13]},
  file         = {Springer Full Text PDF:/home/oslund/Zotero/storage/KVGHH5A6/Bajcsy et al. - 2018 - Revisiting active perception.pdf:application/pdf}
}



@article{batra_evaluation_2020,
  title        = {ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects},
  url          = {http://arxiv.org/abs/2006.13171},
  abstractnote = {We revisit the problem of Object-Goal Navigation (ObjectNav). In its simplest form, ObjectNav is deﬁned as the task of navigating to an object, speciﬁed by its label, in an unexplored environment. In particular, the agent is initialized at a random location and pose in an environment and asked to ﬁnd an instance of an object category, e.g. ‘ﬁnd a chair’, by navigating to it.},
  note         = {arXiv: 2006.13171},
  journal      = {arXiv:2006.13171 [cs]},
  author       = {Batra, Dhruv and Gokaslan, Aaron and Kembhavi, Aniruddha and Maksymets, Oleksandr and Mottaghi, Roozbeh and Savva, Manolis and Toshev, Alexander and Wijmans, Erik},
  year         = {2020},
  month        = {Aug}
}

@article{bengio_representation_2014,
  title        = {Representation Learning: A Review and New Perspectives},
  url          = {http://arxiv.org/abs/1206.5538},
  abstractnote = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  note         = {arXiv: 1206.5538},
  journal      = {arXiv:1206.5538 [cs]},
  author       = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year         = {2014},
  month        = {Apr}
}

@article{bonin-font_visnav_2008,
  title        = {Visual Navigation for Mobile Robots: A Survey},
  volume       = {53},
  issn         = {0921-0296, 1573-0409},
  doi          = {10.1007/s10846-008-9235-4},
  abstractnote = {Mobile robot vision-based navigation has been the source of countless research contributions, from the domains of both vision and control. Vision is becoming more and more common in applications such as localization, automatic map construction, autonomous navigation, path following, inspection, monitoring or risky situation detection. This survey presents those pieces of work, from the nineties until nowadays, which constitute a wide progress in visual navigation techniques for land, aerial and autonomous underwater vehicles. The paper deals with two major approaches: map-based navigation and mapless navigation. Map-based navigation has been in turn subdivided in metric map-based navigation and topological mapbased navigation. Our outline to mapless navigation includes reactive techniques based on qualitative characteristics extraction, appearance-based localization, optical ﬂow, features tracking, plane ground detection/tracking, etc... The recent concept of visual sonar has also been revised.},
  number       = {3},
  journal      = {Journal of Intelligent and Robotic Systems},
  author       = {Bonin-Font, Francisco and Ortiz, Alberto and Oliver, Gabriel},
  year         = {2008},
  month        = {Nov},
  pages        = {263–296}
}


@article{brockman_gym_2016,
  title        = {OpenAI Gym},
  url          = {http://arxiv.org/abs/1606.01540},
  abstractnote = {OpenAI Gym1 is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  note         = {arXiv: 1606.01540},
  journal      = {arXiv:1606.01540 [cs]},
  author       = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year         = {2016},
  month        = {Jun}
}


@article{brockman_openai_2016,
  title        = {{OpenAI} Gym},
  url          = {http://arxiv.org/abs/1606.01540},
  abstract     = {{OpenAI} Gym1 is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of {OpenAI} Gym and the design decisions that went into the software.},
  journaltitle = {{arXiv}:1606.01540 [cs]},
  author       = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  urldate      = {2022-03-10},
  date         = {2016-06-05},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1606.01540},
  keywords     = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  file         = {Brockman et al. - 2016 - OpenAI Gym.pdf:/home/oslund/Zotero/storage/6DWT7K4D/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf}
}

@book{brundage_malicious_2018,
  title        = {The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation},
  url          = {http://arxiv.org/abs/1802.07228},
  doi          = {10.48550/arXiv.1802.07228},
  abstractnote = {This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.},
  note         = {arXiv:1802.07228 [cs]
                  type: article},
  number       = {arXiv:1802.07228},
  institution  = {arXiv},
  author       = {Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and Anderson, Hyrum and Roff, Heather and Allen, Gregory C. and Steinhardt, Jacob and Flynn, Carrick and hÉigeartaigh, Seán Ó and Beard, Simon and Belfield, Haydn and Farquhar, Sebastian and Lyle, Clare and Crootof, Rebecca and Evans, Owain and Page, Michael and Bryson, Joanna and Yampolskiy, Roman and Amodei, Dario},
  year         = {2018},
  month        = {Feb}
}

@article{caicedo_active_2015,
  title        = {Active Object Localization with Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1511.06015},
  abstract     = {We present an active detection model for localizing objects in scenes. The model is class-speciﬁc and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most speciﬁc location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal {VOC} 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
  journaltitle = {{arXiv}:1511.06015 [cs]},
  author       = {Caicedo, Juan C. and Lazebnik, Svetlana},
  urldate      = {2022-02-03},
  date         = {2015-11-18},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1511.06015},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition},
  file         = {Caicedo and Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:/home/oslund/Zotero/storage/ENEDBIBQ/Caicedo and Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:application/pdf}
}


@article{chaplot_object_2020,
  title        = {Object Goal Navigation using Goal-Oriented Semantic Exploration},
  url          = {http://arxiv.org/abs/2007.00643},
  abstractnote = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration’ which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
  note         = {arXiv: 2007.00643},
  journal      = {arXiv:2007.00643 [cs]},
  author       = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
  year         = {2020},
  month        = {Jul}
}

@article{chen_active_2011_2011,
  title        = {Active vision in robotic systems: A survey of recent developments},
  volume       = {30},
  issn         = {0278-3649},
  doi          = {10.1177/0278364911410755},
  abstractnote = {In this paper we provide a broad survey of developments in active vision in robotic applications over the last 15 years. With increasing demand for robotic automation, research in this area has received much attention. Among the many factors that can be attributed to a high-performance robotic system, the planned sensing or acquisition of perceptions on the operating environment is a crucial component. The aim of sensor planning is to determine the pose and settings of vision sensors for undertaking a vision-based task that usually requires obtaining multiple views of the object to be manipulated. Planning for robot vision is a complex problem for an active system due to its sensing uncertainty and environmental uncertainty. This paper describes such problems arising from many applications, e.g. object recognition and modeling, site reconstruction and inspection, surveillance, tracking and search, as well as robotic manipulation and assembly, localization and mapping, navigation and exploration. A bundle of solutions and methods have been proposed to solve these problems in the past. They are summarized in this review while enabling readers to easily refer solution methods for practical applications. Representative contributions, their evaluations, analyses, and future research trends are also addressed in an abstract level.},
  number       = {11},
  journal      = {The International Journal of Robotics Research},
  publisher    = {SAGE Publications Ltd STM},
  author       = {Chen, Shengyong and Li, Youfu and Kwok, Ngai Ming},
  year         = {2011},
  month        = {Sep},
  pages        = {1343–1377}
}


@inproceedings{chen_spatial_2017,
  place        = {Venice},
  title        = {Spatial Memory for Context Reasoning in Object Detection},
  isbn         = {978-1-5386-1032-9},
  url          = {http://ieeexplore.ieee.org/document/8237702/},
  doi          = {10.1109/ICCV.2017.440},
  abstractnote = {Modeling instance-level context and object-object relationships is extremely challenging. It requires reasoning about bounding boxes of different classes, locations etc. Above all, instance-level spatial reasoning inherently requires modeling conditional distributions on previous detections. Unfortunately, our current object detection systems do not have any memory to remember what to condition on! The state-of-the-art object detectors still detect all object in parallel followed by non-maximal suppression (NMS). While memory has been used for tasks such as captioning, they mostly use image-level memory cells without capturing the spatial layout. On the other hand, modeling object-object relationships requires spatial reasoning –not only do we need a memory to store the spatial layout, but also a effective reasoning module to extract spatial patterns. This paper presents a conceptually simple yet powerful solution – Spatial Memory Network (SMN), to model the instance-level context efﬁciently and effectively. Our spatial memory essentially assembles object instances back into a pseudo “image” representation that is easy to be fed into another ConvNet for object-object context reasoning. This leads to a new sequential reasoning architecture where image and memory are processed in parallel to obtain detections which update the memory again. We show our SMN direction is promising as it provides 2.2% improvement over baseline Faster RCNN on the COCO dataset with VGG161.},
  booktitle    = {2017 IEEE International Conference on Computer Vision (ICCV)},
  publisher    = {IEEE},
  author       = {Chen, Xinlei and Gupta, Abhinav},
  year         = {2017},
  month        = {Oct},
  pages        = {4106–4116}
}


@article{cobbe_generalization_2019,
  title        = {Quantifying Generalization in Reinforcement Learning},
  url          = {http://arxiv.org/abs/1812.02341},
  abstractnote = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent’s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
  note         = {arXiv: 1812.02341},
  journal      = {arXiv:1812.02341 [cs, stat]},
  author       = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  year         = {2019},
  month        = {Jul}
}


@article{cobbe_leveraging_2020,
  title        = {Leveraging Procedural Generation to Benchmark Reinforcement Learning},
  url          = {http://arxiv.org/abs/1912.01588},
  abstractnote = {We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efﬁciency and generalization in reinforcement learning. We believe that the community will beneﬁt from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, ﬁnding that larger models signiﬁcantly improve both sample efﬁciency and generalization.},
  note         = {arXiv: 1912.01588},
  journal      = {arXiv:1912.01588 [cs, stat]},
  author       = {Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
  year         = {2020},
  month        = {Jul}
}

@article{colas_hitchhikers_2019,
  title        = {A Hitchhiker’s Guide to Statistical Comparisons of Reinforcement Learning Algorithms},
  url          = {http://arxiv.org/abs/1904.06979},
  abstractnote = {Consistently checking the statistical signiﬁcance of experimental results is the ﬁrst mandatory step towards reproducible science. This paper presents a hitchhiker’s guide to rigorous comparisons of reinforcement learning algorithms. After introducing the concepts of statistical testing, we review the relevant statistical tests and compare them empirically in terms of false positive rate and statistical power as a function of the sample size (number of seeds) and effect size. We further investigate the robustness of these tests to violations of the most common hypotheses (normal distributions, same distributions, equal variances). Beside simulations, we compare empirical distributions obtained by running Soft-Actor Critic and Twin-Delayed Deep Deterministic Policy Gradient on Half-Cheetah. We conclude by providing guidelines and code to perform rigorous comparisons of RL algorithm performances.},
  note         = {arXiv: 1904.06979},
  journal      = {arXiv:1904.06979 [cs, stat]},
  author       = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  year         = {2019},
  month        = {Apr},
  language     = {en}
}

@article{colas_seeds_2018,
  title        = {How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments},
  url          = {http://arxiv.org/abs/1806.08295},
  abstractnote = {Consistently checking the statistical signiﬁcance of experimental results is one of the mandatory methodological steps to address the so-called “reproducibility crisis” in deep reinforcement learning. In this tutorial paper, we explain how the number of random seeds relates to the probabilities of statistical errors. For both the t-test and the bootstrap conﬁdence interval test, we recall theoretical guidelines to determine the number of random seeds one should use to provide a statistically signiﬁcant comparison of the performance of two algorithms. Finally, we discuss the inﬂuence of deviations from the assumptions usually made by statistical tests. We show that they can lead to inaccurate evaluations of statistical errors and provide guidelines to counter these negative eﬀects. We make our code available to perform the tests1.},
  note         = {arXiv: 1806.08295},
  journal      = {arXiv:1806.08295 [cs, stat]},
  author       = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  year         = {2018},
  month        = {Jul},
  language     = {en}
}

@article{dhiman_critical_2019,
  title        = {A Critical Investigation of Deep Reinforcement Learning for Navigation},
  url          = {http://arxiv.org/abs/1802.02274},
  abstractnote = {The navigation problem is classically approached in two steps: an exploration step, where map-information about the environment is gathered; and an exploitation step, where this information is used to navigate efficiently. Deep reinforcement learning (DRL) algorithms, alternatively, approach the problem of navigation in an end-to-end fashion. Inspired by the classical approach, we ask whether DRL algorithms are able to inherently explore, gather and exploit map-information over the course of navigation. We build upon Mirowski et al. [2017] work and introduce a systematic suite of experiments that vary three parameters: the agent’s starting location, the agent’s target location, and the maze structure. We choose evaluation metrics that explicitly measure the algorithm’s ability to gather and exploit map-information. Our experiments show that when trained and tested on the same maps, the algorithm successfully gathers and exploits map-information. However, when trained and tested on different sets of maps, the algorithm fails to transfer the ability to gather and exploit map-information to unseen maps. Furthermore, we find that when the goal location is randomized and the map is kept static, the algorithm is able to gather and exploit map-information but the exploitation is far from optimal. We open-source our experimental suite in the hopes that it serves as a framework for the comparison of future algorithms and leads to the discovery of robust alternatives to classical navigation methods.},
  note         = {arXiv: 1802.02274},
  journal      = {arXiv:1802.02274 [cs]},
  author       = {Dhiman, Vikas and Banerjee, Shurjo and Griffin, Brent and Siskind, Jeffrey M. and Corso, Jason J.},
  year         = {2019},
  month        = {Jan}
}

@article{eckstein_visual_2011,
  title        = {Visual search: A retrospective},
  volume       = {11},
  issn         = {1534-7362},
  url          = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/11.5.14},
  doi          = {10.1167/11.5.14},
  shorttitle   = {Visual search},
  pages        = {14--14},
  number       = {5},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  author       = {Eckstein, M. P.},
  urldate      = {2022-02-22},
  date         = {2011-12-30},
  langid       = {english},
  note         = {207 citations (Crossref) [2022-02-28]},
  file         = {Eckstein - 2011 - Visual search A retrospective.pdf:/home/oslund/Zotero/storage/E87E8PCH/Eckstein - 2011 - Visual search A retrospective.pdf:application/pdf;Visual search\: A retrospective:/home/oslund/Zotero/storage/PX9QE8ZC/eckstein2011.pdf.pdf:application/pdf}
}


@inproceedings{forssen_visual_2008,
  title        = {Informed visual search: Combining attention and object recognition},
  issn         = {1050-4729},
  doi          = {10.1109/ROBOT.2008.4543325},
  abstractnote = {This paper studies the sequential object recognition problem faced by a mobile robot searching for specific objects within a cluttered environment. In contrast to current state-of-the-art object recognition solutions which are evaluated on databases of static images, the system described in this paper employs an active strategy based on identifying potential objects using an attention mechanism and planning to obtain images of these objects from numerous viewpoints. We demonstrate the use of a bag-of-features technique for ranking potential objects, and show that this measure outperforms geometric matching for invariance across viewpoints. Our system implements informed visual search by prioritising map locations and re-examining promising locations first. Experimental results demonstrate that our system is a highly competent object recognition system that is capable of locating numerous challenging objects amongst distractors.},
  booktitle    = {2008 IEEE International Conference on Robotics and Automation},
  author       = {Forssen, Per-Erik and Meger, David and Lai, Kevin and Helmer, Scott and Little, James J. and Lowe, David G.},
  year         = {2008},
  month        = {May},
  pages        = {935–942}
}

@article{galceran_carreras_2013,
  title        = {A survey on coverage path planning for robotics},
  volume       = {61},
  issn         = {09218890},
  doi          = {10/f5j2n5},
  abstractnote = {Coverage Path Planning (CPP) is the task of determining a path that passes over all points of an area or volume of interest while avoiding obstacles. This task is integral to many robotic applications, such as vacuum cleaning robots, painter robots, autonomous underwater vehicles creating image mosaics, demining robots, lawn mowers, automated harvesters, window cleaners and inspection of complex structures, just to name a few. A considerable body of research has addressed the CPP problem. However, no updated surveys on CPP reflecting recent advances in the field have been presented in the past ten years. In this paper, we present a review of the most successful CPP methods, focusing on the achievements made in the past decade. Furthermore, we discuss reported field applications of the described CPP methods. This work aims to become a starting point for researchers who are initiating their endeavors in CPP. Likewise, this work aims to present a comprehensive review of the recent breakthroughs in the field, providing links to the most interesting and successful works.},
  number       = {12},
  journal      = {Robotics and Autonomous Systems},
  author       = {Galceran, Enric and Carreras, Marc},
  year         = {2013},
  month        = {Dec},
  pages        = {1258–1276}
}

@inbook{ghesu_artificial_2016,
  place        = {Cham},
  series       = {Lecture Notes in Computer Science},
  title        = {An Artificial Agent for Anatomical Landmark Detection in Medical Images},
  volume       = {9902},
  isbn         = {978-3-319-46725-2},
  url          = {https://link.springer.com/10.1007/978-3-319-46726-9_27},
  doi          = {10.1007/978-3-319-46726-9_27},
  abstractnote = {Fast and robust detection of anatomical structures or pathologies represents a fundamental task in medical image analysis. Most of the current solutions are however suboptimal and unconstrained by learning an appearance model and exhaustively scanning the space of parameters to detect a speciﬁc anatomical structure. In addition, typical feature computation or estimation of meta-parameters related to the appearance model or the search strategy, is based on local criteria or predeﬁned approximation schemes. We propose a new learning method following a fundamentally diﬀerent paradigm by simultaneously modeling both the object appearance and the parameter search strategy as a uniﬁed behavioral task for an artiﬁcial agent. The method combines the advantages of behavior learning achieved through reinforcement learning with eﬀective hierarchical feature extraction achieved through deep learning. We show that given only a sequence of annotated images, the agent can automatically and strategically learn optimal paths that converge to the sought anatomical landmark location as opposed to exhaustively scanning the entire solution space. The method signiﬁcantly outperforms state-ofthe-art machine learning and deep learning approaches both in terms of accuracy and speed on 2D magnetic resonance images, 2D ultrasound and 3D CT images, achieving average detection errors of 1-2 pixels, while also recognizing the absence of an object from the image.},
  booktitle    = {Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016},
  publisher    = {Springer International Publishing},
  author       = {Ghesu, Florin C. and Georgescu, Bogdan and Mansi, Tommaso and Neumann, Dominik and Hornegger, Joachim and Comaniciu, Dorin},
  editor       = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
  year         = {2016},
  pages        = {229–237},
  collection   = {Lecture Notes in Computer Science}
}

@article{ghesu_multiscale_2019,
  title        = {Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in {CT} Scans},
  volume       = {41},
  issn         = {0162-8828, 2160-9292, 1939-3539},
  url          = {https://ieeexplore.ieee.org/document/8187667/},
  doi          = {10.1109/TPAMI.2017.2782687},
  abstract     = {Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem as a behavior learning task for an artiﬁcial agent. We couple the modeling of the anatomy appearance and the object search in a uniﬁed behavioral framework, using the capabilities of deep reinforcement learning and multi-scale image analysis. In other words, an artiﬁcial agent is trained not only to distinguish the target anatomical object from the rest of the body but also how to ﬁnd the object by learning and following an optimal navigation path to the target object in the imaged volumetric space. We evaluate our approach on 1487 3D-{CT} volumes from 532 patients, totaling over 500,000 image slices and show that we signiﬁcantly outperform state-of-the-art solutions on detecting several anatomical structures with no failed cases from a clinical acceptance perspective, while also improving the detection accuracy by 20-30\%. Most importantly, we improve the detection-speed of the reference methods by 2-3 orders of magnitude, achieving unmatched real-time performance on large 3D-{CT} scans.},
  pages        = {176--189},
  number       = {1},
  journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  author       = {Ghesu, Florin-Cristian and Georgescu, Bogdan and Zheng, Yefeng and Grbic, Sasa and Maier, Andreas and Hornegger, Joachim and Comaniciu, Dorin},
  urldate      = {2022-02-03},
  date         = {2019-01-01},
  langid       = {english},
  file         = {Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:/home/oslund/Zotero/storage/TF4ZNMC6/Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:application/pdf}
}

@book{goodfellow_deep_2016,
  place        = {Cambridge, MA, USA},
  series       = {Adaptive Computation and Machine Learning series},
  title        = {Deep Learning},
  isbn         = {978-0-262-03561-3},
  abstractnote = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.},
  publisher    = {MIT Press},
  author       = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year         = {2016},
  month        = {Nov},
  collection   = {Adaptive Computation and Machine Learning series}
}

@article{gupta_cognitive_2019,
  title        = {Cognitive Mapping and Planning for Visual Navigation},
  url          = {http://arxiv.org/abs/1702.03920},
  abstractnote = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as “going to a chair”. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.},
  note         = {arXiv: 1702.03920},
  journal      = {arXiv:1702.03920 [cs]},
  author       = {Gupta, Saurabh and Tolani, Varun and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
  year         = {2019},
  month        = {Feb}
}


@article{hausknecht_stone_2017,
  title        = {Deep Recurrent Q-Learning for Partially Observable MDPs},
  url          = {http://arxiv.org/abs/1507.06527},
  abstractnote = {Deep Reinforcement Learning has yielded proﬁcient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the ﬁrst post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN’s performance on standard Atari games and partially observed equivalents featuring ﬂickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN’s performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN’s performance degrades less than DQN’s. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN’s input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  note         = {arXiv: 1507.06527},
  journal      = {arXiv:1507.06527 [cs]},
  author       = {Hausknecht, Matthew and Stone, Peter},
  year         = {2017},
  month        = {Jan}
}

@article{henderson_matters_2018,
  title        = {Deep Reinforcement Learning That Matters},
  volume       = {32},
  issn         = {2374-3468},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11694},
  abstractnote = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  number       = {11},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  year         = {2018},
  month        = {Apr}
}


@inproceedings{henriques_vedaldi_2018,
  place        = {Salt Lake City, UT},
  title        = {MapNet: An Allocentric Spatial Memory for Mapping Environments},
  isbn         = {978-1-5386-6420-9},
  url          = {https://ieeexplore.ieee.org/document/8578982/},
  doi          = {10.1109/CVPR.2018.00884},
  abstractnote = {Autonomous agents need to reason about the world beyond their instantaneous sensory input. Integrating information over time, however, requires switching from an egocentric representation of a scene to an allocentric one, expressed in the world reference frame. It must also be possible to update the representation dynamically, which requires localizing and registering the sensor with respect to the world reference. In this paper, we develop a differentiable module that satisﬁes such requirements, while being robust, efﬁcient, and suitable for integration in end-to-end deep networks. The module contains an allocentric spatial memory that can be accessed associatively by feeding to it the current sensory input, resulting in localization, and then updated using an LSTM or similar mechanism. We formulate efﬁcient localization and registration of sensory information as a dual pair of convolution/deconvolution operators in memory space. The map itself is a 2.5D representation of an environment storing information that a deep neural network module learns to distill from RGBD input. The result is a map that contains multi-task information, different from classical approaches to mapping such as structurefrom-motion. We present results using synthetic mazes, a dataset of hours of recorded gameplay of the classic game Doom, and the very recent Active Vision Dataset of real images captured from a robot.},
  booktitle    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher    = {IEEE},
  author       = {Henriques, Joao F. and Vedaldi, Andrea},
  year         = {2018},
  month        = {Jun},
  pages        = {8476–8484}
}


@article{hessel_inductive_2019,
  title        = {On Inductive Biases in Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1907.02908},
  abstractnote = {Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent’s objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep reinforcement learning agents. We investigated whether the performance deteriorates when these components are replaced with adaptive solutions from the literature. In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system. As hypothesized, the system with adaptive components performed better on many of the new tasks.},
  note         = {arXiv: 1907.02908},
  journal      = {arXiv:1907.02908 [cs, stat]},
  author       = {Hessel, Matteo and van Hasselt, Hado and Modayil, Joseph and Silver, David},
  year         = {2019},
  month        = {Jul}
}

@article{hochreiter_schmidhuber_lstm_1997,
  title        = {Long Short-Term Memory},
  volume       = {9},
  issn         = {0899-7667},
  doi          = {10.1162/neco.1997.9.8.1735},
  abstractnote = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter’s (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  number       = {8},
  journal      = {Neural Computation},
  author       = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  year         = {1997},
  month        = {Nov},
  pages        = {1735–1780}
}

@article{itti_computational_2001,
  title        = {Computational modelling of visual attention},
  volume       = {2},
  issn         = {1471-0048},
  doi          = {10.1038/35058500},
  abstractnote = {We review recent work on computational models of focal visual attention, with emphasis on the bottom-up, saliency- or image-based control of attentional deployment. We highlight five important trends that have emerged from the computational literature: First, the perceptual saliency of stimuli critically depends on surrounding context; that is, the same object may or may not appear salient depending on the nature and arrangement of other objects in the scene. Computationally, this means that contextual influences, such as non-classical surround interactions, must be included in models. Second, a unique “saliency map” topographically encoding for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Many successful models are based on such architecture, and electrophysiological as well as psychophysical studies have recently supported the idea that saliency is explicitly encoded in the brain. Third, inhibition-of-return (IOR), the process by which the currently attended location is transiently inhibited, is a critical element of attentional deployment. Without IOR, attention would endlessly be attracted towards the most salient stimulus. IOR thus implements a memory of recently visited locations, and allows attention to thoroughly scan our visual environment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. Understanding the interaction between overt and covert attention is particularly important for models concerned with visual search. Last, scene understanding and object recognition strongly constrain the selection of attended locations. Although several models have approached, in an information-theoretical sense, the problem of optimally deploying attention to analyse a scene, biologically plausible implementations of such a computational strategy remain to be developed.},
  number       = {33},
  journal      = {Nature Reviews Neuroscience},
  publisher    = {Nature Publishing Group},
  author       = {Itti, Laurent and Koch, Christof},
  year         = {2001},
  month        = {Mar},
  pages        = {194–203}
}


@article{kaelbling_pomdp_1998,
  title        = {Planning and acting in partially observable stochastic domains},
  volume       = {101},
  issn         = {00043702},
  doi          = {10.1016/S0004-3702(98)00023-X},
  abstractnote = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPs) and partially observable MDPs (POMDPs). We then outline a novel algorithm for solving POMDPs off line and show how, in some cases, a ﬁnite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of ﬁnding exact solutions to POMDPs, and of some possibilities for ﬁnding approximate solutions. © 1998 Elsevier Science B.V. All rights reserved.},
  number       = {1–2},
  journal      = {Artificial Intelligence},
  author       = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  year         = {1998},
  month        = {May},
  pages        = {99–134}
}


@article{kingma_ba_2017,
  title        = {Adam: A Method for Stochastic Optimization},
  url          = {http://arxiv.org/abs/1412.6980},
  abstractnote = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  note         = {arXiv: 1412.6980},
  journal      = {arXiv:1412.6980 [cs]},
  author       = {Kingma, Diederik P. and Ba, Jimmy},
  year         = {2017},
  month        = {Jan}
}


@article{kirk_survey_2022,
  title        = {A Survey of Generalisation in Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/2111.09794},
  abstractnote = {The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as offline RL generalisation and reward-function variation.},
  note         = {arXiv: 2111.09794},
  journal      = {arXiv:2111.09794 [cs]},
  author       = {Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rocktäschel, Tim},
  year         = {2022},
  month        = {Jan}
}

@article{krishna_tetromino_2020,
  title        = {Complete coverage path planning using reinforcement learning for Tetromino based cleaning and maintenance robot},
  volume       = {112},
  issn         = {0926-5805},
  doi          = {10.1016/j.autcon.2020.103078},
  abstractnote = {Tiling robotics have been deployed in autonomous complete area coverage tasks such as floor cleaning, building inspection, and maintenance, surface painting. One class of tiling robotics, polyomino-based reconfigurable robots, overcome the limitation of fixed-form robots in achieving high-efficiency area coverage by adopting different morphologies to suit the needs of the current environment. Since the reconfigurable actions of these robots are produced by real-time intelligent decisions during operations, an optimal path planning algorithm is paramount to maximize the area coverage while minimizing the energy consumed by these robots. This paper proposes a complete coverage path planning (CCPP) model trained using deep blackreinforcement learning (RL) for the tetromino based reconfigurable robot platform called hTetro to simultaneously generate the optimal set of shapes for any pretrained arbitrary environment shape with a trajectory that has the least overall cost. To this end, a Convolutional Neural Network (CNN) with Long Short Term Memory (LSTM) layers is trained using Actor Critic Experience Replay (ACER) reinforcement learning algorithm. The results are compared with existing approaches which are based on the traditional tiling theory model, including zigzag, spiral, and greedy search schemes. The model is also compared with the Travelling salesman problem (TSP) based Genetic Algorithm (GA) and Ant Colony Optimization (ACO) schemes. The proposed scheme generates a path with lower cost while also requiring lesser time to generate it. The model is also highly robust and can generate a path in any pretrained arbitrary environments.},
  journal      = {Automation in Construction},
  author       = {Krishna Lakshmanan, Anirudh and Elara Mohan, Rajesh and Ramalingam, Balakrishnan and Vu Le, Anh and Veerajagadeshwar, Prabahar and Tiwari, Kamlesh and Ilyas, Muhammad},
  year         = {2020},
  month        = {Apr},
  pages        = {103078}
}

@inbook{mataric_reward_1994,
  place        = {San Francisco (CA)},
  title        = {Reward Functions for Accelerated Learning},
  isbn         = {978-1-55860-335-6},
  url          = {https://www.sciencedirect.com/science/article/pii/B9781558603356500301},
  doi          = {10.1016/B978-1-55860-335-6.50030-1},
  abstractnote = {This paper discusses why traditional reinforcement learning methods, and algorithms applied to those models, result in poor performance in situated domains characterized by multiple goals, noisy state, and inconsistent reinforcement. We propose a methodology for designing reinforcement functions that take advantage of implicit domain knowledge in order to accelerate learning in such domains. The methodology involves the use of heterogeneous reinforcement functions and progress estimators, and applies to learning in domains with a single agent or with multiple agents. The methodology is experimentally validated on a group of mobile robots learning a foraging task.},
  booktitle    = {Machine Learning Proceedings 1994},
  publisher    = {Morgan Kaufmann},
  author       = {Mataric, Maja J},
  editor       = {Cohen, William W. and Hirsh, Haym},
  year         = {1994},
  month        = {Jan},
  pages        = {181–189}
}

@article{meger_curious_2008,
  series       = {From Sensors to Human Spatial Concepts},
  title        = {Curious George: An attentive semantic robot},
  volume       = {56},
  issn         = {0921-8890},
  doi          = {10.1016/j.robot.2008.03.008},
  abstractnote = {State-of-the-art methods have recently achieved impressive performance for recognising the objects present in large databases of pre-collected images. There has been much less focus on building embodied systems that recognise objects present in the real world. This paper describes an intelligent system that attempts to perform robust object recognition in a realistic scenario, where a mobile robot moving through an environment must use the images collected from its camera directly to recognise objects. To perform successful recognition in this scenario, we have chosen a combination of techniques including a peripheral-foveal vision system, an attention system combining bottom-up visual saliency with structure from stereo, and a localisation and mapping technique. The result is a highly capable object recognition system that can be easily trained to locate the objects of interest in an environment, and subsequently build a spatial-semantic map of the region. This capability has been demonstrated during the Semantic Robot Vision Challenge, and is further illustrated with a demonstration of semantic mapping. We also empirically verify that the attention system outperforms an undirected approach even with a significantly lower number of foveations.},
  number       = {6},
  journal      = {Robotics and Autonomous Systems},
  author       = {Meger, David and Forssén, Per-Erik and Lai, Kevin and Helmer, Scott and McCann, Sancho and Southey, Tristram and Baumann, Matthew and Little, James J. and Lowe, David G.},
  year         = {2008},
  month        = {Jun},
  pages        = {503–511},
  collection   = {From Sensors to Human Spatial Concepts},
  language     = {en}
}

@article{minsky_steps_1961,
  title        = {Steps toward Artificial Intelligence},
  volume       = {49},
  issn         = {2162-6634},
  doi          = {10.1109/JRPROC.1961.287775},
  abstractnote = {The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine’s methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.},
  number       = {1},
  journal      = {Proceedings of the IRE},
  author       = {Minsky, Marvin},
  year         = {1961},
  month        = {Jan},
  pages        = {8–30}
}


@inproceedings{minut_mahadevan_2001,
  place     = {Montreal, Quebec, Canada},
  title     = {A reinforcement learning model of selective visual attention},
  isbn      = {978-1-58113-326-4},
  url       = {http://portal.acm.org/citation.cfm?doid=375735.376414},
  doi       = {10/dbwckq},
  booktitle = {Proceedings of the fifth international conference on Autonomous agents  - AGENTS ’01},
  publisher = {ACM Press},
  author    = {Minut, Silviu and Mahadevan, Sridhar},
  year      = {2001},
  pages     = {457–464}
}

@article{mirowski_navigate_2017,
  title        = {Learning to Navigate in Complex Environments},
  url          = {http://arxiv.org/abs/1611.03673},
  abstractnote = {Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.},
  note         = {arXiv: 1611.03673},
  journal      = {arXiv:1611.03673 [cs]},
  author       = {Mirowski, Piotr and Pascanu, Razvan and Viola, Fabio and Soyer, Hubert and Ballard, Andrew J. and Banino, Andrea and Denil, Misha and Goroshin, Ross and Sifre, Laurent and Kavukcuoglu, Koray and Kumaran, Dharshan and Hadsell, Raia},
  year         = {2017},
  month        = {Jan}
}


@article{mnih_asynchronous_2016,
  title        = {Asynchronous Methods for Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1602.01783},
  abstractnote = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  note         = {arXiv: 1602.01783},
  journal      = {arXiv:1602.01783 [cs]},
  author       = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year         = {2016},
  month        = {Jun}
}



@article{mnih_atari_2013,
  title        = {Playing Atari with Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1312.5602},
  abstractnote = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  note         = {arXiv: 1312.5602},
  journal      = {arXiv:1312.5602 [cs]},
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year         = {2013},
  month        = {Dec}
}

@article{mnih_recurrent_2014,
  title        = {Recurrent Models of Visual Attention},
  url          = {http://arxiv.org/abs/1406.6247},
  abstractnote = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-speciﬁc policies. We evaluate our model on several image classiﬁcation tasks, where it signiﬁcantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
  note         = {arXiv: 1406.6247},
  journal      = {arXiv:1406.6247 [cs, stat]},
  author       = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
  year         = {2014},
  month        = {Jun}
}

@article{mnih_human_2015,
  title        = {Human-level control through deep reinforcement learning},
  volume       = {518},
  issn         = {1476-4687},
  doi          = {10.1038/nature14236},
  abstractnote = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  number       = {75407540},
  journal      = {Nature},
  publisher    = {Nature Publishing Group},
  author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year         = {2015},
  month        = {Feb},
  pages        = {529–533}
}

@article{nakayama_situating_2011,
  series       = {Vision Research 50th Anniversary Issue: Part 2},
  title        = {Situating visual search},
  volume       = {51},
  issn         = {0042-6989},
  doi          = {10.1016/j.visres.2010.09.003},
  abstractnote = {Visual search attracted great interest because its ease under certain circumstances seemed to provide a way to understand how properties of early visual cortical areas could explain complex perception without resorting to higher order psychological or neurophysiological mechanisms. Furthermore, there was the hope that properties of visual search itself might even reveal new cortical features or dimensions. The shortcomings of this perspective suggest that we abandon fixed canonical elementary particles of vision as well as a corresponding simple to complex cognitive architecture for vision. Instead recent research has suggested a different organization of the visual brain with putative high level processing occurring very rapidly and often unconsciously. Given this outlook, we reconsider visual search under the broad category of recognition tasks, each having different trade-offs for computational resources, between detail and scope. We conclude noting recent trends showing how visual search is relevant to a wider range of issues in cognitive science, in particular to memory, decision making, and reward.},
  number       = {13},
  journal      = {Vision Research},
  author       = {Nakayama, Ken and Martini, Paolo},
  year         = {2011},
  month        = {Jul},
  pages        = {1526–1537},
  collection   = {Vision Research 50th Anniversary Issue: Part 2}
}

@article{oh_control_2016,
  title        = {Control of Memory, Active Perception, and Action in Minecraft},
  url          = {http://arxiv.org/abs/1605.09128},
  abstractnote = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
  note         = {arXiv: 1605.09128},
  journal      = {arXiv:1605.09128 [cs]},
  author       = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
  year         = {2016},
  month        = {May}
}


@article{pardo_timelimits_2022,
  title        = {Time Limits in Reinforcement Learning},
  url          = {http://arxiv.org/abs/1712.00378},
  abstractnote = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent’s input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
  note         = {arXiv: 1712.00378},
  journal      = {arXiv:1712.00378 [cs]},
  author       = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
  year         = {2022},
  month        = {Jan}
}


@article{parisotto_neural_2017,
  title        = {Neural Map: Structured Memory for Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1702.08360},
  abstractnote = {A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.},
  note         = {arXiv: 1702.08360},
  journal      = {arXiv:1702.08360 [cs]},
  author       = {Parisotto, Emilio and Salakhutdinov, Ruslan},
  year         = {2017},
  month        = {Feb}
}

@article{paszke_pytorch_nodate,
  title    = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. {PyTorch} is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiﬁc computing libraries, while remaining efﬁcient and supporting hardware accelerators such as {GPUs}.},
  pages    = {12},
  author   = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and {DeVito}, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  langid   = {english},
  file     = {Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:/home/oslund/Zotero/storage/C6CWIML9/Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf}
}


@book{russell_artificial_2021,
  location    = {Hoboken, {NJ}},
  edition     = {Fourth Edition},
  title       = {Artificial intelligence: a modern approach},
  isbn        = {978-0-13-461099-3},
  series      = {Pearson Series in Artificial Intelligence},
  shorttitle  = {Artificial intelligence},
  abstract    = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  pagetotal   = {1115},
  publisher   = {Pearson},
  author      = {Russell, Stuart J. and Norvig, Peter},
  editora     = {Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikash and Pearl, Judea and Woolridge, Michael},
  editoratype = {collaborator},
  date        = {2021}
}

@inproceedings{russell_beneficial_2022,
  address      = {New York, NY, USA},
  series       = {IUI ’22},
  title        = {Provably Beneficial Artificial Intelligence},
  isbn         = {978-1-4503-9144-3},
  url          = {https://doi.org/10.1145/3490099.3519388},
  doi          = {10.1145/3490099.3519388},
  abstractnote = {As AI advances in capabilities and moves into the real world, its potential to benefit humanity seems limitless. Yet we see serious problems including racial and gender bias, manipulation by social media, and an arms race in lethal autonomous weapons. Looking further ahead, Alan Turing predicted the eventual loss of human control over machines that exceed human capabilities. I will argue that Turing was right to express concern but wrong to think that doom is inevitable. Instead, we need to develop a new kind of AI that is provably beneficial to humans.},
  booktitle    = {27th International Conference on Intelligent User Interfaces},
  publisher    = {Association for Computing Machinery},
  author       = {Russell, Stuart},
  year         = {2022},
  month        = {Mar},
  pages        = {3},
  collection   = {IUI ’22}
}




@article{russell_ethics_2015,
  volume       = {521},
  issn         = {1476-4687},
  doi          = {10.1038/521415a},
  abstractnote = {Four leading researchers share their concerns and solutions for reducing societal risks from intelligent machines.},
  number       = {75537553},
  journal      = {Nature},
  publisher    = {Nature Publishing Group},
  year         = {2015},
  month        = {May},
  pages        = {415–418},
  language     = {en}
}

@article{russell_priorities_2015,
  title        = {Research Priorities for Robust and Beneficial Artificial Intelligence},
  volume       = {36},
  issn         = {2371-9621},
  doi          = {10.1609/aimag.v36i4.2577},
  abstractnote = {Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.},
  number       = {44},
  journal      = {AI Magazine},
  author       = {Russell, Stuart and Dewey, Daniel and Tegmark, Max},
  year         = {2015},
  month        = {Dec},
  pages        = {105–114},
  language     = {en}
}


@article{savinov_topmem_2018,
  title        = {Semi-parametric Topological Memory for Navigation},
  url          = {http://arxiv.org/abs/1803.00653},
  abstractnote = {We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to conﬁdently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.},
  note         = {arXiv: 1803.00653},
  journal      = {arXiv:1803.00653 [cs]},
  author       = {Savinov, Nikolay and Dosovitskiy, Alexey and Koltun, Vladlen},
  year         = {2018},
  month        = {Mar}
}

@article{schulman_proximal_2017,
  title        = {Proximal Policy Optimization Algorithms},
  url          = {http://arxiv.org/abs/1707.06347},
  abstractnote = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  note         = {arXiv: 1707.06347},
  journal      = {arXiv:1707.06347 [cs]},
  author       = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year         = {2017},
  month        = {Aug}
}


@article{shubina_visual_2010,
  series       = {Special issue on Intelligent Vision Systems},
  title        = {Visual search for an object in a 3D environment using a mobile robot},
  volume       = {114},
  issn         = {1077-3142},
  doi          = {10.1016/j.cviu.2009.06.010},
  abstractnote = {Consider the problem of visually finding an object in a mostly unknown space with a mobile robot. It is clear that all possible views and images cannot be examined in a practical system. Visual attention is a complex phenomenon; we view it as a mechanism that optimizes the search processes inherent in vision (Tsotsos, 2001; Tsotsos et al., 2008) [1], [2]. Here, we describe a particular example of a practical robotic vision system that employs some of these attentive processes. We cast this as an optimization problem, i.e., optimizing the probability of finding the target given a fixed cost limit in terms of total number of robotic actions required to find the visual target. Due to the inherent intractability of this problem, we present an approximate solution and investigate its performance and properties. We conclude that our approach is sufficient to solve this problem and has additional desirable empirical characteristics.},
  number       = {5},
  journal      = {Computer Vision and Image Understanding},
  author       = {Shubina, Ksenia and Tsotsos, John K.},
  year         = {2010},
  month        = {May},
  pages        = {535–547},
  collection   = {Special issue on Intelligent Vision Systems}
}

@article{silver_mastering_2016,
  title        = {Mastering the game of Go with deep neural networks and tree search},
  volume       = {529},
  issn         = {1476-4687},
  doi          = {10.1038/nature16961},
  abstractnote = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  number       = {75877587},
  journal      = {Nature},
  publisher    = {Nature Publishing Group},
  author       = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year         = {2016},
  month        = {Jan},
  pages        = {484–489}
}

@inproceedings{sutton_policygrad_1999,
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  volume    = {12},
  url       = {https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  publisher = {MIT Press},
  author    = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  editor    = {Solla, S. and Leen, T. and Müller, K.},
  year      = {1999}
}



@book{sutton_reinforcement_2018,
  location   = {Cambridge, Massachusetts},
  edition    = {Second edition},
  title      = {Reinforcement learning: an introduction},
  isbn       = {978-0-262-03924-6},
  series     = {Adaptive computation and machine learning series},
  shorttitle = {Reinforcement learning},
  abstract   = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  pagetotal  = {526},
  publisher  = {The {MIT} Press},
  author     = {Sutton, Richard S. and Barto, Andrew G.},
  date       = {2018},
  langid     = {english},
  keywords   = {Reinforcement learning},
  file       = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/oslund/Zotero/storage/HPCN43YL/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf}
}

@article{tesauro1995tdgammon,
  title   = {Temporal difference learning and TD-Gammon},
  author  = {Tesauro, Gerald and others},
  journal = {Communications of the ACM},
  volume  = {38},
  number  = {3},
  pages   = {58--68},
  year    = {1995}
}

@article{uzkent_detection_2020,
  title        = {Efficient Object Detection in Large Images using Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1912.03966},
  abstractnote = {Traditionally, an object detector is applied to every part of the scene of interest, and its accuracy and computational cost increases with higher resolution images. However, in some application domains such as remote sensing, purchasing high spatial resolution images is expensive. To reduce the large computational and monetary cost associated with using high spatial resolution images, we propose a reinforcement learning agent that adaptively selects the spatial resolution of each image that is provided to the detector. In particular, we train the agent in a dual reward setting to choose low spatial resolution images to be run through a coarse level detector when the image is dominated by large objects, and high spatial resolution images to be run through a fine level detector when it is dominated by small objects. This reduces the dependency on high spatial resolution images for building a robust detector and increases run-time efficiency. We perform experiments on the xView dataset, consisting of large images, where we increase run-time efficiency by 50% and use high resolution images only 30% of the time while maintaining similar accuracy as a detector that uses only high resolution images.},
  note         = {arXiv: 1912.03966},
  journal      = {arXiv:1912.03966 [cs]},
  author       = {Uzkent, Burak and Yeh, Christopher and Ermon, Stefano},
  year         = {2020},
  month        = {Apr}
}

@article{vinuesa_sustainable_2020,
  title        = {The role of artificial intelligence in achieving the Sustainable Development Goals},
  volume       = {11},
  issn         = {2041-1723},
  doi          = {10.1038/s41467-019-14108-y},
  abstractnote = {The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that AI can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of AI needs to be supported by the necessary regulatory insight and oversight for AI-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards.},
  number       = {11},
  journal      = {Nature Communications},
  publisher    = {Nature Publishing Group},
  author       = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Felländer, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},
  year         = {2020},
  month        = {Jan},
  pages        = {233},
  language     = {en}
}

@article{vinyals_grandmaster_2019,
  title        = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  volume       = {575},
  issn         = {1476-4687},
  doi          = {10.1038/s41586-019-1724-z},
  abstractnote = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.},
  number       = {77827782},
  journal      = {Nature},
  publisher    = {Nature Publishing Group},
  author       = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year         = {2019},
  month        = {Nov},
  pages        = {350–354}
}

@article{wolfe_five_2017,
  title        = {Five factors that guide attention in visual search},
  volume       = {1},
  rights       = {2017 Macmillan Publishers Limited},
  issn         = {2397-3374},
  url          = {https://www.nature.com/articles/s41562-017-0058},
  doi          = {10.1038/s41562-017-0058},
  abstract     = {How do we find what we are looking for? Even when the desired target is in the current field of view, we need to search because fundamental limits on visual processing make it impossible to recognize everything at once. Searching involves directing attention to objects that might be the target. This deployment of attention is not random. It is guided to the most promising items and locations by five factors discussed here: bottom-up salience, top-down feature guidance, scene structure and meaning, the previous history of search over timescales ranging from milliseconds to years, and the relative value of the targets and distractors. Modern theories of visual search need to incorporate all five factors and specify how these factors combine to shape search behaviour. An understanding of the rules of guidance can be used to improve the accuracy and efficiency of socially important search tasks, from security screening to medical image perception.},
  pages        = {1--8},
  number       = {3},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  author       = {Wolfe, Jeremy M. and Horowitz, Todd S.},
  urldate      = {2022-02-28},
  date         = {2017-03-08},
  langid       = {english},
  note         = {300 citations (Crossref) [2022-02-28]
                  Number: 3
                  Publisher: Nature Publishing Group},
  keywords     = {Human behaviour, Visual system},
  file         = {Snapshot:/home/oslund/Zotero/storage/VF9C7HD9/s41562-017-0058.html:text/html;Full Text PDF:/home/oslund/Zotero/storage/8M4AIW8W/Wolfe and Horowitz - 2017 - Five factors that guide attention in visual search.pdf:application/pdf;Five factors that guide attention in visual search:/home/oslund/Zotero/storage/WZCJAX2U/wolfe2017.pdf.pdf:application/pdf}
}

@article{wolfe_guided_2021,
  title        = {Guided Search 6.0: An updated model of visual search},
  volume       = {28},
  issn         = {1531-5320},
  doi          = {10.3758/s13423-020-01859-9},
  shorttitle   = {Guided Search 6.0},
  abstract     = {This paper describes Guided Search 6.0 ({GS}6), a revised model of visual search. When we encounter a scene, we can see something everywhere. However, we cannot recognize more than a few items at a time. Attention is used to select items so that their features can be "bound" into recognizable objects. Attention is "guided" so that items can be processed in an intelligent order. In {GS}6, this guidance comes from five sources of preattentive information: (1) top-down and (2) bottom-up feature guidance, (3) prior history (e.g., priming), (4) reward, and (5) scene syntax and semantics. These sources are combined into a spatial "priority map," a dynamic attentional landscape that evolves over the course of search. Selective attention is guided to the most active location in the priority map approximately 20 times per second. Guidance will not be uniform across the visual field. It will favor items near the point of fixation. Three types of functional visual field ({FVFs}) describe the nature of these foveal biases. There is a resolution {FVF}, an {FVF} governing exploratory eye movements, and an {FVF} governing covert deployments of attention. To be identified as targets or rejected as distractors, items must be compared to target templates held in memory. The binding and recognition of an attended object is modeled as a diffusion process taking {\textgreater} 150 ms/item. Since selection occurs more frequently than that, it follows that multiple items are undergoing recognition at the same time, though asynchronously, making {GS}6 a hybrid of serial and parallel processes. In {GS}6, if a target is not found, search terminates when an accumulating quitting signal reaches a threshold. Setting of that threshold is adaptive, allowing feedback about performance to shape subsequent searches. Simulation shows that the combination of asynchronous diffusion and a quitting signal can produce the basic patterns of response time and error data from a range of search experiments.},
  pages        = {1060--1092},
  number       = {4},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  author       = {Wolfe, Jeremy M.},
  date         = {2021-08},
  pmid         = {33547630},
  note         = {29 citations (Crossref) [2022-03-02]},
  keywords     = {Attention, Bottom-up, Errors, Eye Movements, Guided search, Humans, Pattern Recognition, Visual, Reaction time, Reaction Time, Selective attention, Top-down, Visual Fields, Visual Perception, Visual search, Visual working memory},
  file         = {Full Text:/home/oslund/Zotero/storage/WBASNPNB/Wolfe - 2021 - Guided Search 6.0 An updated model of visual sear.pdf:application/pdf}
}


@article{wolfe_visual_2010,
  title        = {Visual search},
  volume       = {20},
  url          = {https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC5678963/},
  doi          = {10.1016/j.cub.2010.02.016},
  pages        = {R346},
  number       = {8},
  journaltitle = {Current biology : {CB}},
  author       = {Wolfe, Jeremy M.},
  urldate      = {2022-03-02},
  date         = {2010-04-27},
  langid       = {english},
  pmid         = {21749949},
  note         = {64 citations (Crossref) [2022-03-02]
                  Publisher: {NIH} Public Access},
  file         = {Full Text:/home/oslund/Zotero/storage/CLUR5J4F/Wolfe - 2010 - Visual search.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/F564KWBK/PMC5678963.html:text/html;Visual search:/home/oslund/Zotero/storage/N56NHYTS/wolfe2010.pdf.pdf:application/pdf}
}

@article{yang_semantic_2018,
  title        = {Visual Semantic Navigation using Scene Priors},
  url          = {http://arxiv.org/abs/1810.06543},
  abstractnote = {How do humans navigate to target objects in novel scenes? Do we use the semantic/functional priors we have built over years to efficiently search and navigate? For example, to search for mugs, we search cabinets near the coffee machine and for fruits we try the fridge. In this work, we focus on incorporating semantic priors in the task of semantic navigation. We propose to use Graph Convolutional Networks for incorporating the prior knowledge into a deep reinforcement learning framework. The agent uses the features from the knowledge graph to predict the actions. For evaluation, we use the AI2-THOR framework. Our experiments show how semantic knowledge improves performance significantly. More importantly, we show improvement in generalization to unseen scenes and/or objects. The supplementary video can be accessed at the following link: https://youtu.be/otKjuO805dE .},
  note         = {arXiv: 1810.06543},
  journal      = {arXiv:1810.06543 [cs]},
  author       = {Yang, Wei and Wang, Xiaolong and Farhadi, Ali and Gupta, Abhinav and Mottaghi, Roozbeh},
  year         = {2018},
  month        = {Oct}
}


@inproceedings{ye_active_2018,
  title      = {Active Object Perceiver: Recognition-Guided Policy Learning for Object Searching on Mobile Robots},
  doi        = {10.1109/IROS.2018.8593720},
  shorttitle = {Active Object Perceiver},
  abstract   = {We study the problem of learning a navigation policy for a robot to actively search for an object of interest in an indoor environment solely from its visual inputs. While scene-driven visual navigation has been widely studied, prior efforts on learning navigation policies for robots to find objects are limited. The problem is often more challenging than target scene finding as the target objects can be very small in the view and can be in an arbitrary pose. We approach the problem from an active perceiver perspective, and propose a novel framework that integrates a deep neural network based object recognition module and a deep reinforcement learning based action prediction mechanism. To validate our method, we conduct experiments on both a simulation dataset ({AI}2-{THOR})and a real-world environment with a physical robot. We further propose a new decaying reward function to learn the control policy specific to the object searching task. Experimental results validate the efficacy of our method, which outperforms competing methods in both average trajectory length and success rate.},
  eventtitle = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  pages      = {6857--6863},
  booktitle  = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
  author     = {Ye, Xin and Lin, Zhe and Li, Haoxiang and Zheng, Shibin and Yang, Yezhou},
  date       = {2018-10},
  note       = {{ISSN}: 2153-0866},
  keywords   = {Navigation, Neural networks, Object recognition, Robots, Search problems, Task analysis, Visualization},
  file       = {IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/YYQI6JBM/Ye et al. - 2018 - Active Object Perceiver Recognition-Guided Policy.pdf:application/pdf;IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/9IX4M2IH/8593720.html:text/html}
}

@article{ye_tsotsos_2001,
  title        = {A Complexity-Level Analysis of the Sensor Planning Task for Object Search},
  volume       = {17},
  issn         = {0824-7935, 1467-8640},
  doi          = {10.1111/0824-7935.00166},
  abstractnote = {Object search is the task of searching for a given 3D object in a given 3D environment by a controllable camera. Sensor planning for object search refers to the task of how to select the sensing parameters of the camera so as to bring the target into the ﬁeld of view of the camera and to make the image of the target to be easily recognized by the available recognition algorithms. In this paper, we study the task of sensor planning for object search from the theoretical point of view. We formulate the task and point out many of its important properties. We then analyze this task from the complexity level and prove that this task is NP-Complete.},
  note         = {13 citations (Crossref) [2022-02-28]},
  number       = {4},
  journal      = {Computational Intelligence},
  author       = {Ye, Yiming and Tsotsos, John K.},
  year         = {2001},
  month        = {Nov},
  pages        = {605–620},
  language     = {en}
}

@article{zeng_survey_2020,
  title        = {A Survey on Visual Navigation for Artificial Agents With Deep Reinforcement Learning},
  volume       = {8},
  issn         = {2169-3536},
  doi          = {10.1109/ACCESS.2020.3011438},
  abstractnote = {Visual navigation (vNavigation) is a key and fundamental technology for artificial agents’ interaction with the environment to achieve advanced behaviors. Visual navigation for artificial agents with deep reinforcement learning (DRL) is a new research hotspot in artificial intelligence and robotics that incorporates the decision making of DRL into visual navigation. Visual navigation via DRL, an end-to-end method, directly receives the high-dimensional images and generates an optimal navigation policy. In this paper, we first present an overview on reinforcement learning (RL), deep learning (DL) and deep reinforcement learning (DRL). Then, we systematically describe five main categories of visual DRL navigation: direct DRL vNavigation, hierarchical DRL vNavigation, multi-task DRL vNavigation, memory-inference DRL vNavigation and vision-language DRL vNavigation. These visual DRL navigation algorithms are reviewed in detail. Finally, we discuss the challenges and some possible opportunities to visual DRL navigation for artificial agents.},
  journal      = {IEEE Access},
  author       = {Zeng, Fanyu and Wang, Chen and Ge, Shuzhi Sam},
  year         = {2020},
  pages        = {135426–135442}
}

@article{zhang_overfitting_2018,
  title        = {A Study on Overfitting in Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1804.06893},
  abstractnote = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen “robustly”: commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  note         = {arXiv: 1804.06893},
  journal      = {arXiv:1804.06893 [cs, stat]},
  author       = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  year         = {2018},
  month        = {Apr}
}

@article{zhao_object_2019,
  title        = {Object Detection With Deep Learning: A Review},
  volume       = {30},
  issn         = {2162-237X, 2162-2388},
  doi          = {10.1109/TNNLS.2018.2876865},
  abstractnote = {Due to object detection’s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classiﬁers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modiﬁcations and useful tricks to improve detection performance further. As distinct speciﬁc detection tasks exhibit different characteristics, we also brieﬂy survey several speciﬁc tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
  number       = {11},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  author       = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
  year         = {2019},
  month        = {Nov},
  pages        = {3212–3232}
}

@article{zhu_target_2016,
  title        = {Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning},
  url          = {http://arxiv.org/abs/1609.05143},
  abstract     = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefﬁciency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the ﬁrst issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose {AI}2THOR framework, which provides an environment with highquality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efﬁciently.},
  journaltitle = {{arXiv}:1609.05143 [cs]},
  author       = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
  urldate      = {2022-03-14},
  date         = {2016-09-16},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {1609.05143},
  keywords     = {Computer Science - Computer Vision and Pattern Recognition},
  file         = {Zhu et al. - 2016 - Target-driven Visual Navigation in Indoor Scenes u.pdf:/home/oslund/Zotero/storage/H7ESWWTX/Zhu et al. - 2016 - Target-driven Visual Navigation in Indoor Scenes u.pdf:application/pdf}
}
