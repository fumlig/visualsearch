
@article{jaderberg_reinforcement_2016,
	title = {Reinforcement Learning with Unsupervised Auxiliary Tasks},
	url = {http://arxiv.org/abs/1611.05397},
	abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent signiﬁcantly outperforms the previous state-of-theart on Atari, averaging 880\% expert human performance, and a challenging suite of ﬁrst-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87\% expert human performance on Labyrinth.},
	journaltitle = {{arXiv}:1611.05397 [cs]},
	author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
	urldate = {2022-02-07},
	date = {2016-11-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.05397},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Jaderberg et al. - 2016 - Reinforcement Learning with Unsupervised Auxiliary.pdf:/home/oslund/Zotero/storage/GFZ92HUY/Jaderberg et al. - 2016 - Reinforcement Learning with Unsupervised Auxiliary.pdf:application/pdf},
}

@article{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	journaltitle = {{arXiv}:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2022-02-07},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1707.06347},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/oslund/Zotero/storage/EP5YVRBP/1707.html:text/html;arXiv Fulltext PDF:/home/oslund/Zotero/storage/VVHPSRFA/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@article{krishna_lakshmanan_complete_2020,
	title = {Complete coverage path planning using reinforcement learning for Tetromino based cleaning and maintenance robot},
	volume = {112},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580519305813},
	doi = {10/ghs394},
	pages = {103078},
	journaltitle = {Automation in Construction},
	shortjournal = {Automation in Construction},
	author = {Krishna Lakshmanan, Anirudh and Elara Mohan, Rajesh and Ramalingam, Balakrishnan and Vu Le, Anh and Veerajagadeshwar, Prabahar and Tiwari, Kamlesh and Ilyas, Muhammad},
	urldate = {2022-02-07},
	date = {2020-04},
	langid = {english},
	note = {38 citations (Crossref) [2022-02-07]},
	file = {Complete coverage path planning using reinforcement learning for Tetromino based cleaning and maintenance robot:/home/oslund/Zotero/storage/PN44T3XA/10.1016@j.autcon.2020.103078.pdf.pdf:application/pdf},
}

@article{de_bruin_integrating_2018,
	title = {Integrating State Representation Learning Into Deep Reinforcement Learning},
	volume = {3},
	issn = {2377-3766, 2377-3774},
	url = {http://ieeexplore.ieee.org/document/8276247/},
	doi = {10/gdnb2h},
	pages = {1394--1401},
	number = {3},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	shortjournal = {{IEEE} Robot. Autom. Lett.},
	author = {de Bruin, Tim and Kober, Jens and Tuyls, Karl and Babuska, Robert},
	urldate = {2022-02-07},
	date = {2018-07},
	file = {Full Text:/home/oslund/Zotero/storage/GXN8V6LY/de Bruin et al. - 2018 - Integrating State Representation Learning Into Dee.pdf:application/pdf;Integrating State Representation Learning Into Deep Reinforcement Learning:/home/oslund/Zotero/storage/3EHF5SFS/debruin2018.pdf.pdf:application/pdf},
}

@inproceedings{minut_reinforcement_2001,
	location = {Montreal, Quebec, Canada},
	title = {A reinforcement learning model of selective visual attention},
	isbn = {978-1-58113-326-4},
	url = {http://portal.acm.org/citation.cfm?doid=375735.376414},
	doi = {10/dbwckq},
	eventtitle = {the fifth international conference},
	pages = {457--464},
	booktitle = {Proceedings of the fifth international conference on Autonomous agents  - {AGENTS} '01},
	publisher = {{ACM} Press},
	author = {Minut, Silviu and Mahadevan, Sridhar},
	urldate = {2022-02-07},
	date = {2001},
	langid = {english},
	file = {A reinforcement learning model of selective visual attention:/home/oslund/Zotero/storage/YPN25DML/minut2001.pdf.pdf:application/pdf},
}

@article{galceran_survey_2013,
	title = {A survey on coverage path planning for robotics},
	volume = {61},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092188901300167X},
	doi = {10/f5j2n5},
	abstract = {Coverage Path Planning ({CPP}) is the task of determining a path that passes over all points of an area or volume of interest while avoiding obstacles. This task is integral to many robotic applications, such as vacuum cleaning robots, painter robots, autonomous underwater vehicles creating image mosaics, demining robots, lawn mowers, automated harvesters, window cleaners and inspection of complex structures, just to name a few. A considerable body of research has addressed the {CPP} problem. However, no updated surveys on {CPP} reflecting recent advances in the field have been presented in the past ten years. In this paper, we present a review of the most successful {CPP} methods, focusing on the achievements made in the past decade. Furthermore, we discuss reported field applications of the described {CPP} methods. This work aims to become a starting point for researchers who are initiating their endeavors in {CPP}. Likewise, this work aims to present a comprehensive review of the recent breakthroughs in the field, providing links to the most interesting and successful works.},
	pages = {1258--1276},
	number = {12},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Galceran, Enric and Carreras, Marc},
	urldate = {2022-02-07},
	date = {2013-12},
	langid = {english},
	file = {Submitted Version:/home/oslund/Zotero/storage/YXQEYAW9/Galceran and Carreras - 2013 - A survey on coverage path planning for robotics.pdf:application/pdf;Galceran and Carreras - 2013 - A survey on coverage path planning for robotics.pdf:/home/oslund/Zotero/storage/WQH7B7VZ/Galceran and Carreras - 2013 - A survey on coverage path planning for robotics.pdf:application/pdf;A survey on coverage path planning for robotics:/home/oslund/Zotero/storage/CHG6VELD/galceran2013.pdf.pdf:application/pdf},
}

@article{momennejad_successor_2017,
	title = {The successor representation in human reinforcement learning},
	volume = {1},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-017-0180-8},
	doi = {10.1038/s41562-017-0180-8},
	pages = {680--692},
	number = {9},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Momennejad, I. and Russek, E. M. and Cheong, J. H. and Botvinick, M. M. and Daw, N. D. and Gershman, S. J.},
	urldate = {2022-02-07},
	date = {2017-09},
	langid = {english},
	note = {143 citations (Crossref) [2022-02-07]},
	file = {Accepted Version:/home/oslund/Zotero/storage/IT9QIEMU/Momennejad et al. - 2017 - The successor representation in human reinforcemen.pdf:application/pdf;The successor representation in human reinforcement learning:/home/oslund/Zotero/storage/NJLP5ZR5/07dbbae27d0760f5ad53d58cc6137dbb.pdf.pdf:application/pdf},
}

@article{barreto_fast_2020,
	title = {Fast reinforcement learning with generalized policy updates},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1907370117},
	doi = {10.1073/pnas.1907370117},
	abstract = {The combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.},
	pages = {30079--30087},
	number = {48},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Barreto, André and Hou, Shaobo and Borsa, Diana and Silver, David and Precup, Doina},
	urldate = {2022-02-07},
	date = {2020-12-01},
	langid = {english},
	note = {6 citations (Crossref) [2022-02-07]},
	file = {Barreto et al. - 2020 - Fast reinforcement learning with generalized polic.pdf:/home/oslund/Zotero/storage/H3QHGS9P/Barreto et al. - 2020 - Fast reinforcement learning with generalized polic.pdf:application/pdf;Fast reinforcement learning with generalized policy updates:/home/oslund/Zotero/storage/G6CHD5RF/10.1073@pnas.1907370117.pdf.pdf:application/pdf},
}

@article{gupta_cognitive_nodate,
	title = {Cognitive Mapping and Planning for Visual Navigation},
	abstract = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from ﬁrst-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner ({CMP}) is based on two key ideas: a) a uniﬁed joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. {CMP} constructs a topdown belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that {CMP} outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that {CMP} can also achieve semantically speciﬁed goals, such as “go to a chair”.},
	pages = {10},
	author = {Gupta, Saurabh and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
	file = {Submitted Version:/home/oslund/Zotero/storage/UJ7VTNJN/Gupta et al. - 2020 - Cognitive Mapping and Planning for Visual Navigati.pdf:application/pdf;Gupta et al. - Cognitive Mapping and Planning for Visual Navigati.pdf:/home/oslund/Zotero/storage/WWJZVQ6F/Gupta et al. - Cognitive Mapping and Planning for Visual Navigati.pdf:application/pdf},
}

@article{aloimonos_active_1988,
	title = {Active vision},
	volume = {1},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/BF00133571},
	doi = {10/cn4mdc},
	pages = {333--356},
	number = {4},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vision},
	author = {Aloimonos, John and Weiss, Isaac and Bandyopadhyay, Amit},
	urldate = {2022-02-07},
	date = {1988-01},
	langid = {english},
	note = {705 citations (Crossref) [2022-02-07]},
	file = {Active vision:/home/oslund/Zotero/storage/SFZQUV5N/aloimonos1988.pdf.pdf:application/pdf},
}

@article{yang_reinforcement_2021,
	title = {Reinforcement Learning for General {LTL} Objectives Is Intractable},
	url = {http://arxiv.org/abs/2111.12679},
	abstract = {In recent years, researchers have made significant progress in devising reinforcement-learning algorithms for optimizing linear temporal logic ({LTL}) objectives and {LTL}-like objectives. Despite these advancements, there are fundamental limitations to how well this problem can be solved that previous studies have alluded to but, to our knowledge, have not examined in depth. In this paper, we address theoretically the hardness of learning with general {LTL} objectives. We formalize the problem under the probably approximately correct learning in Markov decision processes ({PAC}-{MDP}) framework, a standard framework for measuring sample complexity in reinforcement learning. In this formalization, we prove that the optimal policy for any {LTL} formula is {PAC}-{MDP}-learnable only if the formula is in the most limited class in the {LTL} hierarchy, consisting of only finite-horizon-decidable properties. Practically, our result implies that it is impossible for a reinforcement-learning algorithm to obtain a {PAC}-{MDP} guarantee on the performance of its learned policy after finitely many interactions with an unconstrained environment for non-finite-horizon-decidable {LTL} objectives.},
	journaltitle = {{arXiv}:2111.12679 [cs]},
	author = {Yang, Cambridge and Littman, Michael and Carbin, Michael},
	urldate = {2022-02-07},
	date = {2021-11-24},
	eprinttype = {arxiv},
	eprint = {2111.12679},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv.org Snapshot:/home/oslund/Zotero/storage/2YZ5HRXB/2111.html:text/html;arXiv Fulltext PDF:/home/oslund/Zotero/storage/RRBQFGLT/Yang et al. - 2021 - Reinforcement Learning for General LTL Objectives .pdf:application/pdf},
}

@article{zhu_target-driven_2016,
	title = {Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1609.05143},
	abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new target goals, and (2) data inefficiency i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to the task of target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows to better generalize. To address the second issue, we propose {AI}2-{THOR} framework, which provides an environment with high-quality 3D scenes and physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and across scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment. The supplementary video can be accessed at the following link: https://youtu.be/{SmBxMDiOrvs}.},
	journaltitle = {{arXiv}:1609.05143 [cs]},
	author = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
	urldate = {2022-02-03},
	date = {2016-09-16},
	eprinttype = {arxiv},
	eprint = {1609.05143},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/home/oslund/Zotero/storage/56GVT79F/1609.html:text/html;arXiv Fulltext PDF:/home/oslund/Zotero/storage/4UNKYK8Z/Zhu et al. - 2016 - Target-driven Visual Navigation in Indoor Scenes u.pdf:application/pdf},
}

@inproceedings{schmid_explore_2019,
	title = {Explore, Approach, and Terminate: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning},
	doi = {10.1109/IROS40897.2019.8967805},
	shorttitle = {Explore, Approach, and Terminate},
	abstract = {Searching for objects and distinguishing task-relevant objects from others is a key requirement for service robots. We propose a reinforcement learning solution to the active visual object search problem. Our method successfully learns to explore the environment, to approach the target object, and to decide when to terminate the search as the target object has been found. We demonstrate the efficiency of our solution on a dataset of real-world images collected by a robot. Our approach outperforms state-space planning or other baseline search strategies, reaching a higher success rate in a shorter time. We also study individual subtasks of active visual object search. Although strong baselines exist for the subtasks, our {RL} solution outperforms them in the overall search task.},
	eventtitle = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {5008--5013},
	booktitle = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	author = {Schmid, Jan Fabian and Lauri, Mikko and Frintrop, Simone},
	date = {2019-11},
	note = {{ISSN}: 2153-0866},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/P2DDVD72/8967805.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/BRVBXTET/Schmid et al. - 2019 - Explore, Approach, and Terminate Evaluating Subta.pdf:application/pdf;Explore, Approach, and Terminate\: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning:/home/oslund/Zotero/storage/JZDRJ7H5/schmid2019.pdf.pdf:application/pdf},
}

@inproceedings{schmid_explore_2019-1,
	location = {Macau, China},
	title = {Explore, Approach, and Terminate: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning},
	isbn = {978-1-72814-004-9},
	url = {https://ieeexplore.ieee.org/document/8967805/},
	doi = {10.1109/IROS40897.2019.8967805},
	shorttitle = {Explore, Approach, and Terminate},
	eventtitle = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {5008--5013},
	booktitle = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Schmid, Jan Fabian and Lauri, Mikko and Frintrop, Simone},
	urldate = {2022-02-03},
	date = {2019-11},
	file = {Explore, Approach, and Terminate\: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning:/home/oslund/Zotero/storage/WFJSHEBR/schmid2019.pdf.pdf:application/pdf},
}

@inproceedings{schmid_explore_2019-2,
	location = {Macau, China},
	title = {Explore, Approach, and Terminate: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning},
	isbn = {978-1-72814-004-9},
	url = {https://ieeexplore.ieee.org/document/8967805/},
	doi = {10.1109/IROS40897.2019.8967805},
	shorttitle = {Explore, Approach, and Terminate},
	eventtitle = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {5008--5013},
	booktitle = {2019 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Schmid, Jan Fabian and Lauri, Mikko and Frintrop, Simone},
	urldate = {2022-02-03},
	date = {2019-11},
	file = {Explore, Approach, and Terminate\: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning:/home/oslund/Zotero/storage/9W5AH49P/schmid2019.pdf.pdf:application/pdf},
}

@article{druon_visual_2020,
	title = {Visual Object Search by Learning Spatial Context},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/8963758/},
	doi = {10.1109/LRA.2020.2967677},
	pages = {1279--1286},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	shortjournal = {{IEEE} Robot. Autom. Lett.},
	author = {Druon, Raphael and Yoshiyasu, Yusuke and Kanezaki, Asako and Watt, Alassane},
	urldate = {2022-02-03},
	date = {2020-04},
	file = {Visual Object Search by Learning Spatial Context:/home/oslund/Zotero/storage/FI7QABUA/druon2020.pdf.pdf:application/pdf},
}

@article{fan_camouflaged_nodate,
	title = {Camouflaged Object Detection},
	pages = {11},
	author = {Fan, Deng-Ping and Ji, Ge-Peng and Sun, Guolei and Cheng, Ming-Ming and Shen, Jianbing and Shao, Ling},
	langid = {english},
	file = {Fan et al. - Camouflaged Object Detection.pdf:/home/oslund/Zotero/storage/ZYQSKSD3/Fan et al. - Camouflaged Object Detection.pdf:application/pdf},
}

@article{chen_active_2011,
	title = {Active vision in robotic systems: A survey of recent developments},
	volume = {30},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364911410755},
	doi = {10.1177/0278364911410755},
	shorttitle = {Active vision in robotic systems},
	abstract = {In this paper we provide a broad survey of developments in active vision in robotic applications over the last 15 years. With increasing demand for robotic automation, research in this area has received much attention. Among the many factors that can be attributed to a high-performance robotic system, the planned sensing or acquisition of perceptions on the operating environment is a crucial component. The aim of sensor planning is to determine the pose and settings of vision sensors for undertaking a vision-based task that usually requires obtaining multiple views of the object to be manipulated. Planning for robot vision is a complex problem for an active system due to its sensing uncertainty and environmental uncertainty. This paper describes such problems arising from many applications, e.g. object recognition and modeling, site reconstruction and inspection, surveillance, tracking and search, as well as robotic manipulation and assembly, localization and mapping, navigation and exploration. A bundle of solutions and methods have been proposed to solve these problems in the past. They are summarized in this review while enabling readers to easily refer solution methods for practical applications. Representative contributions, their evaluations, analyses, and future research trends are also addressed in an abstract level.},
	pages = {1343--1377},
	number = {11},
	journaltitle = {The International Journal of Robotics Research},
	shortjournal = {The International Journal of Robotics Research},
	author = {Chen, Shengyong and Li, Youfu and Kwok, Ngai Ming},
	urldate = {2022-02-03},
	date = {2011-09},
	langid = {english},
	file = {Active vision in robotic systems\: A survey of recent developments:/home/oslund/Zotero/storage/XEIAJSHN/chen2011.pdf.pdf:application/pdf;Chen et al. - 2011 - Active vision in robotic systems A survey of rece.pdf:/home/oslund/Zotero/storage/UL8A83YG/Chen et al. - 2011 - Active vision in robotic systems A survey of rece.pdf:application/pdf},
}

@inproceedings{wu_active_2020,
	location = {Seattle {WA} {USA}},
	title = {Active Object Search},
	isbn = {978-1-4503-7988-5},
	url = {https://dl.acm.org/doi/10.1145/3394171.3413571},
	doi = {10.1145/3394171.3413571},
	eventtitle = {{MM} '20: The 28th {ACM} International Conference on Multimedia},
	pages = {973--981},
	booktitle = {Proceedings of the 28th {ACM} International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Wu, Jie and Chen, Tianshui and Huang, Lishan and Wu, Hefeng and Li, Guanbin and Tian, Ling and Lin, Liang},
	urldate = {2022-02-03},
	date = {2020-10-12},
	langid = {english},
	file = {Active Object Search:/home/oslund/Zotero/storage/VED69K6A/wu2020.pdf.pdf:application/pdf},
}

@incollection{wu_active_2020-1,
	location = {New York, {NY}, {USA}},
	title = {Active Object Search},
	isbn = {978-1-4503-7988-5},
	url = {https://doi.org/10.1145/3394171.3413571},
	abstract = {In this work, we investigate an Active Object Search ({AOS}) task that is not explicitly addressed in the literature. It aims to actively perform as few action steps as possible to search and locate the target object in a 3D indoor scene. Different from classic object detection that passively receives visual information, this task encourages an intelligent agent to perform active search via reasonable action planning; thus it can better recall the target objects, especially for the challenging situations that the target is far from the agent, blocked by an obstacle and out of view. To handle this {AOS} task, we formulate a reinforcement learning framework that consists of a 3D object detector, a state controller and a cross-modal action planner to work cooperatively to find out the target object with minimal action steps. During training, we design a novel cost-sensitive active search reward that penalizes inaccurate object search and redundant action steps. To evaluate this novel task, we construct an Active Object Search ({AOS}) benchmark that contains 5,845 samples from 30 diverse indoor scenes. We conduct extensive qualitative and quantitative evaluations on this benchmark to demonstrate the effectiveness of the proposed approach and analyze the key factors that contribute more to address this task.},
	pages = {973--981},
	booktitle = {Proceedings of the 28th {ACM} International Conference on Multimedia},
	publisher = {Association for Computing Machinery},
	author = {Wu, Jie and Chen, Tianshui and Huang, Lishan and Wu, Hefeng and Li, Guanbin and Tian, Ling and Lin, Liang},
	urldate = {2022-02-03},
	date = {2020-10-12},
	keywords = {active object search, cost-sensitive active search reward, cross-modal action planner, reinforcement learning},
}

@article{wei_metaview_2021,
	title = {{MetaView}: Few-shot Active Object Recognition},
	url = {http://arxiv.org/abs/2103.04242},
	shorttitle = {{MetaView}},
	abstract = {In robot sensing scenarios, instead of passively utilizing human captured views, an agent should be able to actively choose informative viewpoints of a 3D object as discriminative evidence to boost the recognition accuracy. This task is referred to as active object recognition. Recent works on this task rely on a massive amount of training examples to train an optimal view selection policy. But in realistic robot sensing scenarios, the large-scale training data may not exist and whether the intelligent view selection policy can be still learned from few object samples remains unclear. In this paper, we study this new problem which is extremely challenging but very meaningful in robot sensing -- Few-shot Active Object Recognition, i.e., to learn view selection policies from few object samples, which has not been considered and addressed before. We solve the proposed problem by adopting the framework of meta learning and name our method "{MetaView}". Extensive experiments on both category-level and instance-level classification tasks demonstrate that the proposed method can efficiently resolve issues that are hard for state-of-the-art active object recognition methods to handle, and outperform several baselines by large margins.},
	journaltitle = {{arXiv}:2103.04242 [cs]},
	author = {Wei, Wei and Yu, Haonan and Zhang, Haichao and Xu, Wei and Wu, Ying},
	urldate = {2022-02-03},
	date = {2021-03-06},
	eprinttype = {arxiv},
	eprint = {2103.04242},
	keywords = {Computer Science - Robotics},
	file = {arXiv.org Snapshot:/home/oslund/Zotero/storage/98Y7BMAF/2103.html:text/html;arXiv Fulltext PDF:/home/oslund/Zotero/storage/RG6825NT/Wei et al. - 2021 - MetaView Few-shot Active Object Recognition.pdf:application/pdf},
}

@article{noauthor_pdf_nodate,
	title = {[{PDF}] Active object recognition {\textbar} Semantic Scholar},
	url = {https://www.semanticscholar.org/paper/Active-object-recognition-Wilkes-Tsotsos/3e597460999c4b38ac2f96690f6081a56a294f78},
	abstract = {The concept of active object recognition is introduced, and a proposal for its solution is described, which uses an efficient tree-based, probabilistic indexing scheme to find the model object that is likely to have generated the observed data. The concept of active object recognition is introduced, and a proposal for its solution is described. The camera is mounted on the end of a robot arm on a mobile base. The system exploits the mobility of the camera by using low-level image data to drive the camera to a standard viewpoint with respect to an unknown object. From such a viewpoint, the object recognition task is reduced to a two-dimensional pattern recognition problem. The system uses an efficient tree-based, probabilistic indexing scheme to find the model object that is likely to have generated the observed data, and for line tracking uses a modification of the token-based tracking scheme of J.L. Crowley et al. (1988). The system has been successfully tested on a set of origami objects. Given sufficiently accurate low-level data, recognition time is expected to grow only logarithmically with the number of objects stored.\&lt;\&lt;{ETX}\&gt;\&gt;},
	urldate = {2022-02-03},
	langid = {english},
}

@article{paletta_active_2000,
	title = {Active object recognition by view integration and reinforcement learning},
	volume = {31},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889099000792},
	doi = {10.1016/S0921-8890(99)00079-2},
	pages = {71--86},
	number = {1},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Paletta, Lucas and Pinz, Axel},
	urldate = {2022-02-03},
	date = {2000-04},
	langid = {english},
	file = {Active object recognition by view integration and reinforcement learning:/home/oslund/Zotero/storage/34XBNLE7/paletta2000.pdf.pdf:application/pdf},
}

@article{noauthor_active_nodate,
	title = {Active object recognition by view integration and reinforcement learning {\textbar} Semantic Scholar},
	url = {https://www.semanticscholar.org/paper/Active-object-recognition-by-view-integration-and-Paletta-Pinz/adddd055a5983993e6b83d6e6b8e01ecf09584c5},
	abstract = {Semantic Scholar extracted view of \&quot;Active object recognition by view integration and reinforcement learning\&quot; by L. Paletta et al.},
	urldate = {2022-02-03},
	langid = {english},
	file = {Snapshot:/home/oslund/Zotero/storage/UBBVBFX5/adddd055a5983993e6b83d6e6b8e01ecf09584c5.html:text/html},
}

@article{zhang_study_2018,
	title = {A Study on Overfitting in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1804.06893},
	abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning ({RL}). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging {RL} problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep {RL} techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard {RL} agents and find that they could overfit in various ways. Moreover, overfitting could happen “robustly”: commonly used techniques in {RL} that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in {RL}. We conclude with a general discussion on overfitting in {RL} and a study of the generalization behaviors from the perspective of inductive bias.},
	journaltitle = {{arXiv}:1804.06893 [cs, stat]},
	author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
	urldate = {2022-02-03},
	date = {2018-04-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.06893},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf:/home/oslund/Zotero/storage/85VRAUY5/Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf:application/pdf},
}

@article{li_survey_2019,
	title = {A Survey of Multi-View Representation Learning},
	volume = {31},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {http://arxiv.org/abs/1610.01206},
	doi = {10.1109/TKDE.2018.2872063},
	abstract = {Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we ﬁrst review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis ({CCA}) and its several extensions. Then from the perspective of representation fusion we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the ﬁeld of multi-view representation learning and to help researchers ﬁnd the most appropriate tools for particular applications.},
	pages = {1863--1883},
	number = {10},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Li, Yingming and Yang, Ming and Zhang, Zhongfei},
	urldate = {2022-02-03},
	date = {2019-10-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1610.01206},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
	file = {Li et al. - 2019 - A Survey of Multi-View Representation Learning.pdf:/home/oslund/Zotero/storage/LABUYUPB/Li et al. - 2019 - A Survey of Multi-View Representation Learning.pdf:application/pdf;A Survey of Multi-View Representation Learning:/home/oslund/Zotero/storage/SJIH9YRX/li2018.pdf.pdf:application/pdf},
}

@article{ghesu_multi-scale_2019,
	title = {Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in {CT} Scans},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8187667/},
	doi = {10.1109/TPAMI.2017.2782687},
	abstract = {Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem as a behavior learning task for an artiﬁcial agent. We couple the modeling of the anatomy appearance and the object search in a uniﬁed behavioral framework, using the capabilities of deep reinforcement learning and multi-scale image analysis. In other words, an artiﬁcial agent is trained not only to distinguish the target anatomical object from the rest of the body but also how to ﬁnd the object by learning and following an optimal navigation path to the target object in the imaged volumetric space. We evaluated our approach on 1487 3D-{CT} volumes from 532 patients, totaling over 500,000 image slices and show that it signiﬁcantly outperforms state-of-the-art solutions on detecting several anatomical structures with no failed cases from a clinical acceptance perspective, while also achieving a 20-30\% higher detection accuracy. Most importantly, we improve the detection-speed of the reference methods by 2-3 orders of magnitude, achieving unmatched real-time performance on large 3D-{CT} scans.},
	pages = {176--189},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Ghesu, Florin-Cristian and Georgescu, Bogdan and Zheng, Yefeng and Grbic, Sasa and Maier, Andreas and Hornegger, Joachim and Comaniciu, Dorin},
	urldate = {2022-02-03},
	date = {2019-01-01},
	langid = {english},
	file = {Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:/home/oslund/Zotero/storage/DX4BETIQ/Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:application/pdf;Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in CT Scans:/home/oslund/Zotero/storage/846WFHFX/ghesu2017.pdf.pdf:application/pdf},
}

@article{genders_evaluating_2018,
	title = {Evaluating reinforcement learning state representations for adaptive traffic signal control},
	volume = {130},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918303582},
	doi = {10.1016/j.procs.2018.04.008},
	pages = {26--33},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Genders, Wade and Razavi, Saiedeh},
	urldate = {2022-02-03},
	date = {2018},
	langid = {english},
	file = {Genders and Razavi - 2018 - Evaluating reinforcement learning state representa.pdf:/home/oslund/Zotero/storage/GC3AL93Y/Genders and Razavi - 2018 - Evaluating reinforcement learning state representa.pdf:application/pdf;Evaluating reinforcement learning state representations for adaptive traffic signal control:/home/oslund/Zotero/storage/2EFVUMAQ/genders2018.pdf.pdf:application/pdf},
}

@article{eriksson_deep_nodate,
	title = {Deep Reinforcement Learning Applied to an Image-Based Sensor Control Task},
	pages = {53},
	author = {Eriksson, Rickard},
	langid = {english},
	file = {Eriksson - Deep Reinforcement Learning Applied to an Image-Ba.pdf:/home/oslund/Zotero/storage/WR22EUEI/Eriksson - Deep Reinforcement Learning Applied to an Image-Ba.pdf:application/pdf},
}

@inproceedings{yang_predicting_2020,
	location = {Seattle, {WA}, {USA}},
	title = {Predicting Goal-Directed Human Attention Using Inverse Reinforcement Learning},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157666/},
	doi = {10.1109/CVPR42600.2020.00027},
	abstract = {Human gaze behavior prediction is important for behavioral vision and for computer vision applications. Most models mainly focus on predicting free-viewing behavior using saliency maps, but do not generalize to goal-directed behavior, such as when a person searches for a visual target object. We propose the ﬁrst inverse reinforcement learning ({IRL}) model to learn the internal reward function and policy used by humans during visual search. We modeled the viewer’s internal belief states as dynamic contextual belief maps of object locations. These maps were learned and then used to predict behavioral scanpaths for multiple target categories. To train and evaluate our {IRL} model we created {COCO}-Search18, which is now the largest dataset of highquality search ﬁxations in existence. {COCO}-Search18 has 10 participants searching for each of 18 target-object categories in 6202 images, making about 300,000 goal-directed ﬁxations. When trained and evaluated on {COCO}-Search18, the {IRL} model outperformed baseline models in predicting search ﬁxation scanpaths, both in terms of similarity to human search behavior and search efﬁciency. Finally, reward maps recovered by the {IRL} model reveal distinctive targetdependent patterns of object prioritization, which we interpret as a learned object context.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {190--199},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Yang, Zhibo and Huang, Lihan and Chen, Yupei and Wei, Zijun and Ahn, Seoyoung and Zelinsky, Gregory and Samaras, Dimitris and Hoai, Minh},
	urldate = {2022-02-03},
	date = {2020-06},
	langid = {english},
	file = {Yang et al. - 2020 - Predicting Goal-Directed Human Attention Using Inv.pdf:/home/oslund/Zotero/storage/J2HTXJ5P/Yang et al. - 2020 - Predicting Goal-Directed Human Attention Using Inv.pdf:application/pdf;Predicting Goal-Directed Human Attention Using Inverse Reinforcement Learning:/home/oslund/Zotero/storage/9DJYHWR6/10.1109@CVPR42600.2020.00027.pdf.pdf:application/pdf},
}

@inproceedings{pirinen_deep_2018,
	location = {Salt Lake City, {UT}, {USA}},
	title = {Deep Reinforcement Learning of Region Proposal Networks for Object Detection},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578824/},
	doi = {10.1109/CVPR.2018.00726},
	abstract = {We propose drl-{RPN}, a deep reinforcement learningbased visual recognition model consisting of a sequential region proposal network ({RPN}) and an object detector. In contrast to typical {RPNs}, where candidate object regions ({RoIs}) are selected greedily via class-agnostic {NMS}, {drlRPN} optimizes an objective closer to the ﬁnal detection task. This is achieved by replacing the greedy {RoI} selection process with a sequential attention mechanism which is trained via deep reinforcement learning ({RL}). Our model is capable of accumulating class-speciﬁc evidence over time, potentially affecting subsequent proposals and classiﬁcation scores, and we show that such context integration signiﬁcantly boosts detection accuracy. Moreover, drl-{RPN} automatically decides when to stop the search process and has the beneﬁt of being able to jointly learn the parameters of the policy and the detector, both represented as deep networks. Our model can further learn to search over a wide range of exploration-accuracy trade-offs making it possible to specify or adapt the exploration extent at test time. The resulting search trajectories are image- and categorydependent, yet rely only on a single policy over all object categories. Results on the {MS} {COCO} and {PASCAL} {VOC} challenges show that our approach outperforms established, typical state-of-the-art object detection pipelines.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {6945--6954},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Pirinen, Aleksis and Sminchisescu, Cristian},
	urldate = {2022-02-03},
	date = {2018-06},
	langid = {english},
	file = {Deep Reinforcement Learning of Region Proposal Networks for Object Detection:/home/oslund/Zotero/storage/BDTU2638/4d3c6dbb065728f679beb77b5e4cf299.pdf.pdf:application/pdf;Pirinen and Sminchisescu - 2018 - Deep Reinforcement Learning of Region Proposal Net.pdf:/home/oslund/Zotero/storage/2QZYBQKS/Pirinen and Sminchisescu - 2018 - Deep Reinforcement Learning of Region Proposal Net.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	location = {Cambridge, Massachusetts},
	edition = {Second edition},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	pagetotal = {526},
	publisher = {The {MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {2018},
	langid = {english},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/oslund/Zotero/storage/HPCN43YL/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{jie_tree-structured_2017,
	title = {Tree-Structured Reinforcement Learning for Sequential Object Localization},
	url = {http://arxiv.org/abs/1703.02710},
	abstract = {Existing object proposal algorithms usually search for possible object regions over multiple locations and scales separately, which ignore the interdependency among different objects and deviate from the human perception procedure. To incorporate global interdependency between objects into object localization, we propose an effective Tree-structured Reinforcement Learning (Tree-{RL}) approach to sequentially search for objects by fully exploiting both the current observation and historical search paths. The Tree-{RL} approach learns multiple searching policies through maximizing the long-term reward that reﬂects localization accuracies over all the objects. Starting with taking the entire image as a proposal, the Tree-{RL} approach allows the agent to sequentially discover multiple objects via a tree-structured traversing scheme. Allowing multiple near-optimal policies, Tree-{RL} offers more diversity in search paths and is able to ﬁnd multiple objects with a single feedforward pass. Therefore, Tree-{RL} can better cover different objects with various scales which is quite appealing in the context of object proposal. Experiments on {PASCAL} {VOC} 2007 and 2012 validate the effectiveness of the Tree-{RL}, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.},
	journaltitle = {{arXiv}:1703.02710 [cs]},
	author = {Jie, Zequn and Liang, Xiaodan and Feng, Jiashi and Jin, Xiaojie and Lu, Wen Feng and Yan, Shuicheng},
	urldate = {2022-02-03},
	date = {2017-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.02710},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Jie et al. - 2017 - Tree-Structured Reinforcement Learning for Sequent.pdf:/home/oslund/Zotero/storage/C9SZVG4L/Jie et al. - 2017 - Tree-Structured Reinforcement Learning for Sequent.pdf:application/pdf},
}

@incollection{ourselin_artificial_2016,
	location = {Cham},
	title = {An Artificial Agent for Anatomical Landmark Detection in Medical Images},
	volume = {9902},
	isbn = {978-3-319-46725-2 978-3-319-46726-9},
	url = {https://link.springer.com/10.1007/978-3-319-46726-9_27},
	abstract = {Fast and robust detection of anatomical structures or pathologies represents a fundamental task in medical image analysis. Most of the current solutions are however suboptimal and unconstrained by learning an appearance model and exhaustively scanning the space of parameters to detect a speciﬁc anatomical structure. In addition, typical feature computation or estimation of meta-parameters related to the appearance model or the search strategy, is based on local criteria or predeﬁned approximation schemes. We propose a new learning method following a fundamentally diﬀerent paradigm by simultaneously modeling both the object appearance and the parameter search strategy as a uniﬁed behavioral task for an artiﬁcial agent. The method combines the advantages of behavior learning achieved through reinforcement learning with eﬀective hierarchical feature extraction achieved through deep learning. We show that given only a sequence of annotated images, the agent can automatically and strategically learn optimal paths that converge to the sought anatomical landmark location as opposed to exhaustively scanning the entire solution space. The method signiﬁcantly outperforms state-ofthe-art machine learning and deep learning approaches both in terms of accuracy and speed on 2D magnetic resonance images, 2D ultrasound and 3D {CT} images, achieving average detection errors of 1-2 pixels, while also recognizing the absence of an object from the image.},
	pages = {229--237},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention - {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Ghesu, Florin C. and Georgescu, Bogdan and Mansi, Tommaso and Neumann, Dominik and Hornegger, Joachim and Comaniciu, Dorin},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	urldate = {2022-02-03},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-46726-9_27},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Ghesu et al. - 2016 - An Artificial Agent for Anatomical Landmark Detect.pdf:/home/oslund/Zotero/storage/SUFGAH83/Ghesu et al. - 2016 - An Artificial Agent for Anatomical Landmark Detect.pdf:application/pdf},
}

@article{ghesu_multi-scale_2019-1,
	title = {Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in {CT} Scans},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8187667/},
	doi = {10.1109/TPAMI.2017.2782687},
	abstract = {Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem as a behavior learning task for an artiﬁcial agent. We couple the modeling of the anatomy appearance and the object search in a uniﬁed behavioral framework, using the capabilities of deep reinforcement learning and multi-scale image analysis. In other words, an artiﬁcial agent is trained not only to distinguish the target anatomical object from the rest of the body but also how to ﬁnd the object by learning and following an optimal navigation path to the target object in the imaged volumetric space. We evaluate our approach on 1487 3D-{CT} volumes from 532 patients, totaling over 500,000 image slices and show that we signiﬁcantly outperform state-of-the-art solutions on detecting several anatomical structures with no failed cases from a clinical acceptance perspective, while also improving the detection accuracy by 20-30\%. Most importantly, we improve the detection-speed of the reference methods by 2-3 orders of magnitude, achieving unmatched real-time performance on large 3D-{CT} scans.},
	pages = {176--189},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Ghesu, Florin-Cristian and Georgescu, Bogdan and Zheng, Yefeng and Grbic, Sasa and Maier, Andreas and Hornegger, Joachim and Comaniciu, Dorin},
	urldate = {2022-02-03},
	date = {2019-01-01},
	langid = {english},
	file = {Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:/home/oslund/Zotero/storage/TF4ZNMC6/Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:application/pdf},
}

@article{caicedo_active_2015,
	title = {Active Object Localization with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1511.06015},
	abstract = {We present an active detection model for localizing objects in scenes. The model is class-speciﬁc and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most speciﬁc location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal {VOC} 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
	journaltitle = {{arXiv}:1511.06015 [cs]},
	author = {Caicedo, Juan C. and Lazebnik, Svetlana},
	urldate = {2022-02-03},
	date = {2015-11-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.06015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Caicedo and Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:/home/oslund/Zotero/storage/ENEDBIBQ/Caicedo and Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:application/pdf},
}

@inproceedings{pathak_context_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Context Encoders: Feature Learning by Inpainting},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780647/},
	doi = {10.1109/CVPR.2016.278},
	shorttitle = {Context Encoders},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2536--2544},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	urldate = {2022-02-03},
	date = {2016-06},
	file = {Submitted Version:/home/oslund/Zotero/storage/4TYQTPMH/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf},
}

@article{gupta_cognitive_2020,
	title = {Cognitive Mapping and Planning for Visual Navigation},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-019-01236-7},
	doi = {10.1007/s11263-019-01236-7},
	pages = {1311--1330},
	number = {5},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Gupta, Saurabh and Tolani, Varun and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
	urldate = {2022-02-03},
	date = {2020-05},
	langid = {english},
}

@article{luo_end--end_2020,
	title = {End-to-End Active Object Tracking and Its Real-World Deployment via Reinforcement Learning},
	volume = {42},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8642452/},
	doi = {10.1109/TPAMI.2019.2899570},
	pages = {1317--1332},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Luo, Wenhan and Sun, Peng and Zhong, Fangwei and Liu, Wei and Zhang, Tong and Wang, Yizhou},
	urldate = {2022-02-03},
	date = {2020-06-01},
	file = {Submitted Version:/home/oslund/Zotero/storage/E9CH3BWZ/Luo et al. - 2020 - End-to-End Active Object Tracking and Its Real-Wor.pdf:application/pdf},
}

@inproceedings{pathak_curiosity-driven_2017,
	location = {Honolulu, {HI}, {USA}},
	title = {Curiosity-Driven Exploration by Self-Supervised Prediction},
	isbn = {978-1-5386-0733-6},
	url = {http://ieeexplore.ieee.org/document/8014804/},
	doi = {10.1109/CVPRW.2017.70},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	pages = {488--489},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	publisher = {{IEEE}},
	author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
	urldate = {2022-02-03},
	date = {2017-07},
	file = {Submitted Version:/home/oslund/Zotero/storage/HYT2C7P6/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf:application/pdf},
}

@inproceedings{jayaraman_learning_2018,
	location = {Salt Lake City, {UT}},
	title = {Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578233/},
	doi = {10.1109/CVPR.2018.00135},
	shorttitle = {Learning to Look Around},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {1238--1247},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Jayaraman, Dinesh and Grauman, Kristen},
	urldate = {2022-02-03},
	date = {2018-06},
	file = {Submitted Version:/home/oslund/Zotero/storage/9N596VR3/Jayaraman and Grauman - 2018 - Learning to Look Around Intelligently Exploring U.pdf:application/pdf},
}

@article{galceran_survey_2013-1,
	title = {A survey on coverage path planning for robotics},
	volume = {61},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092188901300167X},
	doi = {10.1016/j.robot.2013.09.004},
	pages = {1258--1276},
	number = {12},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Galceran, Enric and Carreras, Marc},
	urldate = {2022-02-03},
	date = {2013-12},
	langid = {english},
}

@inproceedings{ahmed_understanding_2019,
	title = {Understanding the Impact of Entropy on Policy Optimization},
	url = {https://proceedings.mlr.press/v97/ahmed19a.html},
	abstract = {Entropy regularization is commonly used to improve policy optimization in reinforcement learning. It is believed to help with exploration by encouraging the selection of more stochastic policies. In this work, we analyze this claim using new visualizations of the optimization landscape based on randomly perturbing the loss function. We first show that even with access to the exact gradient, policy optimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a policy with higher entropy can make the optimization landscape smoother, thereby connecting local optima and enabling the use of larger learning rates. This paper presents new tools for understanding the optimization landscape, shows that policy entropy serves as a regularizer, and highlights the challenge of designing general-purpose policy optimization algorithms.},
	eventtitle = {International Conference on Machine Learning},
	pages = {151--160},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Ahmed, Zafarali and Roux, Nicolas Le and Norouzi, Mohammad and Schuurmans, Dale},
	urldate = {2022-02-08},
	date = {2019-05-24},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Supplementary PDF:/home/oslund/Zotero/storage/WXQLD46C/Ahmed et al. - 2019 - Understanding the Impact of Entropy on Policy Opti.pdf:application/pdf;Full Text PDF:/home/oslund/Zotero/storage/YQK2FPLN/Ahmed et al. - 2019 - Understanding the Impact of Entropy on Policy Opti.pdf:application/pdf},
}

@article{schmidhuber_deep_2015,
	title = {Deep Learning in Neural Networks: An Overview},
	volume = {61},
	issn = {08936080},
	url = {http://arxiv.org/abs/1404.7828},
	doi = {10/f6v78n},
	shorttitle = {Deep Learning in Neural Networks},
	abstract = {In recent years, deep artiﬁcial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	pages = {85--117},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Schmidhuber, Juergen},
	urldate = {2022-02-09},
	date = {2015-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1404.7828},
	note = {8182 citations (Crossref) [2022-02-10]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:/home/oslund/Zotero/storage/456XFDZE/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf;Deep Learning in Neural Networks\: An Overview:/home/oslund/Zotero/storage/5FCARIRD/schmidhuber2015.pdf.pdf:application/pdf},
}

@article{koutras_marsexplorer_2021,
	title = {{MarsExplorer}: Exploration of Unknown Terrains via Deep Reinforcement Learning and Procedurally Generated Environments},
	url = {http://arxiv.org/abs/2107.09996},
	shorttitle = {{MarsExplorer}},
	abstract = {This paper is an initial endeavor to bridge the gap between powerful Deep Reinforcement Learning methodologies and the problem of exploration/coverage of unknown terrains. Within this scope, {MarsExplorer}, an openai-gym compatible environment tailored to exploration/coverage of unknown areas, is presented. {MarsExplorer} translates the original robotics problem into a Reinforcement Learning setup that various off-the-shelf algorithms can tackle. Any learned policy can be straightforwardly applied to a robotic platform without an elaborate simulation model of the robot’s dynamics to apply a different learning/adaptation phase. One of its core features is the controllable multi-dimensional procedural generation of terrains, which is the key for producing policies with strong generalization capabilities. Four different state-of-the-art {RL} algorithms (A3C, {PPO}, Rainbow, and {SAC}) are trained on the {MarsExplorer} environment, and a proper evaluation of their results compared to the average human-level performance is reported. In the follow-up experimental analysis, the effect of the multi-dimensional difﬁculty setting on the learning capabilities of the best-performing algorithm ({PPO}) is analyzed. A milestone result is the generation of an exploration policy that follows the Hilbert curve without providing this information to the environment or rewarding directly or indirectly Hilbert-curve-like trajectories. The experimental analysis is concluded by evaluating {PPO} learned policy algorithm side-by-side with frontier-based exploration strategies. A study on the performance curves revealed that {PPO}-based policy was capable of performing adaptive-to-the-unknown-terrain sweeping without leaving expensive-to-revisit areas uncovered, underlying the capability of {RL}-based methodologies to tackle exploration tasks efﬁciently. The source code can be found at: https://github.com/dimikout3/{MarsExplorer}.},
	journaltitle = {{arXiv}:2107.09996 [cs]},
	author = {Koutras, Dimitrios I. and Kapoutsis, Athanasios Ch and Amanatiadis, Angelos A. and Kosmatopoulos, Elias B.},
	urldate = {2022-02-10},
	date = {2021-11-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2107.09996},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Koutras et al. - 2021 - MarsExplorer Exploration of Unknown Terrains via .pdf:/home/oslund/Zotero/storage/WFW6EJ9T/Koutras et al. - 2021 - MarsExplorer Exploration of Unknown Terrains via .pdf:application/pdf},
}

@article{dayan_feudal_nodate,
	title = {Feudal Reinforcement Learning},
	abstract = {One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-Iearning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Sub-managers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command.},
	pages = {8},
	author = {Dayan, Peter and Hinton, Geoffrey E},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Dayan and Hinton - Feudal Reinforcement Learning.pdf:/home/oslund/Zotero/storage/LGU4S852/Dayan and Hinton - Feudal Reinforcement Learning.pdf:application/pdf},
}

@article{kirk_survey_2022,
	title = {A Survey of Generalisation in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/2111.09794},
	abstract = {The study of generalisation in deep Reinforcement Learning ({RL}) aims to produce {RL} algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overﬁtting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent ﬁeld. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the ﬁeld, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling {RL}-speciﬁc problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as ofﬂine {RL} generalisation and reward-function variation.},
	journaltitle = {{arXiv}:2111.09794 [cs]},
	author = {Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rocktäschel, Tim},
	urldate = {2022-02-10},
	date = {2022-01-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2111.09794},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Kirk et al. - 2022 - A Survey of Generalisation in Deep Reinforcement L.pdf:/home/oslund/Zotero/storage/752XCYLG/Kirk et al. - 2022 - A Survey of Generalisation in Deep Reinforcement L.pdf:application/pdf},
}

@article{bubeck_universal_2021,
	title = {A Universal Law of Robustness via Isoperimetry},
	url = {http://arxiv.org/abs/2105.12806},
	abstract = {Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisfied. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires \$d\$ times more parameters than mere interpolation, where \$d\$ is the ambient data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate distribution verifying isoperimetry. In the case of two-layers neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions.},
	journaltitle = {{arXiv}:2105.12806 [cs, stat]},
	author = {Bubeck, Sébastien and Sellke, Mark},
	urldate = {2022-02-10},
	date = {2021-10-21},
	eprinttype = {arxiv},
	eprint = {2105.12806},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/oslund/Zotero/storage/SRSQK6UW/2105.html:text/html;arXiv Fulltext PDF:/home/oslund/Zotero/storage/X3V2U75R/Bubeck and Sellke - 2021 - A Universal Law of Robustness via Isoperimetry.pdf:application/pdf},
}

@article{pardo_time_2022,
	title = {Time Limits in Reinforcement Learning},
	url = {http://arxiv.org/abs/1712.00378},
	abstract = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent's input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
	journaltitle = {{arXiv}:1712.00378 [cs]},
	author = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
	urldate = {2022-02-10},
	date = {2022-01-27},
	eprinttype = {arxiv},
	eprint = {1712.00378},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/oslund/Zotero/storage/AXFFBTPU/1712.html:text/html;arXiv Fulltext PDF:/home/oslund/Zotero/storage/K3P2R4RQ/Pardo et al. - 2022 - Time Limits in Reinforcement Learning.pdf:application/pdf},
}

@online{noauthor_rapportsammanfattning_nodate,
	title = {Rapportsammanfattning - Intranätet},
	url = {https://intranet.foi.se/ovrigt/rapportsammanfattning.html},
	type = {text},
	urldate = {2022-02-11},
	langid = {swedish},
	file = {Snapshot:/home/oslund/Zotero/storage/ITXPH6I5/rapportsammanfattning.html:text/html},
}