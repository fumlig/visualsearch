
@inproceedings{minut_reinforcement_2001,
	location = {Montreal, Quebec, Canada},
	title = {A Reinforcement Learning Model of Selective Visual Attention},
	isbn = {978-1-58113-326-4},
	url = {http://portal.acm.org/citation.cfm?doid=375735.376414},
	doi = {10/dbwckq},
	pages = {457--464},
	booktitle = {Proceedings of the Fifth International Conference on Autonomous Agents  - {AGENTS} '01},
	publisher = {{ACM} Press},
	author = {Minut, Silviu and Mahadevan, Sridhar},
	urldate = {2022-02-07},
	date = {2001},
	langid = {english},
}ate

@article{galceran_survey_2013,
	title = {A Survey on Coverage Path Planning for Robotics},
	volume = {61},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092188901300167X},
	doi = {10/f5j2n5},
	abstract = {Coverage Path Planning ({CPP}) is the task of determining a path that passes over all points of an area or volume of interest while avoiding obstacles. This task is integral to many robotic applications, such as vacuum cleaning robots, painter robots, autonomous underwater vehicles creating image mosaics, demining robots, lawn mowers, automated harvesters, window cleaners and inspection of complex structures, just to name a few. A considerable body of research has addressed the {CPP} problem. However, no updated surveys on {CPP} reflecting recent advances in the field have been presented in the past ten years. In this paper, we present a review of the most successful {CPP} methods, focusing on the achievements made in the past decade. Furthermore, we discuss reported field applications of the described {CPP} methods. This work aims to become a starting point for researchers who are initiating their endeavors in {CPP}. Likewise, this work aims to present a comprehensive review of the recent breakthroughs in the field, providing links to the most interesting and successful works.},
	pages = {1258--1276},
	number = {12},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Galceran, Enric and Carreras, Marc},
	urldate = {2022-02-07},
	date = {2013-12},
	langid = {english},
	file = {A survey on coverage path planning for robotics:/home/oslund/Zotero/storage/CHG6VELD/galceran2013.pdf.pdf:application/pdf;Galceran and Carreras - 2013 - A survey on coverage path planning for robotics.pdf:/home/oslund/Zotero/storage/WQH7B7VZ/Galceran and Carreras - 2013 - A survey on coverage path planning for robotics.pdf:application/pdf;Submitted Version:/home/oslund/Zotero/storage/YXQEYAW9/Galceran and Carreras - 2013 - A survey on coverage path planning for robotics.pdf:application/pdf},
}

@article{aloimonos_active_1988,
	title = {Active Vision},
	volume = {1},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/BF00133571},
	doi = {10/cn4mdc},
	pages = {333--356},
	number = {4},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vision},
	author = {Aloimonos, John and Weiss, Isaac and Bandyopadhyay, Amit},
	urldate = {2022-02-07},
	date = {1988-01},
	langid = {english},
	note = {705 citations (Crossref) [2022-02-07]},
	file = {Active vision:/home/oslund/Zotero/storage/SFZQUV5N/aloimonos1988.pdf.pdf:application/pdf},
}

@article{zhang_study_2018,
	title = {A Study on Overfitting in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1804.06893},
	abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning ({RL}). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging {RL} problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep {RL} techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard {RL} agents and find that they could overfit in various ways. Moreover, overfitting could happen “robustly”: commonly used techniques in {RL} that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in {RL}. We conclude with a general discussion on overfitting in {RL} and a study of the generalization behaviors from the perspective of inductive bias.},
	journaltitle = {{arXiv}:1804.06893 [cs, stat]},
	author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
	urldate = {2022-02-03},
	date = {2018-04-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.06893},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf:/home/oslund/Zotero/storage/85VRAUY5/Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	location = {Cambridge, Massachusetts},
	edition = {Second edition},
	title = {Reinforcement Learning: An Introduction},
	isbn = {978-0-262-03924-6},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Reinforcement Learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	pagetotal = {526},
	publisher = {The {MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {2018},
	langid = {english},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/oslund/Zotero/storage/HPCN43YL/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@incollection{ourselin_artificial_2016,
	location = {Cham},
	title = {An Artificial Agent for Anatomical Landmark Detection in Medical Images},
	volume = {9902},
	isbn = {978-3-319-46726-9},
	url = {https://link.springer.com/10.1007/978-3-319-46726-9_27},
	abstract = {Fast and robust detection of anatomical structures or pathologies represents a fundamental task in medical image analysis. Most of the current solutions are however suboptimal and unconstrained by learning an appearance model and exhaustively scanning the space of parameters to detect a speciﬁc anatomical structure. In addition, typical feature computation or estimation of meta-parameters related to the appearance model or the search strategy, is based on local criteria or predeﬁned approximation schemes. We propose a new learning method following a fundamentally diﬀerent paradigm by simultaneously modeling both the object appearance and the parameter search strategy as a uniﬁed behavioral task for an artiﬁcial agent. The method combines the advantages of behavior learning achieved through reinforcement learning with eﬀective hierarchical feature extraction achieved through deep learning. We show that given only a sequence of annotated images, the agent can automatically and strategically learn optimal paths that converge to the sought anatomical landmark location as opposed to exhaustively scanning the entire solution space. The method signiﬁcantly outperforms state-ofthe-art machine learning and deep learning approaches both in terms of accuracy and speed on 2D magnetic resonance images, 2D ultrasound and 3D {CT} images, achieving average detection errors of 1-2 pixels, while also recognizing the absence of an object from the image.},
	pages = {229--237},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention - {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Ghesu, Florin C. and Georgescu, Bogdan and Mansi, Tommaso and Neumann, Dominik and Hornegger, Joachim and Comaniciu, Dorin},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	urldate = {2022-02-03},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-46726-9_27},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Ghesu et al. - 2016 - An Artificial Agent for Anatomical Landmark Detect.pdf:/home/oslund/Zotero/storage/SUFGAH83/Ghesu et al. - 2016 - An Artificial Agent for Anatomical Landmark Detect.pdf:application/pdf},
}

@article{ghesu_multi-scale_2019,
	title = {Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in {CT} Scans},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8187667/},
	doi = {10.1109/TPAMI.2017.2782687},
	abstract = {Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem as a behavior learning task for an artiﬁcial agent. We couple the modeling of the anatomy appearance and the object search in a uniﬁed behavioral framework, using the capabilities of deep reinforcement learning and multi-scale image analysis. In other words, an artiﬁcial agent is trained not only to distinguish the target anatomical object from the rest of the body but also how to ﬁnd the object by learning and following an optimal navigation path to the target object in the imaged volumetric space. We evaluate our approach on 1487 3D-{CT} volumes from 532 patients, totaling over 500,000 image slices and show that we signiﬁcantly outperform state-of-the-art solutions on detecting several anatomical structures with no failed cases from a clinical acceptance perspective, while also improving the detection accuracy by 20-30\%. Most importantly, we improve the detection-speed of the reference methods by 2-3 orders of magnitude, achieving unmatched real-time performance on large 3D-{CT} scans.},
	pages = {176--189},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Ghesu, Florin-Cristian and Georgescu, Bogdan and Zheng, Yefeng and Grbic, Sasa and Maier, Andreas and Hornegger, Joachim and Comaniciu, Dorin},
	urldate = {2022-02-03},
	date = {2019-01-01},
	langid = {english},
	file = {Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:/home/oslund/Zotero/storage/TF4ZNMC6/Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:application/pdf},
}

@article{caicedo_active_2015,
	title = {Active Object Localization with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1511.06015},
	abstract = {We present an active detection model for localizing objects in scenes. The model is class-speciﬁc and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most speciﬁc location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal {VOC} 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.},
	journaltitle = {{arXiv}:1511.06015 [cs]},
	author = {Caicedo, Juan C. and Lazebnik, Svetlana},
	urldate = {2022-02-03},
	date = {2015-11-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.06015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Caicedo and Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:/home/oslund/Zotero/storage/ENEDBIBQ/Caicedo and Lazebnik - 2015 - Active Object Localization with Deep Reinforcement.pdf:application/pdf},
}

@article{eckstein_visual_2011,
	title = {Visual Search: A Retrospective},
	volume = {11},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/11.5.14},
	doi = {10.1167/11.5.14},
	shorttitle = {Visual Search},
	pages = {14--14},
	number = {5},
	journaltitle = {Journal of Vision},
	shortjournal = {Journal of Vision},
	author = {Eckstein, M. P.},
	urldate = {2022-02-22},
	date = {2011-12-30},
	langid = {english},
	note = {207 citations (Crossref) [2022-02-28]},
	file = {Eckstein - 2011 - Visual search A retrospective.pdf:/home/oslund/Zotero/storage/E87E8PCH/Eckstein - 2011 - Visual search A retrospective.pdf:application/pdf;Visual search\: A retrospective:/home/oslund/Zotero/storage/PX9QE8ZC/eckstein2011.pdf.pdf:application/pdf},
}

@article{ye_complexity-level_2001,
	title = {A Complexity-Level Analysis of the Sensor Planning Task for Object Search},
	volume = {17},
	issn = {0824-7935, 1467-8640},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/0824-7935.00166},
	doi = {10.1111/0824-7935.00166},
	abstract = {Object search is the task of searching for a given 3D object in a given 3D environment by a controllable camera. Sensor planning for object search refers to the task of how to select the sensing parameters of the camera so as to bring the target into the ﬁeld of view of the camera and to make the image of the target to be easily recognized by the available recognition algorithms. In this paper, we study the task of sensor planning for object search from the theoretical point of view. We formulate the task and point out many of its important properties. We then analyze this task from the complexity level and prove that this task is {NP}-Complete.},
	pages = {605--620},
	number = {4},
	journaltitle = {Computational Intelligence},
	shortjournal = {Computational Intell},
	author = {Ye, Yiming and Tsotsos, John K.},
	urldate = {2022-02-28},
	date = {2001-11},
	langid = {english},
	note = {13 citations (Crossref) [2022-02-28]},
	file = {A Complexity-Level Analysis of the Sensor Planning Task for Object Search:/home/oslund/Zotero/storage/NHIY39GT/ye2001.pdf.pdf:application/pdf;Ye and Tsotsos - 2001 - A Complexity-Level Analysis of the Sensor Planning.pdf:/home/oslund/Zotero/storage/N43M2BI5/Ye and Tsotsos - 2001 - A Complexity-Level Analysis of the Sensor Planning.pdf:application/pdf},
}

@inproceedings{aydemir_search_2011,
	location = {Shanghai, China},
	title = {Search in the Real World: Active Visual Object Search Based on Spatial Relations},
	isbn = {978-1-61284-386-5},
	url = {http://ieeexplore.ieee.org/document/5980495/},
	doi = {10.1109/ICRA.2011.5980495},
	shorttitle = {Search in the Real World},
	abstract = {Objects are integral to a robot’s understanding of space. Various tasks such as semantic mapping, pick-andcarry missions or manipulation involve interaction with objects. Previous work in the ﬁeld largely builds on the assumption that the object in question starts out within the ready sensory reach of the robot. In this work we aim to relax this assumption by providing the means to perform robust and large-scale active visual object search. Presenting spatial relations that describe topological relationships between objects, we then show how to use these to create potential search actions. We introduce a method for efﬁciently selecting search strategies given probabilities for those relations. Finally we perform experiments to verify the feasibility of our approach.},
	eventtitle = {2011 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {2818--2824},
	booktitle = {2011 {IEEE} International Conference on Robotics and Automation},
	publisher = {{IEEE}},
	author = {Aydemir, A. and Sjoo, K. and Folkesson, J. and Pronobis, A. and Jensfelt, P.},
	urldate = {2022-02-28},
	date = {2011-05},
	langid = {english},
	note = {48 citations (Crossref) [2022-02-28]},
	file = {Aydemir et al. - 2011 - Search in the real world Active visual object sea.pdf:/home/oslund/Zotero/storage/G93XW8VC/Aydemir et al. - 2011 - Search in the real world Active visual object sea.pdf:application/pdf;Search in the real world\: Active visual object search based on spatial relations:/home/oslund/Zotero/storage/I33UQM4R/aydemir2011.pdf.pdf:application/pdf},
}

@article{aydemir_active_2013,
	title = {Active Visual Object Search in Unknown Environments Using Uncertain Semantics},
	volume = {29},
	issn = {1941-0468},
	doi = {10.1109/TRO.2013.2256686},
	abstract = {In this paper, we study the problem of active visual search ({AVS}) in large, unknown, or partially known environments. We argue that by making use of uncertain semantics of the environment, a robot tasked with finding an object can devise efficient search strategies that can locate everyday objects at the scale of an entire building floor, which is previously unknown to the robot. To realize this, we present a probabilistic model of the search environment, which allows for prioritizing the search effort to those parts of the environment that are most promising for a specific object type. Further, we describe a method for reasoning about the unexplored part of the environment for goal-directed exploration with the purpose of object search. We demonstrate the validity of our approach by comparing it with two other search systems in terms of search trajectory length and time. First, we implement a greedy coverage-based search strategy that is found in previous work. Second, we let human participants search for objects as an alternative comparison for our method. Our results show that {AVS} strategies that exploit uncertain semantics of the environment are a very promising idea, and our method pushes the state-of-the-art forward in {AVS}.},
	pages = {986--1002},
	number = {4},
	journaltitle = {{IEEE} Transactions on Robotics},
	author = {Aydemir, Alper and Pronobis, Andrzej and Göbelbecker, Moritz and Jensfelt, Patric},
	date = {2013-08},
	note = {65 citations (Crossref) [2022-02-28]
Conference Name: {IEEE} Transactions on Robotics},
	keywords = {Task analysis, Visualization, Active vision, Autonomous systems, Robot sensing systems, Search problems, semantic mapping, Semantics, visual object search},
	file = {Active Visual Object Search in Unknown Environments Using Uncertain Semantics:/home/oslund/Zotero/storage/F35NFVFE/aydemir2013.pdf.pdf:application/pdf;IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/LBZD2554/6507635.html:text/html},
}

@article{bajcsy_active_1988,
	title = {Active Perception},
	volume = {76},
	issn = {1558-2256},
	doi = {10.1109/5.5968},
	abstract = {Active perception (active vision specifically) is defined as a study of modeling and control strategies for perception. Local methods are distinguished from global models by their extent of application in space and time. The local models represent procedures and parameters such as optical distortions of the lens, focal lens, spatial resolution, bandpass filter, etc, The global models, on the other hand, characterize the overall performance and make predictions on how the individual modules interact. The control strategies are formulated as a search of such sequences of steps that would minimize a loss function while still seeking the most information. Examples are shown as the existence proof of the proposed theory on obtaining range from focus and stereo/vergence on 2-D segmentation of an image and 3-D shape parameterization.{\textless}{\textgreater}},
	pages = {966--1005},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Bajcsy, R.},
	date = {1988-08},
	note = {646 citations (Crossref) [2022-02-28]
Conference Name: Proceedings of the {IEEE}},
	keywords = {Lenses, Band pass filters, Focusing, Image segmentation, Optical distortion, Optical losses, Predictive models, Shape, Spatial resolution},
	file = {Active perception:/home/oslund/Zotero/storage/6BV9NPKM/bajcsy1988.pdf.pdf:application/pdf;IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/FCL5ZG3R/5968.html:text/html},
}

@article{wolfe_five_2017,
	title = {Five Factors That Guide Attention in Visual Search},
	volume = {1},
	rights = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-017-0058},
	doi = {10.1038/s41562-017-0058},
	abstract = {How do we find what we are looking for? Even when the desired target is in the current field of view, we need to search because fundamental limits on visual processing make it impossible to recognize everything at once. Searching involves directing attention to objects that might be the target. This deployment of attention is not random. It is guided to the most promising items and locations by five factors discussed here: bottom-up salience, top-down feature guidance, scene structure and meaning, the previous history of search over timescales ranging from milliseconds to years, and the relative value of the targets and distractors. Modern theories of visual search need to incorporate all five factors and specify how these factors combine to shape search behaviour. An understanding of the rules of guidance can be used to improve the accuracy and efficiency of socially important search tasks, from security screening to medical image perception.},
	pages = {1--8},
	number = {3},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {Wolfe, Jeremy M. and Horowitz, Todd S.},
	urldate = {2022-02-28},
	date = {2017-03-08},
	langid = {english},
	note = {300 citations (Crossref) [2022-02-28]
Number: 3
Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Visual system},
	file = {Five factors that guide attention in visual search:/home/oslund/Zotero/storage/WZCJAX2U/wolfe2017.pdf.pdf:application/pdf;Full Text PDF:/home/oslund/Zotero/storage/8M4AIW8W/Wolfe and Horowitz - 2017 - Five factors that guide attention in visual search.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/VF9C7HD9/s41562-017-0058.html:text/html},
}

@article{wolfe_guided_2021,
	title = {Guided Search 6.0: An Updated Model of Visual Search},
	volume = {28},
	issn = {1531-5320},
	doi = {10.3758/s13423-020-01859-9},
	shorttitle = {Guided Search 6.0},
	abstract = {This paper describes Guided Search 6.0 ({GS}6), a revised model of visual search. When we encounter a scene, we can see something everywhere. However, we cannot recognize more than a few items at a time. Attention is used to select items so that their features can be "bound" into recognizable objects. Attention is "guided" so that items can be processed in an intelligent order. In {GS}6, this guidance comes from five sources of preattentive information: (1) top-down and (2) bottom-up feature guidance, (3) prior history (e.g., priming), (4) reward, and (5) scene syntax and semantics. These sources are combined into a spatial "priority map," a dynamic attentional landscape that evolves over the course of search. Selective attention is guided to the most active location in the priority map approximately 20 times per second. Guidance will not be uniform across the visual field. It will favor items near the point of fixation. Three types of functional visual field ({FVFs}) describe the nature of these foveal biases. There is a resolution {FVF}, an {FVF} governing exploratory eye movements, and an {FVF} governing covert deployments of attention. To be identified as targets or rejected as distractors, items must be compared to target templates held in memory. The binding and recognition of an attended object is modeled as a diffusion process taking {\textgreater} 150 ms/item. Since selection occurs more frequently than that, it follows that multiple items are undergoing recognition at the same time, though asynchronously, making {GS}6 a hybrid of serial and parallel processes. In {GS}6, if a target is not found, search terminates when an accumulating quitting signal reaches a threshold. Setting of that threshold is adaptive, allowing feedback about performance to shape subsequent searches. Simulation shows that the combination of asynchronous diffusion and a quitting signal can produce the basic patterns of response time and error data from a range of search experiments.},
	pages = {1060--1092},
	number = {4},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Wolfe, Jeremy M.},
	date = {2021-08},
	pmid = {33547630},
	note = {29 citations (Crossref) [2022-03-02]},
	keywords = {Humans, Attention, Eye Movements, Reaction Time, Visual Perception, Bottom-up, Errors, Guided search, Pattern Recognition, Visual, Reaction time, Selective attention, Top-down, Visual Fields, Visual search, Visual working memory},
	file = {Full Text:/home/oslund/Zotero/storage/WBASNPNB/Wolfe - 2021 - Guided Search 6.0 An updated model of visual sear.pdf:application/pdf},
}

@article{wolfe_visual_2010,
	title = {Visual Search},
	volume = {20},
	url = {https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC5678963/},
	doi = {10.1016/j.cub.2010.02.016},
	pages = {R346},
	number = {8},
	journaltitle = {Current biology : {CB}},
	author = {Wolfe, Jeremy M.},
	urldate = {2022-03-02},
	date = {2010-04-27},
	langid = {english},
	pmid = {21749949},
	note = {64 citations (Crossref) [2022-03-02]
Publisher: {NIH} Public Access},
	file = {Full Text:/home/oslund/Zotero/storage/CLUR5J4F/Wolfe - 2010 - Visual search.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/F564KWBK/PMC5678963.html:text/html;Visual search:/home/oslund/Zotero/storage/N56NHYTS/wolfe2010.pdf.pdf:application/pdf},
}

@article{brockman_openai_2016,
	title = {{OpenAI} Gym},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {{OpenAI} Gym1 is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of {OpenAI} Gym and the design decisions that went into the software.},
	journaltitle = {{arXiv}:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	urldate = {2022-03-10},
	date = {2016-06-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.01540},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Brockman et al. - 2016 - OpenAI Gym.pdf:/home/oslund/Zotero/storage/6DWT7K4D/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf},
}

@article{bajcsy_revisiting_2018,
	title = {Revisiting Active Perception},
	volume = {42},
	issn = {1573-7527},
	url = {https://doi.org/10.1007/s10514-017-9615-3},
	doi = {10.1007/s10514-017-9615-3},
	abstract = {Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.},
	pages = {177--196},
	number = {2},
	journaltitle = {Autonomous Robots},
	shortjournal = {Auton Robot},
	author = {Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K.},
	urldate = {2022-03-13},
	date = {2018-02-01},
	langid = {english},
	note = {86 citations (Crossref) [2022-03-13]},
	file = {Springer Full Text PDF:/home/oslund/Zotero/storage/KVGHH5A6/Bajcsy et al. - 2018 - Revisiting active perception.pdf:application/pdf},
}

@book{russell_artificial_2021,
	location = {Hoboken, {NJ}},
	edition = {Fourth Edition},
	title = {Artificial Intelligence: A Modern Approach},
	isbn = {978-0-13-461099-3},
	series = {Pearson Series in Artificial Intelligence},
	shorttitle = {Artificial Intelligence},
	abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
	pagetotal = {1115},
	publisher = {Pearson},
	author = {Russell, Stuart J. and Norvig, Peter},
	editora = {Chang, Ming-wei and Devlin, Jacob and Dragan, Anca and Forsyth, David and Goodfellow, Ian and Malik, Jitendra and Mansinghka, Vikash and Pearl, Judea and Woolridge, Michael},
	editoratype = {collaborator},
	date = {2021},
}

@inproceedings{ye_active_2018,
	title = {Active Object Perceiver: Recognition-Guided Policy Learning for Object Searching on Mobile Robots},
	doi = {10.1109/IROS.2018.8593720},
	shorttitle = {Active Object Perceiver},
	abstract = {We study the problem of learning a navigation policy for a robot to actively search for an object of interest in an indoor environment solely from its visual inputs. While scene-driven visual navigation has been widely studied, prior efforts on learning navigation policies for robots to find objects are limited. The problem is often more challenging than target scene finding as the target objects can be very small in the view and can be in an arbitrary pose. We approach the problem from an active perceiver perspective, and propose a novel framework that integrates a deep neural network based object recognition module and a deep reinforcement learning based action prediction mechanism. To validate our method, we conduct experiments on both a simulation dataset ({AI}2-{THOR})and a real-world environment with a physical robot. We further propose a new decaying reward function to learn the control policy specific to the object searching task. Experimental results validate the efficacy of our method, which outperforms competing methods in both average trajectory length and success rate.},
	eventtitle = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {6857--6863},
	booktitle = {2018 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	author = {Ye, Xin and Lin, Zhe and Li, Haoxiang and Zheng, Shibin and Yang, Yezhou},
	date = {2018-10},
	note = {20 citations (Crossref) [2022-03-18]
{ISSN}: 2153-0866},
	keywords = {Robots, Task analysis, Object recognition, Visualization, Search problems, Navigation, Neural networks},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/9IX4M2IH/8593720.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/YYQI6JBM/Ye et al. - 2018 - Active Object Perceiver Recognition-Guided Policy.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	location = {Cambridge, {MA}, {USA}},
	title = {Deep Learning},
	isbn = {978-0-262-03561-3},
	series = {Adaptive Computation and Machine Learning series},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.},
	pagetotal = {800},
	publisher = {{MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	editorb = {Bach, Francis},
	editorbtype = {redactor},
	date = {2016-11-18},
	langid = {english},
	file = {Open Access:/home/oslund/Zotero/storage/7NWDTAYM/www.deeplearningbook.org.html:text/html},
}

@article{yang_visual_2018,
	title = {Visual Semantic Navigation using Scene Priors},
	url = {http://arxiv.org/abs/1810.06543},
	abstract = {How do humans navigate to target objects in novel scenes? Do we use the semantic/functional priors we have built over years to efficiently search and navigate? For example, to search for mugs, we search cabinets near the coffee machine and for fruits we try the fridge. In this work, we focus on incorporating semantic priors in the task of semantic navigation. We propose to use Graph Convolutional Networks for incorporating the prior knowledge into a deep reinforcement learning framework. The agent uses the features from the knowledge graph to predict the actions. For evaluation, we use the {AI}2-{THOR} framework. Our experiments show how semantic knowledge improves performance significantly. More importantly, we show improvement in generalization to unseen scenes and/or objects. The supplementary video can be accessed at the following link: https://youtu.be/{otKjuO}805dE .},
	journaltitle = {{arXiv}:1810.06543 [cs]},
	author = {Yang, Wei and Wang, Xiaolong and Farhadi, Ali and Gupta, Abhinav and Mottaghi, Roozbeh},
	urldate = {2022-03-18},
	date = {2018-10-15},
	eprinttype = {arxiv},
	eprint = {1810.06543},
	keywords = {⛔ No {DOI} found, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/C9U6SN2U/Yang et al. - 2018 - Visual Semantic Navigation using Scene Priors.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/IJA7G38X/1810.html:text/html},
}

@article{gupta_cognitive_2019,
	title = {Cognitive Mapping and Planning for Visual Navigation},
	url = {http://arxiv.org/abs/1702.03920},
	abstract = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner ({CMP}) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. {CMP} constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test {CMP} on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that {CMP} outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as 'going to a chair'. We also deploy {CMP} on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.},
	journaltitle = {{arXiv}:1702.03920 [cs]},
	author = {Gupta, Saurabh and Tolani, Varun and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
	urldate = {2022-03-21},
	date = {2019-02-07},
	eprinttype = {arxiv},
	eprint = {1702.03920},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/SMIATS3U/Gupta et al. - 2019 - Cognitive Mapping and Planning for Visual Navigati.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/IMPMR6XT/1702.html:text/html},
}

@article{hausknecht_deep_2017,
	title = {Deep Recurrent Q-Learning for Partially Observable {MDPs}},
	url = {http://arxiv.org/abs/1507.06527},
	abstract = {Deep Reinforcement Learning has yielded proﬁcient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network ({DQN}) by replacing the ﬁrst post-convolutional fully-connected layer with a recurrent {LSTM}. The resulting Deep Recurrent Q-Network ({DRQN}), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates {DQN}’s performance on standard Atari games and partially observed equivalents featuring ﬂickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, {DRQN}’s performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, {DRQN}’s performance degrades less than {DQN}’s. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the {DQN}’s input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
	journaltitle = {{arXiv}:1507.06527 [cs]},
	author = {Hausknecht, Matthew and Stone, Peter},
	urldate = {2022-03-21},
	date = {2017-01-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1507.06527},
	keywords = {Computer Science - Machine Learning},
	file = {Hausknecht and Stone - 2017 - Deep Recurrent Q-Learning for Partially Observable.pdf:/home/oslund/Zotero/storage/TTA3Q2JA/Hausknecht and Stone - 2017 - Deep Recurrent Q-Learning for Partially Observable.pdf:application/pdf},
}

@article{mnih_playing_2013,
	title = {Playing Atari with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	journaltitle = {{arXiv}:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	urldate = {2022-03-21},
	date = {2013-12-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1312.5602},
	keywords = {Computer Science - Machine Learning},
	file = {Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:/home/oslund/Zotero/storage/XXP45926/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous Methods for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core {CPU} instead of a {GPU}. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	journaltitle = {{arXiv}:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	urldate = {2022-03-22},
	date = {2016-06-16},
	eprinttype = {arxiv},
	eprint = {1602.01783},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/NTQHXKYQ/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/ZWDTNVWS/1602.html:text/html},
}

@article{mnih_human-level_2015,
	title = {Human-level Control Through Deep Reinforcement Learning},
	volume = {518},
	rights = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	pages = {529--533},
	number = {7540},
	journaltitle = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	urldate = {2022-03-22},
	date = {2015-02},
	langid = {english},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	keywords = {Computer science},
	file = {Full Text PDF:/home/oslund/Zotero/storage/3CC9PQS2/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/H3XQZBEI/nature14236.html:text/html},
}

@article{kaelbling_planning_1998,
	title = {Planning and Acting in Partially Observable Stochastic Domains},
	volume = {101},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X},
	doi = {10.1016/S0004-3702(98)00023-X},
	abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes ({MDPs}) and partially observable {MDPs} ({POMDPs}). We then outline a novel algorithm for solving {POMDPs} off line and show how, in some cases, a ﬁnite-memory controller can be extracted from the solution to a {POMDP}. We conclude with a discussion of how our approach relates to previous work, the complexity of ﬁnding exact solutions to {POMDPs}, and of some possibilities for ﬁnding approximate solutions. © 1998 Elsevier Science B.V. All rights reserved.},
	pages = {99--134},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	urldate = {2022-03-23},
	date = {1998-05},
	langid = {english},
	file = {Kaelbling et al. - 1998 - Planning and acting in partially observable stocha.pdf:/home/oslund/Zotero/storage/DNR6ZS9N/Kaelbling et al. - 1998 - Planning and acting in partially observable stocha.pdf:application/pdf},
}

@inproceedings{henriques_mapnet_2018,
	location = {Salt Lake City, {UT}},
	title = {{MapNet}: An Allocentric Spatial Memory for Mapping Environments},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578982/},
	doi = {10.1109/CVPR.2018.00884},
	shorttitle = {{MapNet}},
	abstract = {Autonomous agents need to reason about the world beyond their instantaneous sensory input. Integrating information over time, however, requires switching from an egocentric representation of a scene to an allocentric one, expressed in the world reference frame. It must also be possible to update the representation dynamically, which requires localizing and registering the sensor with respect to the world reference. In this paper, we develop a differentiable module that satisﬁes such requirements, while being robust, efﬁcient, and suitable for integration in end-to-end deep networks. The module contains an allocentric spatial memory that can be accessed associatively by feeding to it the current sensory input, resulting in localization, and then updated using an {LSTM} or similar mechanism. We formulate efﬁcient localization and registration of sensory information as a dual pair of convolution/deconvolution operators in memory space. The map itself is a 2.5D representation of an environment storing information that a deep neural network module learns to distill from {RGBD} input. The result is a map that contains multi-task information, different from classical approaches to mapping such as structurefrom-motion. We present results using synthetic mazes, a dataset of hours of recorded gameplay of the classic game Doom, and the very recent Active Vision Dataset of real images captured from a robot.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {8476--8484},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Henriques, Joao F. and Vedaldi, Andrea},
	urldate = {2022-03-23},
	date = {2018-06},
	langid = {english},
	file = {Henriques and Vedaldi - 2018 - MapNet An Allocentric Spatial Memory for Mapping .pdf:/home/oslund/Zotero/storage/6G8P6FHC/Henriques and Vedaldi - 2018 - MapNet An Allocentric Spatial Memory for Mapping .pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long Short-Term Memory},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	date = {1997-11},
	note = {Conference Name: Neural Computation},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/WGGTFJ68/6795963.html:text/html},
}

@article{henderson_deep_2018,
	title = {Deep Reinforcement Learning That Matters},
	volume = {32},
	rights = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11694},
	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning ({RL}). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep {RL} methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep {RL} more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	urldate = {2022-03-23},
	date = {2018-04-29},
	langid = {english},
	note = {Number: 1},
	keywords = {Machine Learning},
	file = {Full Text PDF:/home/oslund/Zotero/storage/5QWPU4KE/Henderson et al. - 2018 - Deep Reinforcement Learning That Matters.pdf:application/pdf},
}

@article{agarwal_deep_2022,
	title = {Deep Reinforcement Learning at the Edge of the Statistical Precipice},
	url = {http://arxiv.org/abs/2108.13264},
	abstract = {Deep reinforcement learning ({RL}) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep {RL} benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a ﬁnite number of training runs. Beginning with the Arcade Learning Environment ({ALE}), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few-run deep {RL} regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the ﬁeld. We illustrate this point using a case study on the Atari 100k benchmark, where we ﬁnd substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the ﬁeld’s conﬁdence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance proﬁles to account for the variability in results, as well as present more robust and efﬁcient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used {RL} benchmarks including the {ALE}, Procgen, and the {DeepMind} Control Suite, again revealing discrepancies in prior comparisons. Our ﬁndings call for a change in how we evaluate performance in deep {RL}, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable2, to prevent unreliable results from stagnating the ﬁeld.},
	journaltitle = {{arXiv}:2108.13264 [cs, stat]},
	author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron and Bellemare, Marc G.},
	urldate = {2022-03-23},
	date = {2022-01-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2108.13264},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Statistics - Methodology},
	file = {Agarwal et al. - 2022 - Deep Reinforcement Learning at the Edge of the Sta.pdf:/home/oslund/Zotero/storage/ASFYV4LD/Agarwal et al. - 2022 - Deep Reinforcement Learning at the Edge of the Sta.pdf:application/pdf},
}

@article{anderson_evaluation_2018,
	title = {On Evaluation of Embodied Navigation Agents},
	url = {http://arxiv.org/abs/1807.06757},
	abstract = {Skillful mobile operation in three-dimensional environments is a primary topic of study in Artiﬁcial Intelligence. The past two years have seen a surge of creative work on navigation. This creative output has produced a plethora of sometimes incompatible task deﬁnitions and evaluation protocols. To coordinate ongoing and future research in this area, we have convened a working group to study empirical methodology in navigation research. The present document summarizes the consensus recommendations of this working group. We discuss different problem statements and the role of generalization, present evaluation measures, and provide standard scenarios that can be used for benchmarking.},
	journaltitle = {{arXiv}:1807.06757 [cs]},
	author = {Anderson, Peter and Chang, Angel and Chaplot, Devendra Singh and Dosovitskiy, Alexey and Gupta, Saurabh and Koltun, Vladlen and Kosecka, Jana and Malik, Jitendra and Mottaghi, Roozbeh and Savva, Manolis and Zamir, Amir R.},
	urldate = {2022-03-23},
	date = {2018-07-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.06757},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Anderson et al. - 2018 - On Evaluation of Embodied Navigation Agents.pdf:/home/oslund/Zotero/storage/5URQWMZY/Anderson et al. - 2018 - On Evaluation of Embodied Navigation Agents.pdf:application/pdf},
}

@article{batra_objectnav_2020,
	title = {{ObjectNav} Revisited: On Evaluation of Embodied Agents Navigating to Objects},
	url = {http://arxiv.org/abs/2006.13171},
	shorttitle = {{ObjectNav} Revisited},
	abstract = {We revisit the problem of Object-Goal Navigation ({ObjectNav}). In its simplest form, {ObjectNav} is deﬁned as the task of navigating to an object, speciﬁed by its label, in an unexplored environment. In particular, the agent is initialized at a random location and pose in an environment and asked to ﬁnd an instance of an object category, e.g. ‘ﬁnd a chair’, by navigating to it.},
	journaltitle = {{arXiv}:2006.13171 [cs]},
	author = {Batra, Dhruv and Gokaslan, Aaron and Kembhavi, Aniruddha and Maksymets, Oleksandr and Mottaghi, Roozbeh and Savva, Manolis and Toshev, Alexander and Wijmans, Erik},
	urldate = {2022-03-23},
	date = {2020-08-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.13171},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Batra et al. - 2020 - ObjectNav Revisited On Evaluation of Embodied Age.pdf:/home/oslund/Zotero/storage/PTEKLYB5/Batra et al. - 2020 - ObjectNav Revisited On Evaluation of Embodied Age.pdf:application/pdf},
}

@article{cobbe_leveraging_2020,
	title = {Leveraging Procedural Generation to Benchmark Reinforcement Learning},
	url = {http://arxiv.org/abs/1912.01588},
	abstract = {We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efﬁciency and generalization in reinforcement learning. We believe that the community will beneﬁt from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate {RL} agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, ﬁnding that larger models signiﬁcantly improve both sample efﬁciency and generalization.},
	journaltitle = {{arXiv}:1912.01588 [cs, stat]},
	author = {Cobbe, Karl and Hesse, Christopher and Hilton, Jacob and Schulman, John},
	urldate = {2022-03-23},
	date = {2020-07-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1912.01588},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Cobbe et al. - 2020 - Leveraging Procedural Generation to Benchmark Rein.pdf:/home/oslund/Zotero/storage/LYWGQ5C4/Cobbe et al. - 2020 - Leveraging Procedural Generation to Benchmark Rein.pdf:application/pdf},
}

@article{cobbe_quantifying_2019,
	title = {Quantifying Generalization in Reinforcement Learning},
	url = {http://arxiv.org/abs/1812.02341},
	abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in {RL}, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called {CoinRun}, designed as a benchmark for generalization in {RL}. Using {CoinRun}, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
	journaltitle = {{arXiv}:1812.02341 [cs, stat]},
	author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
	urldate = {2022-03-24},
	date = {2019-07-14},
	eprinttype = {arxiv},
	eprint = {1812.02341},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/BBNEI6HZ/Cobbe et al. - 2019 - Quantifying Generalization in Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/VPVTNL43/1812.html:text/html},
}

@article{hessel_inductive_2019,
	title = {On Inductive Biases in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1907.02908},
	abstract = {Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep reinforcement learning agents. We investigated whether the performance deteriorates when these components are replaced with adaptive solutions from the literature. In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system. As hypothesized, the system with adaptive components performed better on many of the new tasks.},
	journaltitle = {{arXiv}:1907.02908 [cs, stat]},
	author = {Hessel, Matteo and van Hasselt, Hado and Modayil, Joseph and Silver, David},
	urldate = {2022-03-24},
	date = {2019-07-05},
	eprinttype = {arxiv},
	eprint = {1907.02908},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/AJ4E27FY/Hessel et al. - 2019 - On Inductive Biases in Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/C4UGLTSJ/1907.html:text/html},
}

@article{mirowski_learning_2017,
	title = {Learning to Navigate in Complex Environments},
	url = {http://arxiv.org/abs/1611.03673},
	abstract = {Learning to navigate in complex environments with dynamic elements is an important milestone in developing {AI} agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.},
	journaltitle = {{arXiv}:1611.03673 [cs]},
	author = {Mirowski, Piotr and Pascanu, Razvan and Viola, Fabio and Soyer, Hubert and Ballard, Andrew J. and Banino, Andrea and Denil, Misha and Goroshin, Ross and Sifre, Laurent and Kavukcuoglu, Koray and Kumaran, Dharshan and Hadsell, Raia},
	urldate = {2022-03-24},
	date = {2017-01-13},
	eprinttype = {arxiv},
	eprint = {1611.03673},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/42FWD3ZH/Mirowski et al. - 2017 - Learning to Navigate in Complex Environments.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/3VZZ2HSM/1611.html:text/html},
}

@article{dhiman_critical_2019,
	title = {A Critical Investigation of Deep Reinforcement Learning for Navigation},
	url = {http://arxiv.org/abs/1802.02274},
	abstract = {The navigation problem is classically approached in two steps: an exploration step, where map-information about the environment is gathered; and an exploitation step, where this information is used to navigate efficiently. Deep reinforcement learning ({DRL}) algorithms, alternatively, approach the problem of navigation in an end-to-end fashion. Inspired by the classical approach, we ask whether {DRL} algorithms are able to inherently explore, gather and exploit map-information over the course of navigation. We build upon Mirowski et al. [2017] work and introduce a systematic suite of experiments that vary three parameters: the agent's starting location, the agent's target location, and the maze structure. We choose evaluation metrics that explicitly measure the algorithm's ability to gather and exploit map-information. Our experiments show that when trained and tested on the same maps, the algorithm successfully gathers and exploits map-information. However, when trained and tested on different sets of maps, the algorithm fails to transfer the ability to gather and exploit map-information to unseen maps. Furthermore, we find that when the goal location is randomized and the map is kept static, the algorithm is able to gather and exploit map-information but the exploitation is far from optimal. We open-source our experimental suite in the hopes that it serves as a framework for the comparison of future algorithms and leads to the discovery of robust alternatives to classical navigation methods.},
	journaltitle = {{arXiv}:1802.02274 [cs]},
	author = {Dhiman, Vikas and Banerjee, Shurjo and Griffin, Brent and Siskind, Jeffrey M. and Corso, Jason J.},
	urldate = {2022-03-24},
	date = {2019-01-04},
	eprinttype = {arxiv},
	eprint = {1802.02274},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/A5KIEP6W/Dhiman et al. - 2019 - A Critical Investigation of Deep Reinforcement Lea.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/KTIV9X5V/1802.html:text/html},
}

@article{chaplot_object_2020,
	title = {Object Goal Navigation using Goal-Oriented Semantic Exploration},
	url = {http://arxiv.org/abs/2007.00643},
	abstract = {This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the {CVPR}-2020 Habitat {ObjectNav} Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.},
	journaltitle = {{arXiv}:2007.00643 [cs]},
	author = {Chaplot, Devendra Singh and Gandhi, Dhiraj and Gupta, Abhinav and Salakhutdinov, Ruslan},
	urldate = {2022-03-24},
	date = {2020-07-01},
	eprinttype = {arxiv},
	eprint = {2007.00643},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/AMKMIBUP/Chaplot et al. - 2020 - Object Goal Navigation using Goal-Oriented Semanti.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/5DUEHPGV/2007.html:text/html},
}

@article{kirk_survey_2022,
	title = {A Survey of Generalisation in Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/2111.09794},
	abstract = {The study of generalisation in deep Reinforcement Learning ({RL}) aims to produce {RL} algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling {RL}-specific problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as offline {RL} generalisation and reward-function variation.},
	journaltitle = {{arXiv}:2111.09794 [cs]},
	author = {Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rocktäschel, Tim},
	urldate = {2022-03-24},
	date = {2022-01-30},
	eprinttype = {arxiv},
	eprint = {2111.09794},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/AQJQLDV3/Kirk et al. - 2022 - A Survey of Generalisation in Deep Reinforcement L.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/6Y3USDBA/2111.html:text/html},
}

@article{minsky_steps_1961,
	title = {Steps toward Artificial Intelligence},
	volume = {49},
	issn = {2162-6634},
	doi = {10.1109/JRPROC.1961.287775},
	abstract = {The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.},
	pages = {8--30},
	number = {1},
	year = {1961},
	journaltitle = {Proceedings of the {IRE}},
	author = {Minsky, Marvin},
	date = {1961-01},
	note = {Conference Name: Proceedings of the {IRE}},
	keywords = {Artificial intelligence, Application software, Environmental management, High performance computing, Information processing, Libraries, Military computing, Planets, Problem-solving, Technological innovation},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/NTII3D55/4066245.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/XGC68GAS/Minsky - 1961 - Steps toward Artificial Intelligence.pdf:application/pdf},
}

@article{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	journaltitle = {{arXiv}:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2022-03-28},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1707.06347},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/4RGRB5JN/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/KIGY27GM/1707.html:text/html},
}

@article{parisotto_neural_2017,
	title = {Neural Map: Structured Memory for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1702.08360},
	shorttitle = {Neural Map},
	abstract = {A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning ({DRL}) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an {LSTM} layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that {DRL} agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous {DRL} memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.},
	journaltitle = {{arXiv}:1702.08360 [cs]},
	author = {Parisotto, Emilio and Salakhutdinov, Ruslan},
	urldate = {2022-03-29},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1702.08360},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/YCBM423I/Parisotto and Salakhutdinov - 2017 - Neural Map Structured Memory for Deep Reinforceme.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/S224VHST/1702.html:text/html},
}

@article{bahdanau_neural_2016,
	title = {Neural Machine Translation by Jointly Learning to Align and Translate},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	journaltitle = {{arXiv}:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	urldate = {2022-03-30},
	date = {2016-05-19},
	eprinttype = {arxiv},
	eprint = {1409.0473},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/BJVCR2P3/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/PVS646Y8/1409.html:text/html},
}

@article{zeng_survey_2020,
	title = {A Survey on Visual Navigation for Artificial Agents With Deep Reinforcement Learning},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3011438},
	abstract = {Visual navigation ({vNavigation}) is a key and fundamental technology for artificial agents' interaction with the environment to achieve advanced behaviors. Visual navigation for artificial agents with deep reinforcement learning ({DRL}) is a new research hotspot in artificial intelligence and robotics that incorporates the decision making of {DRL} into visual navigation. Visual navigation via {DRL}, an end-to-end method, directly receives the high-dimensional images and generates an optimal navigation policy. In this paper, we first present an overview on reinforcement learning ({RL}), deep learning ({DL}) and deep reinforcement learning ({DRL}). Then, we systematically describe five main categories of visual {DRL} navigation: direct {DRL} {vNavigation}, hierarchical {DRL} {vNavigation}, multi-task {DRL} {vNavigation}, memory-inference {DRL} {vNavigation} and vision-language {DRL} {vNavigation}. These visual {DRL} navigation algorithms are reviewed in detail. Finally, we discuss the challenges and some possible opportunities to visual {DRL} navigation for artificial agents.},
	pages = {135426--135442},
	journaltitle = {{IEEE} Access},
	author = {Zeng, Fanyu and Wang, Chen and Ge, Shuzhi Sam},
	date = {2020},
	note = {Conference Name: {IEEE} Access},
	keywords = {Learning (artificial intelligence), Task analysis, Visualization, deep reinforcement learning, Navigation, Machine learning, Simultaneous localization and mapping, artificial agents, Survey, visual navigation},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/H57GGRFQ/9146614.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/L75ZEB8G/Zeng et al. - 2020 - A Survey on Visual Navigation for Artificial Agent.pdf:application/pdf},
}

@inproceedings{chen_spatial_2017,
	location = {Venice},
	title = {Spatial Memory for Context Reasoning in Object Detection},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237702/},
	doi = {10.1109/ICCV.2017.440},
	abstract = {Modeling instance-level context and object-object relationships is extremely challenging. It requires reasoning about bounding boxes of different classes, locations etc. Above all, instance-level spatial reasoning inherently requires modeling conditional distributions on previous detections. Unfortunately, our current object detection systems do not have any memory to remember what to condition on! The state-of-the-art object detectors still detect all object in parallel followed by non-maximal suppression ({NMS}). While memory has been used for tasks such as captioning, they mostly use image-level memory cells without capturing the spatial layout. On the other hand, modeling object-object relationships requires spatial reasoning –not only do we need a memory to store the spatial layout, but also a effective reasoning module to extract spatial patterns. This paper presents a conceptually simple yet powerful solution – Spatial Memory Network ({SMN}), to model the instance-level context efﬁciently and effectively. Our spatial memory essentially assembles object instances back into a pseudo “image” representation that is easy to be fed into another {ConvNet} for object-object context reasoning. This leads to a new sequential reasoning architecture where image and memory are processed in parallel to obtain detections which update the memory again. We show our {SMN} direction is promising as it provides 2.2\% improvement over baseline Faster {RCNN} on the {COCO} dataset with {VGG}161.},
	eventtitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {4106--4116},
	booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Chen, Xinlei and Gupta, Abhinav},
	urldate = {2022-03-30},
	date = {2017-10},
	langid = {english},
	file = {Chen and Gupta - 2017 - Spatial Memory for Context Reasoning in Object Det.pdf:/home/oslund/Zotero/storage/WXST9FKL/Chen and Gupta - 2017 - Spatial Memory for Context Reasoning in Object Det.pdf:application/pdf},
}

@incollection{mataric_reward_1994,
	location = {San Francisco ({CA})},
	title = {Reward Functions for Accelerated Learning},
	isbn = {978-1-55860-335-6},
	url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500301},
	abstract = {This paper discusses why traditional reinforcement learning methods, and algorithms applied to those models, result in poor performance in situated domains characterized by multiple goals, noisy state, and inconsistent reinforcement. We propose a methodology for designing reinforcement functions that take advantage of implicit domain knowledge in order to accelerate learning in such domains. The methodology involves the use of heterogeneous reinforcement functions and progress estimators, and applies to learning in domains with a single agent or with multiple agents. The methodology is experimentally validated on a group of mobile robots learning a foraging task.},
	pages = {181--189},
	booktitle = {Machine Learning Proceedings 1994},
	publisher = {Morgan Kaufmann},
	author = {Mataric, Maja J},
	editor = {Cohen, William W. and Hirsh, Haym},
	urldate = {2022-03-31},
	date = {1994-01-01},
	langid = {english},
	doi = {10.1016/B978-1-55860-335-6.50030-1},
	file = {ScienceDirect Snapshot:/home/oslund/Zotero/storage/MSPT8Y2F/B9781558603356500301.html:text/html},
}

@article{savinov_semi-parametric_2018,
	title = {Semi-parametric Topological Memory for Navigation},
	url = {http://arxiv.org/abs/1803.00653},
	abstract = {We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semiparametric topological memory ({SPTM}) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use {SPTM} as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an {SPTM}-based navigation agent can build a topological map of the environment and use it to conﬁdently navigate towards goals. The average success rate of the {SPTM} agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three.},
	journaltitle = {{arXiv}:1803.00653 [cs]},
	author = {Savinov, Nikolay and Dosovitskiy, Alexey and Koltun, Vladlen},
	urldate = {2022-04-05},
	date = {2018-03-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.00653},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {Savinov et al. - 2018 - Semi-parametric Topological Memory for Navigation.pdf:/home/oslund/Zotero/storage/9S4F377G/Savinov et al. - 2018 - Semi-parametric Topological Memory for Navigation.pdf:application/pdf},
}

@article{arkin_approximation_2000,
	title = {Approximation Algorithms for Lawn Mowing and Milling},
	volume = {17},
	issn = {0925-7721},
	url = {https://www.sciencedirect.com/science/article/pii/S0925772100000158},
	doi = {10.1016/S0925-7721(00)00015-8},
	abstract = {We study the problem of finding shortest tours/paths for “lawn mowing” and “milling” problems: Given a region in the plane, and given the shape of a “cutter” (typically, a circle or a square), find a shortest tour/path for the cutter such that every point within the region is covered by the cutter at some position along the tour/path. In the milling version of the problem, the cutter is constrained to stay within the region. The milling problem arises naturally in the area of automatic tool path generation for {NC} pocket machining. The lawn mowing problem arises in optical inspection, spray painting, and optimal search planning. Both problems are {NP}-hard in general. We give efficient constant-factor approximation algorithms for both problems. In particular, we give a (3+ε)-approximation algorithm for the lawn mowing problem and a 2.5-approximation algorithm for the milling problem. Furthermore, we give a simple 65-approximation algorithm for the {TSP} problem in simple grid graphs, which leads to an 115-approximation algorithm for milling simple rectilinear polygons.},
	pages = {25--50},
	number = {1},
	journaltitle = {Computational Geometry},
	shortjournal = {Computational Geometry},
	author = {Arkin, Esther M. and Fekete, Sándor P. and Mitchell, Joseph S. B.},
	urldate = {2022-04-05},
	date = {2000-10-01},
	langid = {english},
	keywords = {Approximation algorithms, Computational geometry, Geometric optimization, Lawn mowing, Milling, {NC} machining, {NP}-completeness, Traveling salesman problem ({TSP})},
	file = {ScienceDirect Full Text PDF:/home/oslund/Zotero/storage/KGYJ6LFZ/Arkin et al. - 2000 - Approximation algorithms for lawn mowing and milli.pdf:application/pdf;ScienceDirect Snapshot:/home/oslund/Zotero/storage/P22X2DLS/S0925772100000158.html:text/html},
}

@article{zhao_object_2019,
	title = {Object Detection With Deep Learning: A Review},
	volume = {30},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/8627998/},
	doi = {10.1109/TNNLS.2018.2876865},
	shorttitle = {Object Detection With Deep Learning},
	abstract = {Due to object detection’s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classiﬁers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modiﬁcations and useful tricks to improve detection performance further. As distinct speciﬁc detection tasks exhibit different characteristics, we also brieﬂy survey several speciﬁc tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.},
	pages = {3212--3232},
	number = {11},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-Tao and Wu, Xindong},
	urldate = {2022-04-05},
	date = {2019-11},
	langid = {english},
	file = {Zhao et al. - 2019 - Object Detection With Deep Learning A Review.pdf:/home/oslund/Zotero/storage/QF6YRAJN/Zhao et al. - 2019 - Object Detection With Deep Learning A Review.pdf:application/pdf},
}

@article{chen_active_2011,
	title = {Active Vision in Robotic Systems: A Survey of Recent Developments},
	volume = {30},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364911410755},
	doi = {10.1177/0278364911410755},
	shorttitle = {Active Vision In Robotic Systems},
	abstract = {In this paper we provide a broad survey of developments in active vision in robotic applications over the last 15 years. With increasing demand for robotic automation, research in this area has received much attention. Among the many factors that can be attributed to a high-performance robotic system, the planned sensing or acquisition of perceptions on the operating environment is a crucial component. The aim of sensor planning is to determine the pose and settings of vision sensors for undertaking a vision-based task that usually requires obtaining multiple views of the object to be manipulated. Planning for robot vision is a complex problem for an active system due to its sensing uncertainty and environmental uncertainty. This paper describes such problems arising from many applications, e.g. object recognition and modeling, site reconstruction and inspection, surveillance, tracking and search, as well as robotic manipulation and assembly, localization and mapping, navigation and exploration. A bundle of solutions and methods have been proposed to solve these problems in the past. They are summarized in this review while enabling readers to easily refer solution methods for practical applications. Representative contributions, their evaluations, analyses, and future research trends are also addressed in an abstract level.},
	pages = {1343--1377},
	number = {11},
	journaltitle = {The International Journal of Robotics Research},
	shortjournal = {The International Journal of Robotics Research},
	author = {Chen, Shengyong and Li, Youfu and Kwok, Ngai Ming},
	urldate = {2022-04-05},
	date = {2011-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd {STM}},
	keywords = {Active vision, computer vision, purposive perception planning, robotics, sensor placement, uncertainty, viewpoint scheduling},
}

@article{itti_computational_2001,
	title = {Computational Modelling of Visual Attention},
	volume = {2},
	rights = {2001 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/35058500},
	doi = {10.1038/35058500},
	abstract = {We review recent work on computational models of focal visual attention, with emphasis on the bottom-up, saliency- or image-based control of attentional deployment. We highlight five important trends that have emerged from the computational literature: First, the perceptual saliency of stimuli critically depends on surrounding context; that is, the same object may or may not appear salient depending on the nature and arrangement of other objects in the scene. Computationally, this means that contextual influences, such as non-classical surround interactions, must be included in models. Second, a unique 'saliency map' topographically encoding for stimulus conspicuity over the visual scene has proved to be an efficient and plausible bottom-up control strategy. Many successful models are based on such architecture, and electrophysiological as well as psychophysical studies have recently supported the idea that saliency is explicitly encoded in the brain. Third, inhibition-of-return ({IOR}), the process by which the currently attended location is transiently inhibited, is a critical element of attentional deployment. Without {IOR}, attention would endlessly be attracted towards the most salient stimulus. {IOR} thus implements a memory of recently visited locations, and allows attention to thoroughly scan our visual environment. Fourth, attention and eye movements tightly interplay, posing computational challenges with respect to the coordinate system used to control attention. Understanding the interaction between overt and covert attention is particularly important for models concerned with visual search. Last, scene understanding and object recognition strongly constrain the selection of attended locations. Although several models have approached, in an information-theoretical sense, the problem of optimally deploying attention to analyse a scene, biologically plausible implementations of such a computational strategy remain to be developed.},
	pages = {194--203},
	number = {3},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Itti, Laurent and Koch, Christof},
	urldate = {2022-04-05},
	date = {2001-03},
	langid = {english},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
	file = {Accepted Version:/home/oslund/Zotero/storage/ZZJJWS53/Itti and Koch - 2001 - Computational modelling of visual attention.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/SDMN2N25/35058500.html:text/html},
}

@article{andrychowicz_what_2020,
	title = {What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study},
	url = {http://arxiv.org/abs/2006.05990},
	shorttitle = {What Matters In On-Policy Reinforcement Learning?},
	abstract = {In recent years, on-policy reinforcement learning ({RL}) has been successfully applied to many different continuous control tasks. While {RL} algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in {RL} and slows down overall progress [27]. As a step towards ﬁlling that gap, we implement {\textgreater}50 such “choices” in a uniﬁed on-policy {RL} framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250’000 agents in ﬁve continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of {RL} agents.},
	journaltitle = {{arXiv}:2006.05990 [cs, stat]},
	author = {Andrychowicz, Marcin and Raichuk, Anton and Stańczyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, Léonard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and Gelly, Sylvain and Bachem, Olivier},
	urldate = {2022-04-06},
	date = {2020-06-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.05990},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf:/home/oslund/Zotero/storage/9QPUDF4C/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf:application/pdf},
}

@article{silver_mastering_2016,
	title = {Mastering the Game of Go With Deep Neural Networks and Tree Search},
	volume = {529},
	rights = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961%7D},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program {AlphaGo} achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	pages = {484--489},
	number = {7587},
	journaltitle = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	urldate = {2022-04-06},
	date = {2016-01},
	langid = {english},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science, Reward},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster Level in {StarCraft} {II} Using Multi-agent Reinforcement Learning},
	volume = {575},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of {StarCraft} has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top {StarCraft} players. We chose to address the challenge of {StarCraft} using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, {AlphaStar}, in the full game of {StarCraft} {II}, through a series of online games against human players. {AlphaStar} was rated at Grandmaster level for all three {StarCraft} races and above 99.8\% of officially ranked human players.},
	pages = {350--354},
	number = {7782},
	journaltitle = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and {McKinney}, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	urldate = {2022-04-06},
	date = {2019-11},
	langid = {english},
	note = {Number: 7782
Publisher: Nature Publishing Group},
	keywords = {Statistics, Computer science},
}

@article{bonin-font_visual_2008,
	title = {Visual Navigation for Mobile Robots: A Survey},
	volume = {53},
	issn = {0921-0296, 1573-0409},
	url = {http://link.springer.com/10.1007/s10846-008-9235-4},
	doi = {10.1007/s10846-008-9235-4},
	shorttitle = {Visual Navigation for Mobile Robots},
	abstract = {Mobile robot vision-based navigation has been the source of countless research contributions, from the domains of both vision and control. Vision is becoming more and more common in applications such as localization, automatic map construction, autonomous navigation, path following, inspection, monitoring or risky situation detection. This survey presents those pieces of work, from the nineties until nowadays, which constitute a wide progress in visual navigation techniques for land, aerial and autonomous underwater vehicles. The paper deals with two major approaches: map-based navigation and mapless navigation. Map-based navigation has been in turn subdivided in metric map-based navigation and topological mapbased navigation. Our outline to mapless navigation includes reactive techniques based on qualitative characteristics extraction, appearance-based localization, optical ﬂow, features tracking, plane ground detection/tracking, etc... The recent concept of visual sonar has also been revised.},
	pages = {263--296},
	number = {3},
	journaltitle = {Journal of Intelligent and Robotic Systems},
	shortjournal = {J Intell Robot Syst},
	author = {Bonin-Font, Francisco and Ortiz, Alberto and Oliver, Gabriel},
	urldate = {2022-04-07},
	date = {2008-11},
	langid = {english},
	file = {Bonin-Font et al. - 2008 - Visual Navigation for Mobile Robots A Survey.pdf:/home/oslund/Zotero/storage/UVCGTUUC/Bonin-Font et al. - 2008 - Visual Navigation for Mobile Robots A Survey.pdf:application/pdf},
}

@article{nakayama_situating_2011,
	title = {Situating Visual Search},
	volume = {51},
	issn = {0042-6989},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698910004372},
	doi = {10.1016/j.visres.2010.09.003},
	series = {Vision Research 50th Anniversary Issue: Part 2},
	abstract = {Visual search attracted great interest because its ease under certain circumstances seemed to provide a way to understand how properties of early visual cortical areas could explain complex perception without resorting to higher order psychological or neurophysiological mechanisms. Furthermore, there was the hope that properties of visual search itself might even reveal new cortical features or dimensions. The shortcomings of this perspective suggest that we abandon fixed canonical elementary particles of vision as well as a corresponding simple to complex cognitive architecture for vision. Instead recent research has suggested a different organization of the visual brain with putative high level processing occurring very rapidly and often unconsciously. Given this outlook, we reconsider visual search under the broad category of recognition tasks, each having different trade-offs for computational resources, between detail and scope. We conclude noting recent trends showing how visual search is relevant to a wider range of issues in cognitive science, in particular to memory, decision making, and reward.},
	pages = {1526--1537},
	number = {13},
	journaltitle = {Vision Research},
	shortjournal = {Vision Research},
	author = {Nakayama, Ken and Martini, Paolo},
	urldate = {2022-04-08},
	date = {2011-07-01},
	langid = {english},
	keywords = {Reinforcement learning, Attention, Object recognition},
	file = {ScienceDirect Snapshot:/home/oslund/Zotero/storage/MAJ5L27N/S0042698910004372.html:text/html;Submitted Version:/home/oslund/Zotero/storage/7KZP9DK4/Nakayama and Martini - 2011 - Situating visual search.pdf:application/pdf},
}

@inproceedings{eidenberger_fast_2008,
	title = {Fast Parametric Viewpoint Estimation for Active Object Detection},
	doi = {10.1109/MFI.2008.4648083},
	abstract = {Most current solutions to active perception planning struggle with complex state representations or fast and efficient sensor parameter selection strategies. The goal is to find new viewpoints or optimize sensor parameters for further measurements in order to classify an object and precisely locate its position. This paper presents an exclusively parametric approach for the state estimation and decision making process to achieve very low computational complexity and short calculation times. The proposed approach assumes a realistic, high dimensional and continuous state space for the representation of objects expressing their rotation, translation and class. Its probability distribution is described by multivariate mixtures of Gaussians which allow the representation of arbitrary object hypotheses. In a statistical framework Bayesian state estimation updates the current state probability distribution based on a scene observation which depends on the sensor parameters. These are selected in a decision process which aims on reducing the uncertainty in the state distribution. Approximations of information theoretic measurements are used as evaluation criteria.},
	eventtitle = {2008 {IEEE} International Conference on Multisensor Fusion and Integration for Intelligent Systems},
	pages = {309--314},
	booktitle = {2008 {IEEE} International Conference on Multisensor Fusion and Integration for Intelligent Systems},
	author = {Eidenberger, Robert and Grundmann, Thilo and Feiten, Wendelin and Zoellner, Raoul},
	date = {2008-08},
	note = {8 citations (Crossref) [2022-04-15]},
	keywords = {Robot sensing systems, Sensors, Feature extraction, Entropy, Equations, Probability distribution, State estimation},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/RMFB5WA3/4648083.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/UGSZFK7W/Eidenberger et al. - 2008 - Fast parametric viewpoint estimation for active ob.pdf:application/pdf},
}

@article{zeng_view_2020,
	title = {View Planning in Robot Active Vision: A Survey of Systems, Algorithms, and Applications},
	volume = {6},
	issn = {2096-0433, 2096-0662},
	url = {https://link.springer.com/10.1007/s41095-020-0179-3},
	doi = {10.1007/s41095-020-0179-3},
	shorttitle = {View Planning in Robot Active Vision},
	abstract = {Rapid development of artiﬁcial intelligence motivates researchers to expand the capabilities of intelligent and autonomous robots. In many robotic applications, robots are required to make planning decisions based on perceptual information to achieve diverse goals in an eﬃcient and eﬀective way. The planning problem has been investigated in active robot vision, in which a robot analyzes its environment and its own state in order to move sensors to obtain more useful information under certain constraints. View planning, which aims to ﬁnd the best view sequence for a sensor, is one of the most challenging issues in active robot vision. The quality and eﬃciency of view planning are critical for many robot systems and are inﬂuenced by the nature of their tasks, hardware conditions, scanning states, and planning strategies. In this paper, we ﬁrst summarize some basic concepts of active robot vision, and then review representative work on systems, algorithms and applications from four perspectives: object reconstruction, scene reconstruction, object recognition, and pose estimation. Finally, some potential directions are outlined for future work.},
	pages = {225--245},
	number = {3},
	journaltitle = {Computational Visual Media},
	shortjournal = {Comp. Visual Media},
	author = {Zeng, Rui and Wen, Yuhui and Zhao, Wang and Liu, Yong-Jin},
	urldate = {2022-04-11},
	date = {2020-09},
	langid = {english},
	note = {13 citations (Crossref) [2022-04-15]},
	file = {Zeng et al. - 2020 - View planning in robot active vision A survey of .pdf:/home/oslund/Zotero/storage/MQDK6N57/Zeng et al. - 2020 - View planning in robot active vision A survey of .pdf:application/pdf},
}

@article{shubina_visual_2010,
	title = {Visual Search for an Object in a 3D Environment Using a Mobile Robot},
	volume = {114},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314210000378},
	doi = {10.1016/j.cviu.2009.06.010},
	series = {Special issue on Intelligent Vision Systems},
	abstract = {Consider the problem of visually finding an object in a mostly unknown space with a mobile robot. It is clear that all possible views and images cannot be examined in a practical system. Visual attention is a complex phenomenon; we view it as a mechanism that optimizes the search processes inherent in vision (Tsotsos, 2001; Tsotsos et al., 2008) [1], [2]. Here, we describe a particular example of a practical robotic vision system that employs some of these attentive processes. We cast this as an optimization problem, i.e., optimizing the probability of finding the target given a fixed cost limit in terms of total number of robotic actions required to find the visual target. Due to the inherent intractability of this problem, we present an approximate solution and investigate its performance and properties. We conclude that our approach is sufficient to solve this problem and has additional desirable empirical characteristics.},
	pages = {535--547},
	number = {5},
	journaltitle = {Computer Vision and Image Understanding},
	shortjournal = {Computer Vision and Image Understanding},
	author = {Shubina, Ksenia and Tsotsos, John K.},
	urldate = {2022-04-11},
	date = {2010-05-01},
	langid = {english},
	note = {61 citations (Crossref) [2022-04-15]},
	keywords = {Active vision, Visual search, Visual attention, Active object recognition, Mobile robot, Search, Sensor planning},
	file = {ScienceDirect Full Text PDF:/home/oslund/Zotero/storage/3J9BWHTF/Shubina and Tsotsos - 2010 - Visual search for an object in a 3D environment us.pdf:application/pdf;ScienceDirect Snapshot:/home/oslund/Zotero/storage/YGRI2H8T/S1077314210000378.html:text/html},
}

@article{fulton_safe_2018,
	title = {Safe Reinforcement Learning via Formal Methods: Toward Safe Control Through Proof and Learning},
	volume = {32},
	rights = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12107},
	shorttitle = {Safe Reinforcement Learning via Formal Methods},
	abstract = {Formal verification provides a high degree of confidence in safe system operation, but only if reality matches the verified model. Although a good model will be accurate most of the time, even the best models are incomplete. This is especially true in Cyber-Physical Systems because high-fidelity physical models of systems are expensive to develop and often intractable to verify. Conversely, reinforcement learning-based controllers are lauded for their flexibility in unmodeled environments, but do not provide guarantees of safe operation. This paper presents an approach for provably safe learning that provides the best of both worlds: the exploration and optimization capabilities of learning along with the safety guarantees of formal verification. Our main insight is that formal verification combined with verified runtime monitoring can ensure the safety of a learning agent. Verification results are preserved whenever learning agents limit exploration within the confounds of verified control choices as long as observed reality comports with the model used for off-line verification. When a model violation is detected, the agent abandons efficiency and instead attempts to learn a control strategy that guides the agent to a modeled portion of the state space. We prove that our approach toward incorporating knowledge about safe control into learning systems preserves safety guarantees, and demonstrate that we retain the empirical performance benefits provided by reinforcement learning. We also explore various points in the design space for these justified speculative controllers in a simple model of adaptive cruise control model for autonomous cars.},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Fulton, Nathan and Platzer, André},
	urldate = {2022-05-25},
	date = {2018-04-26},
	langid = {english},
	note = {Number: 1},
	keywords = {Cyber-Physical Systems, No {DOI} found},
	file = {Full Text PDF:/home/oskar/library/zotero/storage/FRLQ6XXC/Fulton and Platzer - 2018 - Safe Reinforcement Learning via Formal Methods To.pdf:application/pdf},
}

@article{oh_role_2004,
	title = {The Role of Spatial Working Memory in Visual Search Efficiency},
	volume = {11},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/BF03196570},
	doi = {10.3758/BF03196570},
	pages = {275--281},
	number = {2},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychonomic Bulletin \& Review},
	author = {Oh, Sei-Hwan and Kim, Min-Shik},
	urldate = {2022-04-11},
	date = {2004-04},
	langid = {english},
	note = {129 citations (Crossref) [2022-04-15]},
	file = {Oh and Kim - 2004 - The role of spatial working memory in visual searc.pdf:/home/oslund/Zotero/storage/T4TSCHDP/Oh and Kim - 2004 - The role of spatial working memory in visual searc.pdf:application/pdf},
}

@inproceedings{forssen_informed_2008,
	title = {Informed Visual Search: Combining Attention and Object Recognition},
	doi = {10.1109/ROBOT.2008.4543325},
	shorttitle = {Informed visual search},
	abstract = {This paper studies the sequential object recognition problem faced by a mobile robot searching for specific objects within a cluttered environment. In contrast to current state-of-the-art object recognition solutions which are evaluated on databases of static images, the system described in this paper employs an active strategy based on identifying potential objects using an attention mechanism and planning to obtain images of these objects from numerous viewpoints. We demonstrate the use of a bag-of-features technique for ranking potential objects, and show that this measure outperforms geometric matching for invariance across viewpoints. Our system implements informed visual search by prioritising map locations and re-examining promising locations first. Experimental results demonstrate that our system is a highly competent object recognition system that is capable of locating numerous challenging objects amongst distractors.},
	eventtitle = {2008 {IEEE} International Conference on Robotics and Automation},
	pages = {935--942},
	booktitle = {2008 {IEEE} International Conference on Robotics and Automation},
	author = {Forssen, Per-Erik and Meger, David and Lai, Kevin and Helmer, Scott and Little, James J. and Lowe, David G.},
	date = {2008-05},
	note = {35 citations (Crossref) [2022-04-15]
{ISSN}: 1050-4729},
	keywords = {Humans, Mobile robots, Object recognition, Robot vision systems, Cameras, Footwear, Image databases, Robotics and automation, Training data, {USA} Councils},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/Z8SHBN62/4543325.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/R6LCQGBB/Forssen et al. - 2008 - Informed visual search Combining attention and ob.pdf:application/pdf},
}

@article{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2022-04-12},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/CP8XJG78/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/UHHPEXVG/1412.html:text/html},
}

@article{meger_curious_2008,
	title = {Curious George: An Attentive Semantic Robot},
	volume = {56},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889008000316},
	doi = {10.1016/j.robot.2008.03.008},
	series = {From Sensors to Human Spatial Concepts},
	shorttitle = {Curious George},
	abstract = {State-of-the-art methods have recently achieved impressive performance for recognising the objects present in large databases of pre-collected images. There has been much less focus on building embodied systems that recognise objects present in the real world. This paper describes an intelligent system that attempts to perform robust object recognition in a realistic scenario, where a mobile robot moving through an environment must use the images collected from its camera directly to recognise objects. To perform successful recognition in this scenario, we have chosen a combination of techniques including a peripheral-foveal vision system, an attention system combining bottom-up visual saliency with structure from stereo, and a localisation and mapping technique. The result is a highly capable object recognition system that can be easily trained to locate the objects of interest in an environment, and subsequently build a spatial-semantic map of the region. This capability has been demonstrated during the Semantic Robot Vision Challenge, and is further illustrated with a demonstration of semantic mapping. We also empirically verify that the attention system outperforms an undirected approach even with a significantly lower number of foveations.},
	pages = {503--511},
	number = {6},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Meger, David and Forssén, Per-Erik and Lai, Kevin and Helmer, Scott and {McCann}, Sancho and Southey, Tristram and Baumann, Matthew and Little, James J. and Lowe, David G.},
	urldate = {2022-04-27},
	date = {2008-06-30},
	langid = {english},
	note = {103 citations (Crossref) [2022-05-17]},
	keywords = {Object recognition, Visual attention, Object permanence, Saliency, Semantic robot vision},
	file = {ScienceDirect Snapshot:/home/oslund/Zotero/storage/2LVX53VH/S0921889008000316.html:text/html},
}

@article{colas_hitchhikers_2019,
	title = {A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning Algorithms},
	url = {http://arxiv.org/abs/1904.06979},
	abstract = {Consistently checking the statistical signiﬁcance of experimental results is the ﬁrst mandatory step towards reproducible science. This paper presents a hitchhiker’s guide to rigorous comparisons of reinforcement learning algorithms. After introducing the concepts of statistical testing, we review the relevant statistical tests and compare them empirically in terms of false positive rate and statistical power as a function of the sample size (number of seeds) and effect size. We further investigate the robustness of these tests to violations of the most common hypotheses (normal distributions, same distributions, equal variances). Beside simulations, we compare empirical distributions obtained by running Soft-Actor Critic and Twin-Delayed Deep Deterministic Policy Gradient on Half-Cheetah. We conclude by providing guidelines and code to perform rigorous comparisons of {RL} algorithm performances.},
	journaltitle = {{arXiv}:1904.06979 [cs, stat]},
	author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	urldate = {2022-05-02},
	date = {2019-04-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.06979},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Statistics - Methodology},
	file = {Colas et al. - 2019 - A Hitchhiker's Guide to Statistical Comparisons of.pdf:/home/oslund/Zotero/storage/PSDBDGIG/Colas et al. - 2019 - A Hitchhiker's Guide to Statistical Comparisons of.pdf:application/pdf},
}

@article{colas_how_2018,
	title = {How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments},
	url = {http://arxiv.org/abs/1806.08295},
	shorttitle = {How Many Random Seeds?},
	abstract = {Consistently checking the statistical signiﬁcance of experimental results is one of the mandatory methodological steps to address the so-called “reproducibility crisis” in deep reinforcement learning. In this tutorial paper, we explain how the number of random seeds relates to the probabilities of statistical errors. For both the t-test and the bootstrap conﬁdence interval test, we recall theoretical guidelines to determine the number of random seeds one should use to provide a statistically signiﬁcant comparison of the performance of two algorithms. Finally, we discuss the inﬂuence of deviations from the assumptions usually made by statistical tests. We show that they can lead to inaccurate evaluations of statistical errors and provide guidelines to counter these negative eﬀects. We make our code available to perform the tests1.},
	journaltitle = {{arXiv}:1806.08295 [cs, stat]},
	author = {Colas, Cédric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	urldate = {2022-05-02},
	date = {2018-07-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.08295},
	keywords = {⛔ No {DOI} found, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Colas et al. - 2018 - How Many Random Seeds Statistical Power Analysis .pdf:/home/oslund/Zotero/storage/HMJEHCMI/Colas et al. - 2018 - How Many Random Seeds Statistical Power Analysis .pdf:application/pdf},
}

@article{russell_research_2015,
	title = {Research Priorities for Robust and Beneficial Artificial Intelligence},
	volume = {36},
	rights = {Copyright (c) 2015},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2577},
	doi = {10.1609/aimag.v36i4.2577},
	abstract = {Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that {AI} remains robust and beneficial.},
	pages = {105--114},
	number = {4},
	journaltitle = {{AI} Magazine},
	author = {Russell, Stuart and Dewey, Daniel and Tegmark, Max},
	urldate = {2022-05-16},
	date = {2015-12-31},
	langid = {english},
	note = {195 citations (Crossref) [2022-05-17]
Number: 4},
	file = {Full Text PDF:/home/oslund/Zotero/storage/BFNV6U9W/Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf:application/pdf},
}

@article{vinuesa_role_2020,
	title = {The Role of Artificial Intelligence in Achieving the Sustainable Development Goals},
	volume = {11},
	rights = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-14108-y},
	doi = {10.1038/s41467-019-14108-y},
	abstract = {The emergence of artificial intelligence ({AI}) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals. Using a consensus-based expert elicitation process, we find that {AI} can enable the accomplishment of 134 targets across all the goals, but it may also inhibit 59 targets. However, current research foci overlook important aspects. The fast development of {AI} needs to be supported by the necessary regulatory insight and oversight for {AI}-based technologies to enable sustainable development. Failure to do so could result in gaps in transparency, safety, and ethical standards.},
	pages = {233},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Felländer, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},
	urldate = {2022-05-16},
	date = {2020-01-13},
	langid = {english},
	note = {297 citations (Crossref) [2022-05-17]
Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Developing world, Energy efficiency},
	file = {Full Text PDF:/home/oslund/Zotero/storage/4GRXM8BQ/Vinuesa et al. - 2020 - The role of artificial intelligence in achieving t.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/Y4FFG4VZ/s41467-019-14108-y.html:text/html},
}

@inproceedings{russell_provably_2022,
	location = {New York, {NY}, {USA}},
	title = {Provably Beneficial Artificial Intelligence},
	isbn = {978-1-4503-9144-3},
	url = {https://doi.org/10.1145/3490099.3519388},
	doi = {10.1145/3490099.3519388},
	series = {{IUI} '22},
	abstract = {As {AI} advances in capabilities and moves into the real world, its potential to benefit humanity seems limitless. Yet we see serious problems including racial and gender bias, manipulation by social media, and an arms race in lethal autonomous weapons. Looking further ahead, Alan Turing predicted the eventual loss of human control over machines that exceed human capabilities. I will argue that Turing was right to express concern but wrong to think that doom is inevitable. Instead, we need to develop a new kind of {AI} that is provably beneficial to humans.},
	pages = {3},
	booktitle = {27th International Conference on Intelligent User Interfaces},
	publisher = {Association for Computing Machinery},
	author = {Russell, Stuart},
	urldate = {2022-05-16},
	date = {2022-03-22},
	note = {0 citations (Crossref) [2022-05-17]},
}

@report{brundage_malicious_2018,
	title = {The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation},
	url = {http://arxiv.org/abs/1802.07228},
	shorttitle = {The Malicious Use of Artificial Intelligence},
	abstract = {This report surveys the landscape of potential security threats from malicious uses of {AI}, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which {AI} may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for {AI} researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.},
	number = {{arXiv}:1802.07228},
	institution = {{arXiv}},
	author = {Brundage, Miles and Avin, Shahar and Clark, Jack and Toner, Helen and Eckersley, Peter and Garfinkel, Ben and Dafoe, Allan and Scharre, Paul and Zeitzoff, Thomas and Filar, Bobby and Anderson, Hyrum and Roff, Heather and Allen, Gregory C. and Steinhardt, Jacob and Flynn, Carrick and {hÉigeartaigh}, Seán Ó and Beard, Simon and Belfield, Haydn and Farquhar, Sebastian and Lyle, Clare and Crootof, Rebecca and Evans, Owain and Page, Michael and Bryson, Joanna and Yampolskiy, Roman and Amodei, Dario},
	urldate = {2022-05-16},
	date = {2018-02-20},
	doi = {10.48550/arXiv.1802.07228},
	eprinttype = {arxiv},
	eprint = {1802.07228 [cs]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/WMLW6PP3/Brundage et al. - 2018 - The Malicious Use of Artificial Intelligence Fore.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/45W8Q6KR/1802.html:text/html},
}

@article{bengio_representation_2013,
	title = {Representation Learning: A Review and New Perspectives},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2013.50},
	shorttitle = {Representation Learning},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	pages = {1798--1828},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	date = {2013-08},
	note = {5407 citations (Crossref) [2022-05-19]
Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Abstracts, autoencoder, Boltzmann machine, Deep learning, Feature extraction, feature learning, Learning systems, Machine learning, Manifolds, neural nets, Neural networks, representation learning, Speech recognition, unsupervised learning},
	file = {IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/2R6T4EMN/Bengio et al. - 2013 - Representation Learning A Review and New Perspect.pdf:application/pdf},
}

@inproceedings{mnih_recurrent_2014,
	title = {Recurrent Models of Visual Attention},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html},
	abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and kavukcuoglu, koray},
	urldate = {2022-05-19},
	date = {2014},
	keywords = {No {DOI} found},
	file = {Full Text PDF:/home/oslund/Zotero/storage/YUJ8L9RE/Mnih et al. - 2014 - Recurrent Models of Visual Attention.pdf:application/pdf},
}

@inproceedings{oh_control_2016,
	title = {Control of Memory, Active Perception, and Action in Minecraft},
	url = {https://proceedings.mlr.press/v48/oh16.html},
	abstract = {In this paper, we introduce a new set of reinforcement learning ({RL}) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning ({DRL}) architectures with our new memory-based {DRL} architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for {RL} methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current {DRL} architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing {DRL} architectures.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2790--2799},
	booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Oh, Junhyuk and Chockalingam, Valliappa and Satinder and Lee, Honglak},
	urldate = {2022-05-19},
	date = {2016-06-11},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {Full Text PDF:/home/oslund/Zotero/storage/SSBHL2ZC/Oh et al. - 2016 - Control of Memory, Active Perception, and Action i.pdf:application/pdf},
}

@inproceedings{pardo_time_2018,
	title = {Time Limits in Reinforcement Learning},
	url = {https://proceedings.mlr.press/v80/pardo18a.html},
	abstract = {In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and repeating the process in a series of episodes. The task that the agent has to learn can either be to maximize its performance over (i) that fixed period, or (ii) an indefinite period where time limits are only used during training to diversify experience. In this paper, we provide a formal account for how time limits could effectively be handled in each of the two cases and explain why not doing so can cause state-aliasing and invalidation of experience replay, leading to suboptimal policies and training instability. In case (i), we argue that the terminations due to time limits are in fact part of the environment, and thus a notion of the remaining time should be included as part of the agent’s input to avoid violation of the Markov property. In case (ii), the time limits are not part of the environment and are only used to facilitate learning. We argue that this insight should be incorporated by bootstrapping from the value of the state at the end of each partial episode. For both cases, we illustrate empirically the significance of our considerations in improving the performance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.},
	eventtitle = {International Conference on Machine Learning},
	pages = {4045--4054},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
	urldate = {2022-05-19},
	date = {2018-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/oslund/Zotero/storage/7FBXX4N4/Pardo et al. - 2018 - Time Limits in Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	shorttitle = {{PyTorch}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. {PyTorch} is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as {GPUs}.
In this paper, we detail the principles that drove the implementation of {PyTorch} and how they are reflected in its architecture. We emphasize that every aspect of {PyTorch} is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of {PyTorch} on several commonly used benchmarks.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and {DeVito}, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	urldate = {2022-05-19},
	date = {2019},
	keywords = {No {DOI} found},
	file = {Full Text PDF:/home/oslund/Zotero/storage/E3RN243A/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@inproceedings{sutton_policy_1999,
	title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
	volume = {12},
	url = {https://proceedings.neurips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
	abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  {REINFORCE} method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Sutton, Richard S and {McAllester}, David and Singh, Satinder and Mansour, Yishay},
	urldate = {2022-05-19},
	date = {1999},
	keywords = {No {DOI} found},
	file = {Full Text PDF:/home/oslund/Zotero/storage/ZNFF3UBJ/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{zhu_target-driven_2017,
	title = {Target-driven Visual Navigation in Indoor Scenes Using Deep Reinforcement Learning},
	doi = {10.1109/ICRA.2017.7989381},
	abstract = {Two less addressed issues of deep reinforcement learning are (1) lack of generalization capability to new goals, and (2) data inefficiency, i.e., the model requires several (and often costly) episodes of trial and error to converge, which makes it impractical to be applied to real-world scenarios. In this paper, we address these two issues and apply our model to target-driven visual navigation. To address the first issue, we propose an actor-critic model whose policy is a function of the goal as well as the current state, which allows better generalization. To address the second issue, we propose the {AI}2-{THOR} framework, which provides an environment with high-quality 3D scenes and a physics engine. Our framework enables agents to take actions and interact with objects. Hence, we can collect a huge number of training samples efficiently. We show that our proposed method (1) converges faster than the state-of-the-art deep reinforcement learning methods, (2) generalizes across targets and scenes, (3) generalizes to a real robot scenario with a small amount of fine-tuning (although the model is trained in simulation), (4) is end-to-end trainable and does not need feature engineering, feature matching between frames or 3D reconstruction of the environment.},
	eventtitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {3357--3364},
	booktitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Zhu, Yuke and Mottaghi, Roozbeh and Kolve, Eric and Lim, Joseph J. and Gupta, Abhinav and Fei-Fei, Li and Farhadi, Ali},
	date = {2017-05},
	note = {474 citations (Crossref) [2022-05-19]},
	keywords = {Learning (artificial intelligence), Navigation, Physics, Robots, Three-dimensional displays, Training, Visualization},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/TQR7AUF2/7989381.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/JFZJDNVL/Zhu et al. - 2017 - Target-driven visual navigation in indoor scenes u.pdf:application/pdf},
}

@article{tesauro_temporal_1995,
	title = {Temporal Difference Learning and {TD}-Gammon},
	volume = {38},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/203330.203343},
	doi = {10.1145/203330.203343},
	abstract = {Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.},
	pages = {58--68},
	number = {3},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Tesauro, Gerald},
	urldate = {2022-05-19},
	date = {1995-03-01},
	note = {678 citations (Crossref) [2022-05-19]},
	file = {Full Text PDF:/home/oslund/Zotero/storage/6HXAGMBY/Tesauro - 1995 - Temporal difference learning and TD-Gammon.pdf:application/pdf},
}

@inproceedings{andreopoulos_theory_2009,
	title = {A Theory of Active Object Localization},
	doi = {10.1109/ICCV.2009.5459332},
	abstract = {We present some theoretical results related to the problem of actively searching for a target in a 3D environment, under the constraint of a maximum search time. We define the object localization problem as the maximization over the search region of the Lebesgue integral of the scene structure probabilities. We study variants of the problem as they relate to actively selecting a finite set of optimal viewpoints of the scene for detecting and localizing an object. We do a complexity-level analysis and show that the problem variants are {NP}-Complete or {NP}-Hard. We study the tradeoffs of localizing vs. detecting a target object, using single-view and multiple-view recognition, under imperfect dead-reckoning and an imperfect recognition algorithm. These results motivate a set of properties that efficient and reliable active object localization algorithms should satisfy.},
	eventtitle = {2009 {IEEE} 12th International Conference on Computer Vision},
	pages = {903--910},
	booktitle = {2009 {IEEE} 12th International Conference on Computer Vision},
	author = {Andreopoulos, Alexander and Tsotsos, John K.},
	date = {2009-09},
	note = {15 citations (Crossref) [2022-05-19]
{ISSN}: 2380-7504},
	keywords = {Cameras, Computer science, Constraint theory, Costs, Data acquisition, Layout, Machine vision, Object detection, Spatial resolution, Target recognition},
	file = {IEEE Xplore Abstract Record:/home/oslund/Zotero/storage/XQXBVEYG/5459332.html:text/html;IEEE Xplore Full Text PDF:/home/oslund/Zotero/storage/FU8TJPN2/Andreopoulos and Tsotsos - 2009 - A theory of active object localization.pdf:application/pdf},
}

@inproceedings{agarwal_optimality_2020,
	title = {Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes},
	url = {https://proceedings.mlr.press/v125/agarwal20a.html},
	abstract = {Policy gradient ({PG}) methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including:  if and how fast they converge to a globally optimal solution (say with a sufficiently rich policy class);  how they cope with approximation error due to using a restricted class of parametric policies; or  their finite sample behavior.  Such characterizations are important not only to compare these methods to their approximate value function counterparts (where such issues are relatively well understood, at least in the worst case), but also to help with more principled approaches to algorithm design. This work provides provable characterizations of computational, approximation, and sample size issues with regards to policy gradient methods in the context of discounted Markov Decision Processes ({MDPs}). We focus on both: 1) “tabular” policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy, and 2) restricted policy classes, which may not contain the optimal policy and where we provide agnostic learning results.  In the {\textbackslash}emph\{tabular setting\}, our main results are: 1) convergence rate to global optimum for direct parameterization and projected gradient ascent  2) an asymptotic convergence to global optimum for softmax policy parameterization and {PG}; and a convergence rate with additional entropy regularization, and 3) dimension-free convergence to global optimum for softmax policy parameterization and Natural Policy Gradient ({NPG}) method with exact gradients. In {\textbackslash}emph\{function approximation\}, we further analyze {NPG} with exact as well as inexact gradients under certain smoothness assumptions on the policy parameterization and establish rates of convergence in terms of the quality of the initial state distribution. One insight of this work is in formalizing how a favorable initial state distribution provides a means to circumvent worst-case exploration issues.  Overall, these results place {PG} methods under a solid theoretical footing, analogous to the global convergence guarantees of iterative value function based algorithms.},
	eventtitle = {Conference on Learning Theory},
	pages = {64--66},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	publisher = {{PMLR}},
	author = {Agarwal, Alekh and Kakade, Sham M. and Lee, Jason D. and Mahajan, Gaurav},
	urldate = {2022-05-20},
	date = {2020-07-15},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/oslund/Zotero/storage/LIU962II/Agarwal et al. - 2020 - Optimality and Approximation with Policy Gradient .pdf:application/pdf},
}

@report{kolve_ai2-thor_2019,
	title = {{AI}2-{THOR}: An Interactive 3D Environment for Visual {AI}},
	url = {http://arxiv.org/abs/1712.05474},
	shorttitle = {{AI}2-{THOR}},
	abstract = {We introduce The House Of {inteRactions} ({THOR}), a framework for visual {AI} research, available at http://ai2thor.allenai.org. {AI}2-{THOR} consists of near photo-realistic 3D indoor scenes, where {AI} agents can navigate in the scenes and interact with objects to perform tasks. {AI}2-{THOR} enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of {AI}2-{THOR} is to facilitate building visually intelligent models and push the research forward in this domain.},
	number = {{arXiv}:1712.05474},
	institution = {{arXiv}},
	author = {Kolve, Eric and Mottaghi, Roozbeh and Han, Winson and {VanderBilt}, Eli and Weihs, Luca and Herrasti, Alvaro and Gordon, Daniel and Zhu, Yuke and Gupta, Abhinav and Farhadi, Ali},
	urldate = {2022-05-20},
	date = {2019-03-15},
	doi = {10.48550/arXiv.1712.05474},
	eprinttype = {arxiv},
	eprint = {1712.05474 [cs]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/oslund/Zotero/storage/GD3S9UQ7/Kolve et al. - 2019 - AI2-THOR An Interactive 3D Environment for Visual.pdf:application/pdf;arXiv.org Snapshot:/home/oslund/Zotero/storage/FQLFGRF7/1712.html:text/html},
}

@article{savva_habitat_2019,
	title = {Habitat: A Platform for Embodied {AI} Research},
	url = {https://arxiv.org/abs/1904.01201v2},
	doi = {10.48550/arXiv.1904.01201},
	shorttitle = {Habitat},
	abstract = {We present Habitat, a platform for research in embodied artificial intelligence ({AI}). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single {GPU}. (ii) Habitat-{API}: a modular high-level library for end-to-end development of embodied {AI} algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and {SLAM} approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms {SLAM} if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments \{train, test\} x \{Matterport3D, Gibson\} for multiple sensors \{blind, {RGB}, {RGBD}, D\} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied {AI}.},
	author = {Savva, Manolis and Kadian, Abhishek and Maksymets, Oleksandr and Zhao, Yili and Wijmans, Erik and Jain, Bhavana and Straub, Julian and Liu, Jia and Koltun, Vladlen and Malik, Jitendra and Parikh, Devi and Batra, Dhruv},
	urldate = {2022-05-20},
	date = {2019-04-02},
	langid = {english},
	file = {Full Text PDF:/home/oslund/Zotero/storage/DU7RCCQN/Savva et al. - 2019 - Habitat A Platform for Embodied AI Research.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/6S9Z55BY/1904.html:text/html},
}

@inproceedings{xia_gibson_2018,
	title = {Gibson Env: Real-World Perception for Embodied Agents},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_Gibson_Env_Real-World_CVPR_2018_paper.html},
	shorttitle = {Gibson Env},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {9068--9079},
	author = {Xia, Fei and Zamir, Amir R. and He, Zhiyang and Sax, Alexander and Malik, Jitendra and Savarese, Silvio},
	urldate = {2022-05-20},
	date = {2018},
	file = {Full Text PDF:/home/oslund/Zotero/storage/ASKJG8VP/Xia et al. - 2018 - Gibson Env Real-World Perception for Embodied Age.pdf:application/pdf;Snapshot:/home/oslund/Zotero/storage/T4JJSIH4/Xia_Gibson_Env_Real-World_CVPR_2018_paper.html:text/html},
}