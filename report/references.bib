
@article{ghesu_multi-scale_2019,
	title = {Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in {CT} Scans},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8187667/},
	doi = {10.1109/TPAMI.2017.2782687},
	abstract = {Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem as a behavior learning task for an artiﬁcial agent. We couple the modeling of the anatomy appearance and the object search in a uniﬁed behavioral framework, using the capabilities of deep reinforcement learning and multi-scale image analysis. In other words, an artiﬁcial agent is trained not only to distinguish the target anatomical object from the rest of the body but also how to ﬁnd the object by learning and following an optimal navigation path to the target object in the imaged volumetric space. We evaluate our approach on 1487 3D-{CT} volumes from 532 patients, totaling over 500,000 image slices and show that we signiﬁcantly outperform state-of-the-art solutions on detecting several anatomical structures with no failed cases from a clinical acceptance perspective, while also improving the detection accuracy by 20-30\%. Most importantly, we improve the detection-speed of the reference methods by 2-3 orders of magnitude, achieving unmatched real-time performance on large 3D-{CT} scans.},
	pages = {176--189},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Ghesu, Florin-Cristian and Georgescu, Bogdan and Zheng, Yefeng and Grbic, Sasa and Maier, Andreas and Hornegger, Joachim and Comaniciu, Dorin},
	urldate = {2022-02-03},
	date = {2019-01-01},
	langid = {english},
	file = {Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:/home/oskar/library/zotero/storage/TF4ZNMC6/Ghesu et al. - 2019 - Multi-Scale Deep Reinforcement Learning for Real-T.pdf:application/pdf},
}

@incollection{ourselin_artificial_2016,
	location = {Cham},
	title = {An Artificial Agent for Anatomical Landmark Detection in Medical Images},
	volume = {9902},
	isbn = {978-3-319-46725-2 978-3-319-46726-9},
	url = {https://link.springer.com/10.1007/978-3-319-46726-9_27},
	abstract = {Fast and robust detection of anatomical structures or pathologies represents a fundamental task in medical image analysis. Most of the current solutions are however suboptimal and unconstrained by learning an appearance model and exhaustively scanning the space of parameters to detect a speciﬁc anatomical structure. In addition, typical feature computation or estimation of meta-parameters related to the appearance model or the search strategy, is based on local criteria or predeﬁned approximation schemes. We propose a new learning method following a fundamentally diﬀerent paradigm by simultaneously modeling both the object appearance and the parameter search strategy as a uniﬁed behavioral task for an artiﬁcial agent. The method combines the advantages of behavior learning achieved through reinforcement learning with eﬀective hierarchical feature extraction achieved through deep learning. We show that given only a sequence of annotated images, the agent can automatically and strategically learn optimal paths that converge to the sought anatomical landmark location as opposed to exhaustively scanning the entire solution space. The method signiﬁcantly outperforms state-ofthe-art machine learning and deep learning approaches both in terms of accuracy and speed on 2D magnetic resonance images, 2D ultrasound and 3D {CT} images, achieving average detection errors of 1-2 pixels, while also recognizing the absence of an object from the image.},
	pages = {229--237},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention - {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Ghesu, Florin C. and Georgescu, Bogdan and Mansi, Tommaso and Neumann, Dominik and Hornegger, Joachim and Comaniciu, Dorin},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	urldate = {2022-02-03},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-46726-9_27},
	note = {Series Title: Lecture Notes in Computer Science},
	file = {Ghesu et al. - 2016 - An Artificial Agent for Anatomical Landmark Detect.pdf:/home/oskar/library/zotero/storage/SUFGAH83/Ghesu et al. - 2016 - An Artificial Agent for Anatomical Landmark Detect.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	location = {Cambridge, Massachusetts},
	edition = {Second edition},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	series = {Adaptive computation and machine learning series},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	pagetotal = {526},
	publisher = {The {MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date = {2018},
	langid = {english},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/oskar/library/zotero/storage/HPCN43YL/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{bajcsy_1988, title={Active perception}, volume={76}, ISSN={1558-2256}, DOI={10.1109/5.5968}, abstractNote={Active perception (active vision specifically) is defined as a study of modeling and control strategies for perception. Local methods are distinguished from global models by their extent of application in space and time. The local models represent procedures and parameters such as optical distortions of the lens, focal lens, spatial resolution, bandpass filter, etc, The global models, on the other hand, characterize the overall performance and make predictions on how the individual modules interact. The control strategies are formulated as a search of such sequences of steps that would minimize a loss function while still seeking the most information. Examples are shown as the existence proof of the proposed theory on obtaining range from focus and stereo/vergence on 2-D segmentation of an image and 3-D shape parameterization.<>}, note={646 citations (Crossref) [2022-02-28]}, number={8}, journal={Proceedings of the IEEE}, author={Bajcsy, R.}, year={1988}, month={Aug}, pages={966–1005} }


@article{aloimonos_active_1988,
	title = {Active vision},
	volume = {1},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/BF00133571},
	doi = {10/cn4mdc},
	pages = {333--356},
	number = {4},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vision},
	author = {Aloimonos, John and Weiss, Isaac and Bandyopadhyay, Amit},
	urldate = {2022-02-07},
	date = {1988-01},
	langid = {english},
	note = {705 citations (Crossref) [2022-02-07]},
	file = {Active vision:/home/oskar/library/zotero/storage/SFZQUV5N/aloimonos1988.pdf.pdf:application/pdf},
}

@article{bajcsy_aloimonos_tsotsos_2018, title={Revisiting active perception}, volume={42}, ISSN={1573-7527}, DOI={10.1007/s10514-017-9615-3}, abstractNote={Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.}, note={86 citations (Crossref) [2022-03-13]}, number={2}, journal={Autonomous Robots}, author={Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K.}, year={2018}, month={Feb}, pages={177–196} }

@article{eckstein_visual_2011,
	title = {Visual search: A retrospective},
	volume = {11},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/11.5.14},
	doi = {10.1167/11.5.14},
	shorttitle = {Visual search},
	pages = {14--14},
	number = {5},
	journaltitle = {Journal of Vision},
	shortjournal = {Journal of Vision},
	author = {Eckstein, M. P.},
	urldate = {2022-02-22},
	date = {2011-12-30},
	langid = {english},
	note = {207 citations (Crossref) [2022-02-28]},
	file = {Visual search\: A retrospective:/home/oskar/library/zotero/storage/PX9QE8ZC/eckstein2011.pdf.pdf:application/pdf;Eckstein - 2011 - Visual search A retrospective.pdf:/home/oskar/library/zotero/storage/E87E8PCH/Eckstein - 2011 - Visual search A retrospective.pdf:application/pdf},
}

@article{wolfe_guided_2021,
	title = {Guided Search 6.0: An updated model of visual search},
	volume = {28},
	issn = {1531-5320},
	doi = {10.3758/s13423-020-01859-9},
	shorttitle = {Guided Search 6.0},
	abstract = {This paper describes Guided Search 6.0 ({GS}6), a revised model of visual search. When we encounter a scene, we can see something everywhere. However, we cannot recognize more than a few items at a time. Attention is used to select items so that their features can be "bound" into recognizable objects. Attention is "guided" so that items can be processed in an intelligent order. In {GS}6, this guidance comes from five sources of preattentive information: (1) top-down and (2) bottom-up feature guidance, (3) prior history (e.g., priming), (4) reward, and (5) scene syntax and semantics. These sources are combined into a spatial "priority map," a dynamic attentional landscape that evolves over the course of search. Selective attention is guided to the most active location in the priority map approximately 20 times per second. Guidance will not be uniform across the visual field. It will favor items near the point of fixation. Three types of functional visual field ({FVFs}) describe the nature of these foveal biases. There is a resolution {FVF}, an {FVF} governing exploratory eye movements, and an {FVF} governing covert deployments of attention. To be identified as targets or rejected as distractors, items must be compared to target templates held in memory. The binding and recognition of an attended object is modeled as a diffusion process taking {\textgreater} 150 ms/item. Since selection occurs more frequently than that, it follows that multiple items are undergoing recognition at the same time, though asynchronously, making {GS}6 a hybrid of serial and parallel processes. In {GS}6, if a target is not found, search terminates when an accumulating quitting signal reaches a threshold. Setting of that threshold is adaptive, allowing feedback about performance to shape subsequent searches. Simulation shows that the combination of asynchronous diffusion and a quitting signal can produce the basic patterns of response time and error data from a range of search experiments.},
	pages = {1060--1092},
	number = {4},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Wolfe, Jeremy M.},
	date = {2021-08},
	pmid = {33547630},
	note = {29 citations (Crossref) [2022-03-02]},
	keywords = {Attention, Bottom-up, Errors, Eye Movements, Guided search, Humans, Pattern Recognition, Visual, Reaction time, Reaction Time, Selective attention, Top-down, Visual Fields, Visual Perception, Visual search, Visual working memory},
	file = {Full Text:/home/oskar/library/zotero/storage/WBASNPNB/Wolfe - 2021 - Guided Search 6.0 An updated model of visual sear.pdf:application/pdf},
}

@article{wolfe_visual_2010,
	title = {Visual search},
	volume = {20},
	url = {https://www.ncbi.nlm.nih.gov/labs/pmc/articles/PMC5678963/},
	doi = {10.1016/j.cub.2010.02.016},
	pages = {R346},
	number = {8},
	journaltitle = {Current biology : {CB}},
	author = {Wolfe, Jeremy M.},
	urldate = {2022-03-02},
	date = {2010-04-27},
	langid = {english},
	pmid = {21749949},
	note = {64 citations (Crossref) [2022-03-02]
Publisher: {NIH} Public Access},
	file = {Visual search:/home/oskar/library/zotero/storage/N56NHYTS/wolfe2010.pdf.pdf:application/pdf;Snapshot:/home/oskar/library/zotero/storage/F564KWBK/PMC5678963.html:text/html;Full Text:/home/oskar/library/zotero/storage/CLUR5J4F/Wolfe - 2010 - Visual search.pdf:application/pdf},
}

@article{wolfe_horowitz_2017, title={Five factors that guide attention in visual search}, volume={1}, ISSN={2397-3374}, DOI={10.1038/s41562-017-0058}, abstractNote={How do we find what we are looking for? Even when the desired target is in the current field of view, we need to search because fundamental limits on visual processing make it impossible to recognize everything at once. Searching involves directing attention to objects that might be the target. This deployment of attention is not random. It is guided to the most promising items and locations by five factors discussed here: bottom-up salience, top-down feature guidance, scene structure and meaning, the previous history of search over timescales ranging from milliseconds to years, and the relative value of the targets and distractors. Modern theories of visual search need to incorporate all five factors and specify how these factors combine to shape search behaviour. An understanding of the rules of guidance can be used to improve the accuracy and efficiency of socially important search tasks, from security screening to medical image perception.}, note={300 citations (Crossref) [2022-02-28]}, number={33}, journal={Nature Human Behaviour}, publisher={Nature Publishing Group}, author={Wolfe, Jeremy M. and Horowitz, Todd S.}, year={2017}, month={Mar}, pages={1–8} }