\section{Method}

\begin{frame}
    \frametitle{Problem Formulation}

    \begin{itemize}
        \item Agent searches scene \(S \subset \mathbb{R}^d\) .
        \item Scene contains set of targets \(\{t_0, \dots t_n\}\), \(t_i \in S\).
        \item Agent perceives view \(V \subset S\).
        \item Move actions transform view to new subspace.
        \item Trigger action indicates that target(s) is in view.
        \item \textbf{Locate all targets while minimizing the number of time steps}.
    \end{itemize}
\end{frame}

\subsection{Environments}

\begin{frame}
    \frametitle{Environments}
    
    \begin{itemize}
        \item Three environments with varying characteristics.
        \item Search space discretized into \(16 \times 16\) camera positions.
        \item Each camera position has a unique view \(V \subset S\).
        \item Three targets in all scenes.
        \item Target probability correlated with scene appearance.
        \item Should be possible to do better than exhaustive search on average.
        \item Scenes procedurally generated:
        \begin{itemize}
            \item Pseudorandom seed determines scene appearance and target positions.
            \item Gives control over difficulty to solve.
            \item Can vary training and test set sizes.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Observation, Action and Reward}

    At each time step \(t\):

    \begin{itemize}
        \item The agent receives observation \(o_t = \left\langle x_t, p_t \right\rangle\), where
        \begin{itemize}
            \item \(x_t \in \mathbb{R}^{3 \times 64 \times 64}\) is an RGB image of current view, and
            \item \(p_t \in \{0, \dots, 15\} \times \{0, \dots, 15\}\) is the position of the camera.
        \end{itemize}
        \item Takes action \(a_t \in \{\texttt{TRIGGER}, \texttt{UP}, \texttt{DOWN}, \texttt{LEFT}, \texttt{RIGHT}\}\), where
        \begin{itemize}
            \item \texttt{TRIGGER} indicates that a target is in view, and
            \item \texttt{UP}, \texttt{DOWN}, \texttt{LEFT}, \texttt{RIGHT} move the view in each cardinal direction.
        \end{itemize}
        \item Receives reward \(r_t = h - 0.001\) where \(h = \left\vert T \cap V \right\vert\) is the number of targets in view.
        \begin{itemize}
            \item Rewarded for finding targets.
            \item Constant penalty encourages quick episode completion.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Gaussian Environment}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item 2D scene.
                \item Three gaussian kernels with random center.
                \item Sum of kernels determine appearance of scene and probability of targets.
                \item Clear correlation between appearance and desired behavior.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[scale=1.0]{figures/gaussian.pdf}
            \end{figure}
        \end{column}
    \end{columns}    
\end{frame}

\begin{frame}
    \frametitle{Terrain Environment}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Similar to previous environment.
                \item Terrain seen from above.
                \item Gradient noise used to generate height map.
                \item Color determined by height.
                \item Targets placed with uniform probability across coastlines.
                \item More realistic, higher variance.
                \item Analogous to search and rescue with UAV.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[scale=1.0]{figures/terrain.pdf}
            \end{figure}
        \end{column}
    \end{columns}   
\end{frame}

\begin{frame}
    \frametitle{Camera Environment}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item 3D scene viewed from a perspective projection camera.
                \item Height map from terrain environment turned into mesh, same appearance and target probability as before.
                \item Camera location fixed at center of scene.
                \item Moving actions control pan and tilt (pitch and yaw).
                \item Visually complex, difficult to interpret.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \includegraphics[scale=1.0]{figures/camera.pdf}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\subsection{Approach}

\begin{frame}
    \frametitle{Architecture}

    \begin{itemize}
        \item Actor-critic method trained with PPO~\cite{schulman_ppo_2017}.
        \item Image \(x_t\) passed through CNN.
        \item Latent image representation \(h_t\) and position \(p_t\) passed through RNN. Two variants:
        \begin{enumerate}
            \item LSTM with input \(\left\lbrack h_t, p_t \right\rbrack\).
            \item Spatial memory.
        \end{enumerate}
        \item Policy and value heads approximate \(\pi\) and \(v_\pi\) with MLPs.
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[scale=0.7]{figures/architecture.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Recurrent Steps}

    \begin{enumerate}
        \item LSTM:
        \begin{itemize}
            \item Proven to work for POMDPs \cite{hausknecht_stone_2017,mnih_asynchronous_2016,mirowski_navigate_2017,gupta_cognitive_2019}.
            \item May struggle with remembering over many time steps.
            \item Important for exhaustive search and scene understanding.
        \end{itemize}
        \item Spatial memory (inspired by \cite{parisotto_salakhutdinov_2017}):
        \begin{itemize}
            \item Structured memory \(M_t \in \mathbb{R}^{C \times 16 \times 16}\) as hidden state \\
            (one slot per camera position \(p_t\) / unique view \(V\) / image \(x_t\)).
            \item Read vector \(r_t = f(M_t)\), \(f\) is CNN.
            \item Write vector \(w_t = g(\left\lbrack h_t, r_t \right\rbrack)\), \(g\) is MLP.
            \item Action probabilities \(\pi(\left\lbrack r_t, w_t \right\rbrack)\) and value \(v(\left\lbrack r_t, w_t \right\rbrack)\).
            \item \(w_t\) written to index \(p_t\) of \(M_{t+1}\).
        \end{itemize}
    \end{enumerate}
\end{frame}

\subsection{Experiments}

\begin{frame}
    \frametitle{Experiments}

    \begin{itemize}
        \item Train for 25M time steps.
        \item Results reported across 3 runs with different seeds.
        \item Interval estimates via stratified bootstrap confidence intervals.
        \item Separate training and test sets.
        \item Same hyperparameters in all runs.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Experiment I: Search Space and Reward Signal}

    \begin{itemize}
        \item Larger search spaces take longer to train:
        \begin{itemize}
            \item Sparse reward might not be sufficient.
            \item Stronger demands on memory (remember searched positions, scene understanding).
        \end{itemize}
        \item Investigate impact by comparing agents on \(8 \times 8\), \(16 \times 16\), \(24 \times 24\), \(32 \times 32\) versions of gaussian environment.
        \item Evaluate two additional reward signals that may speed up training:
        \begin{itemize}
            \item \(r'_t = r_t + e\), where \(e=0.1\) if \(a_t \neq \mathtt{TRIGGER}\) moves the view to an unexplored region and \(0\) otherwise.
            \item \(r''_t = r_t + d\), where \(d=0.1\) if \(a_t \neq \mathtt{TRIGGER}\) moves the view towards the nearest target and \(0\) otherwise.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Experiment II: Search Performance}

    \begin{itemize}
        \item Compare to random searcher, exhaustive searcher, human searcher with prior knowledge of scenes.
        \item Use held out samples as test set.
        \item Average number of steps on test set.
        \item SPL metric~\cite{anderson_evaluation_2018}, with \(N\) as the number of test samples, \(S_i\) indicating success, \(p_i\) as the number of steps and \(l_i\) as the shortest path length:
    \end{itemize}
    \[
        \frac{1}{N} \sum_{i=1}^N S_i \frac{l_i}{\max(p_i,l_i)}
    \]
\end{frame}

\begin{frame}
    \frametitle{Experiment III: Generalization with Limited Training Samples}

    \begin{itemize}
        \item Limit number of scene samples seen during training to 100, 1000, 10 000, \dots.
        \item Use terrain environment, high appearance variance and somewhat realistic.
        \item Fix seed pool used to generate scenes seen during training.
        \item Train agents until convergence (or for a fixed number of time steps).
        \item Test on held out scenes from full distribution. 
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Implementation}

    \begin{itemize}
        \item OpenAI Gym environment interface.
        \item PyTorch for models and automatic differentiation.
        \item Intel Core i9-10900X CPU.
        \item NVIDIA GeForce RTX 2080 Ti GPU.
    \end{itemize}
\end{frame}